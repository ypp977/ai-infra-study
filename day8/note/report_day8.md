# Day 8 - CUDA åŸºç¡€

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- é…ç½® CUDA ç¼–è¯‘ç¯å¢ƒï¼ˆ`nvcc`ï¼‰
- ç¼–å†™ç¬¬ä¸€ä¸ª CUDA ç¨‹åºï¼ˆHello CUDAï¼‰
- å®ç°å¹¶è¿è¡Œ **å‘é‡åŠ æ³•** CUDA Kernel
- é€šè¿‡ä¿®æ”¹ blockDim/gridDim å‚æ•°ï¼Œè§‚å¯Ÿæ€§èƒ½å·®å¼‚

------

## 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡

### 1. æ£€æŸ¥ GPU é©±åŠ¨ä¸ CUDA

```bash
nvidia-smi
```

]è¾“å‡ºé‡Œèƒ½çœ‹åˆ° **é©±åŠ¨ç‰ˆæœ¬ / CUDA Version**ï¼Œç¡®è®¤ CUDA â‰¥ 11.0ã€‚

![image-20250830193821913](./report_day8.assets/image-20250830193821913.png)

### 2. æ£€æŸ¥ `nvcc`

```bash
nvcc --version
```

![image-20250830193643923](./report_day8.assets/image-20250830193643923.png)

å¦‚æœæ²¡æœ‰ï¼Œå®‰è£… CUDA Toolkitï¼ˆLinux ä¸ºä¾‹ï¼‰ï¼š

1. æ·»åŠ  NVIDIA å®˜æ–¹ä»“åº“ï¼š

```bash
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
```

1. å®‰è£… CUDA Toolkitï¼ˆæ¯”å¦‚ CUDA 12.4ï¼‰ï¼š

```bash
sudo apt install -y cuda-toolkit-12-4
```

é…ç½®ç¯å¢ƒå˜é‡ï¼ˆå†™åˆ° `~/.bashrc`ï¼‰ï¼š

```bash
export PATH=/usr/local/cuda-12.4/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH
```

é‡æ–°åŠ è½½ï¼š

```bash
source ~/.bashrc
```

### 3. æ–°å»ºå·¥ä½œç›®å½•

```
mkdir -p ~/ai-infra-study/day8 && cd ~/ai-infra-study/day8
```

------

## 2ï¸âƒ£ Hello CUDA ç¨‹åº

### `cuda_hello.cu`

```c++
#include <stdio.h>

__global__ void hello_kernel() {
    printf("Hello from GPU thread (%d,%d,%d)\n",
           threadIdx.x, blockIdx.x, blockDim.x);
}

int main() {
    // <<<grid, block>>>: å¯åŠ¨ 2 ä¸ª blockï¼Œæ¯ä¸ª block 3 ä¸ªçº¿ç¨‹
    hello_kernel<<<2, 3>>>();
    cudaDeviceSynchronize();  // ç­‰å¾… GPU å®Œæˆ
    return 0;
}
```

### ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -o hello cuda_hello.cu
./hello
```

âœ… é¢„æœŸè¾“å‡ºï¼ˆé¡ºåºå¯èƒ½ä¸åŒï¼Œå› ä¸ºçº¿ç¨‹å¹¶è¡Œï¼‰ï¼š

![image-20250830200619998](./report_day8.assets/image-20250830200619998.png)

------

## 3ï¸âƒ£ å‘é‡åŠ æ³• CUDA ç¨‹åº

### `vector_add.cu`

```c++
#include <stdio.h>
#include <cuda_runtime.h>

__global__ void vector_add(const float *a, const float *b, float *c,int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x; // è®¡ç®—å…¨å±€ç´¢å¼•

    if(i < n) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    int n = 1 << 16; // 65536ä¸ªå…ƒç´ 
    size_t bytes = n * sizeof(float);

    // åˆ†é… host å†…å­˜
    float *host_a = (float *)malloc(bytes);
    float *host_b = (float *)malloc(bytes);
    float *host_c = (float *)malloc(bytes);

    // åˆå§‹åŒ–æ•°æ®
    for (int i = 0; i < n;i++) {
        host_a[i] = 1.0f;
        host_b[i] = 2.0f;
    }

    // åˆ†é…device å†…å­˜
    float *device_a, *device_b, *device_c;
    cudaMalloc(&device_a, bytes);
    cudaMalloc(&device_b, bytes);
    cudaMalloc(&device_c, bytes);

    // å¤åˆ¶æ•°æ® Host -> Device
    cudaMemcpy(device_a, host_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes, cudaMemcpyHostToDevice);

    // è®¡ç®— grid/block é…ç½®
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;

    // å¯åŠ¨kernel
    vector_add<<<gridSize, blockSize>>>(device_a, device_b, device_c, n);
    cudaDeviceSynchronize();

    // å¤åˆ¶ç»“æœ device -> host;
    cudaMemcpy(host_c, device_c, bytes, cudaMemcpyDeviceToHost);

    // éªŒè¯ç»“æœ
    for (int i = 0; i < 10;i++) {
        printf("c[%d] = %f\n", i, host_c[i]);
    }

    // æ¸…ç†
    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);
    
    free(host_a);
    free(host_b);
    free(host_c);

    return 0;
}
```

### ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 vector_add.cu -o vec
./vec
```

âœ… é¢„æœŸè¾“å‡ºï¼š

![image-20250831001452040](./report_day8.assets/image-20250831001452040.png)

------

## 4ï¸âƒ£ æ€§èƒ½å®éªŒ

ä¿®æ”¹ `blockSize`ï¼Œé‡æ–°ç¼–è¯‘è¿è¡Œï¼Œè®°å½•æ—¶é—´ã€‚
 åœ¨ kernel å‰ååŠ æ—¶é—´æµ‹é‡ï¼š

```c++
// å®šä¹‰ä¸¤ä¸ª CUDA äº‹ä»¶å˜é‡ï¼Œç”¨æ¥æ ‡è®°æ—¶é—´ç‚¹
cudaEvent_t start, stop;

// åˆ›å»ºäº‹ä»¶å¯¹è±¡ï¼ˆç±»ä¼¼äºåœ¨ GPU ä¸Šçš„â€œæ—¶é—´æˆ³â€ï¼‰
cudaEventCreate(&start);
cudaEventCreate(&stop);

// åœ¨ GPU ä¸Šè®°å½•ä¸€ä¸ªèµ·å§‹æ—¶é—´ç‚¹ï¼ˆstartï¼‰
cudaEventRecord(start);

// å¯åŠ¨ CUDA kernelï¼ˆè¿™é‡Œæ˜¯å‘é‡åŠ æ³•ï¼‰
// <<<gridSize, blockSize>>> æŒ‡å®š grid å’Œ block çš„å¤§å°
vector_add<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);

// åœ¨ GPU ä¸Šè®°å½•ä¸€ä¸ªç»“æŸæ—¶é—´ç‚¹ï¼ˆstopï¼‰
cudaEventRecord(stop);

// ç­‰å¾… GPU å®Œæˆ stop äº‹ä»¶ï¼ˆä¿è¯ kernel æ‰§è¡Œå®Œå†ç»§ç»­ï¼‰
cudaEventSynchronize(stop);

// è®¡ç®— start å’Œ stop ä¹‹é—´çš„æ—¶é—´å·®ï¼ˆå•ä½ï¼šæ¯«ç§’ msï¼‰
float ms = 0;
cudaEventElapsedTime(&ms, start, stop);

// æ‰“å°ç»“æœï¼Œæ˜¾ç¤ºå½“å‰ blockSize ä¸‹ kernel çš„è¿è¡Œæ—¶é—´
printf("blockSize=%d, Time=%.3f ms\n", blockSize, ms);

```

å®éªŒå»ºè®®ï¼š

- blockSize = 64, 128, 256, 512, 1024

blockSize å®éªŒç»“æœ

| blockSize | æ‰§è¡Œæ—¶é—´ (ms) |
| --------- | ------------- |
| 64        | 0.080         |
| 128       | 0.006         |
| 256       | 0.051         |
| 512       | 0.060         |
| 1024      | 0.033         |

ç»“è®º

- æœ€ä¼˜é…ç½®åœ¨ **blockSize=128**ï¼Œè€—æ—¶æœ€ä½ã€‚
- blockSize è¿‡å°ï¼ˆ64ï¼‰æˆ–è¿‡å¤§ï¼ˆ1024ï¼‰éƒ½ä¼šå¸¦æ¥é¢å¤–å¼€é”€ã€‚
- warp å¤§å°ï¼ˆ32ï¼‰çš„æ•´æ•°å€ä¸€èˆ¬èƒ½è·å¾—è¾ƒå¥½æ€§èƒ½ã€‚

------

## 5ï¸âƒ£ æ·±åº¦è¿½é—®ï¼ˆæ€è€ƒé¢˜ï¼‰

### 1. grid-stride loop ç›¸æ¯”ä¸€æ¬¡æ€§å¤§ grid çš„ä¼˜åŠ£ï¼Ÿ

#### ğŸŸ¢ Grid-Stride Loop çš„ä¼˜ç‚¹

1. **å¯æ‰©å±•æ€§å¼º**
   - ä½ å¯ä»¥åªå¯åŠ¨ä¸€ä¸ªâ€œé€‚åº¦å¤§å°â€çš„ gridï¼ˆä¾‹å¦‚ 256Ã—32ï¼‰ï¼Œä½† kernel å†…éƒ¨ç”¨ `i += gridDim.x*blockDim.x` å¾ªç¯ï¼Œå°±èƒ½è¦†ç›–ä»»æ„å¤§æ•°ç»„ã€‚
   - ç¨‹åºå‘˜ä¸ç”¨æ‹…å¿ƒ n ç‰¹åˆ«å¤§æ—¶ gridSize è¶…è¿‡ GPU çš„æœ€å¤§é™åˆ¶ã€‚
2. **ä»£ç æ›´é€šç”¨**
   - å†™ä¸€æ¬¡ kernelï¼Œä¸ç”¨åœ¨ host ç«¯é¢‘ç¹æ”¹ gridSizeï¼Œå°±èƒ½åº”å¯¹ä¸åŒæ•°æ®è§„æ¨¡ã€‚
   - é€‚åˆå†™â€œåº“å‡½æ•°â€é£æ ¼çš„é€šç”¨ kernelã€‚
3. **æ›´å¥½åˆ©ç”¨ GPU èµ„æº**
   - å¦‚æœ gridSize ä¸è¶³ä»¥å®Œå…¨è¦†ç›–æ•°æ®ï¼Œä¸€æ¬¡æ€§å¤§ grid å¯èƒ½å°±æ²¡æ³•è¿è¡Œå®Œæ•´æ•°æ®ï¼›è€Œ grid-stride loop èƒ½ä¿è¯æ‰€æœ‰æ•°æ®éƒ½è¢«éå†ã€‚
   - åœ¨å¤š GPU æˆ–åŠ¨æ€ workload çš„åœºæ™¯ä¸‹ï¼Œå®¹æ˜“åšåˆ°â€œè´Ÿè½½å‡è¡¡â€ã€‚
4. **æ”¯æŒå¤šæ¬¡è°ƒç”¨**
   - å¦‚æœæ•°æ®é‡è¿œå¤§äºå•æ¬¡ grid èƒ½æ‰¿å—çš„å¤§å°ï¼Œstride loop èƒ½åœ¨å• kernel å†…å®Œæˆï¼Œé¿å…å¤šæ¬¡ kernel launchï¼ˆå‡å°‘ CPU-GPU å¯åŠ¨å¼€é”€ï¼‰ã€‚

------

#### ğŸ”´ Grid-Stride Loop çš„ç¼ºç‚¹

1. **ä»£ç å¯è¯»æ€§ç¨å·®**
   - å¾ªç¯å½¢å¼å¯¹ CUDA æ–°æ‰‹ä¸ç›´è§‚ï¼Œä¸å¦‚ä¸€æ¬¡æ€§å¤§ grid ç›´ç™½ã€‚
2. **æ¯ä¸ªçº¿ç¨‹è¦è·‘å¾ªç¯**
   - å•ä¸ªçº¿ç¨‹å¯èƒ½æ‰§è¡Œå¤šæ¬¡å¾ªç¯è¿­ä»£ï¼Œç›¸æ¯”â€œä¸€æ¬¡æ€§å¤§ gridâ€çš„â€œä¸€æ¬¡æå®šâ€ï¼Œå¯èƒ½å¢åŠ å¯„å­˜å™¨ä½¿ç”¨é‡ï¼Œé™ä½ warp å ç”¨ç‡ã€‚
3. **æ€§èƒ½ä¸Šç•¥æœ‰ overhead**
   - åœ¨æ•°æ®è§„æ¨¡åˆšå¥½èƒ½è¢«ä¸€æ¬¡æ€§å¤§ grid æ•´é½è¦†ç›–æ—¶ï¼Œstride loop å¤šäº†ä¸€å±‚ `for` å¾ªç¯åˆ¤æ–­ï¼Œæ€§èƒ½å¯èƒ½æ¯”å¤§ grid ç•¥ä½ã€‚

------

#### ğŸŸ¡ ä¸€æ¬¡æ€§å¤§ grid çš„ä¼˜ç‚¹

1. **å®ç°ç®€å•**ï¼šåªè¦ç®—å¥½ `gridSize = (n+blockSize-1)/blockSize`ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ ï¼Œå†™æ³•ç›´è§‚ã€‚
2. **æ€§èƒ½æç®€**ï¼šæ²¡æœ‰é¢å¤–çš„å¾ªç¯åˆ¤æ–­ï¼Œå•ä¸ªçº¿ç¨‹åªè·‘ä¸€æ¬¡é€»è¾‘ã€‚
3. **é€‚åˆå°/ä¸­ç­‰è§„æ¨¡æ•°æ®**ï¼šä¸€æ¬¡å°±èƒ½è¦†ç›–çš„åœºæ™¯ä¸‹æ€§èƒ½é€šå¸¸æœ€ä½³ã€‚

------

#### ğŸ”µ ä¸€æ¬¡æ€§å¤§ grid çš„ç¼ºç‚¹

1. **æ‰©å±•æ€§å·®**ï¼šå¦‚æœæ•°æ®é‡éå¸¸å¤§ï¼ˆ> GPU æœ€å¤§ gridSize * blockSizeï¼‰ï¼Œè¿™ç§å†™æ³•å°±ä¸èƒ½ç›´æ¥è·‘ã€‚
2. **ä¸å¤Ÿçµæ´»**ï¼šæ¯æ¬¡æ•°æ®è§„æ¨¡ä¸åŒï¼Œéƒ½è¦é‡æ–°ç®— gridSizeï¼Œä¸é€‚åˆä½œæˆé€šç”¨ kernelã€‚
3. **æ½œåœ¨èµ„æºæµªè´¹**ï¼šå¦‚æœ gridSize ç®—å¾—å¤ªå¤§ï¼Œéƒ¨åˆ†çº¿ç¨‹å¯èƒ½ idleï¼ˆæµªè´¹è°ƒåº¦èµ„æºï¼‰ã€‚

------

#### ğŸ¯ æ€»ç»“

- **ä¸€æ¬¡æ€§å¤§ grid**ï¼šé€‚åˆ**æ•°æ®è§„æ¨¡å·²çŸ¥ã€èƒ½è¢«ä¸€æ¬¡ grid è¦†ç›–**çš„åœºæ™¯ï¼Œå†™æ³•ç›´è§‚ï¼Œæ€§èƒ½ç•¥å¥½ã€‚
- **grid-stride loop**ï¼šé€‚åˆ**é€šç”¨åº“å‡½æ•°ã€æ•°æ®è§„æ¨¡ä¸ç¡®å®šã€å¤§è§„æ¨¡æ•°æ®**ï¼Œæ‰©å±•æ€§å’Œå¥å£®æ€§æ›´å¼ºï¼Œæ˜¯ **CUDA å®˜æ–¹æ¨èçš„æœ€ä½³å®è·µ**ï¼ˆNVIDIA çš„ sample å’Œ cuBLAS éƒ½ç”¨è¿™ç§æ¨¡å¼ï¼‰ã€‚

### 2. çº¿ç¨‹ç´¢å¼•è®¡ç®—æº¢å‡º/è¶Šç•Œçš„å¸¸è§å‘ä¸é˜²å¾¡å¼ç¼–ç¨‹ï¼Ÿ

#### ğŸ•³ï¸ å¸¸è§å‘

##### 1. å¿˜è®°è¾¹ç•Œæ£€æŸ¥

```
__global__ void kernel(float *a, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    a[i] = 1.0f;  // âŒ å¦‚æœ i >= n å°±è¶Šç•Œ
}
```

- **é—®é¢˜**ï¼šå¦‚æœ `gridSize * blockSize > n`ï¼Œæœ€åå‡ ä¸ªçº¿ç¨‹ä¼šè®¿é—®è¶Šç•Œã€‚
- **åæœ**ï¼šå¯èƒ½é™é»˜å†™åå†…å­˜ï¼Œç”šè‡³å¯¼è‡´ä¸å¯é¢„æµ‹çš„é”™è¯¯ã€‚

------

##### 2. å¤šç»´ç´¢å¼•è®¡ç®—é”™è¯¯

```
int i = blockIdx.x * blockDim.x + threadIdx.x;
int j = blockIdx.y * blockDim.y + threadIdx.y;
a[i + j] = ...; // âŒ å¿˜äº†ä¹˜ strideï¼Œå¯¼è‡´ä¸åŒçº¿ç¨‹è¦†ç›–åŒä¸€å†…å­˜
```

- **é—®é¢˜**ï¼šäºŒç»´/ä¸‰ç»´ block è®¡ç®—æ—¶ï¼Œå¸¸å¿˜è®°ä¹˜ä¸Šè¡Œå®½ï¼ˆstrideï¼‰ã€‚
- **åæœ**ï¼šç»“æœé”™ä¹±ï¼Œå¤šä¸ªçº¿ç¨‹å†™åŒä¸€åœ°å€ã€‚

------

##### 3. æ•´æ•°æº¢å‡º

```
int idx = blockIdx.x * blockDim.x + threadIdx.x;
```

- **é—®é¢˜**ï¼šå¦‚æœ `gridDim.x * blockDim.x` è¶…è¿‡ `2^31-1`ï¼Œ`int` æº¢å‡ºã€‚
- **åæœ**ï¼šç´¢å¼•å€¼å˜è´Ÿæ•°æˆ–é”™ä½ã€‚
- **çœŸå®æƒ…å†µ**ï¼šå¤§å‹é—®é¢˜é‡Œå¯èƒ½çœŸçš„è¶…è¿‡ `2B` å…ƒç´ ï¼Œè¦ç”¨ `size_t` æˆ– `long long`ã€‚

------

##### 4. stride loop å†™æ³•é”™ä½

```
for (int i = threadIdx.x; i < n; i += blockDim.x) {
    ...
}
```

- **é—®é¢˜**ï¼šå¿˜äº†åŠ ä¸Š `blockIdx.x * blockDim.x`ï¼Œå¯¼è‡´å¤šä¸ª block é‡å å¤„ç†ç›¸åŒæ•°æ®ã€‚

------

##### 5. host/device ä¸ä¸€è‡´

```
int n = 1 << 30; // host æ˜¯ long longï¼Œdevice kernel é‡Œå´ç”¨ int
```

- **é—®é¢˜**ï¼šCPU å’Œ GPU ç«¯æ•°æ®ç±»å‹ä¸åŒï¼Œå¯¼è‡´ç´¢å¼•é”™ä¹±ã€‚

------

#### ğŸ›¡ï¸ é˜²å¾¡å¼ç¼–ç¨‹æŠ€å·§

##### âœ… 1. è¾¹ç•Œæ£€æŸ¥

åœ¨ kernel å†…å§‹ç»ˆåšï¼š

```
int i = blockIdx.x * blockDim.x + threadIdx.x;
if (i < n) {
    a[i] = 1.0f;
}
```

> å³ä½¿ä½ è®¤ä¸ºä¸ä¼šè¶Šç•Œï¼Œä¹Ÿè¦å†™è¿™è¡Œï¼Œå±äºâ€œé˜²å¾¡æ€§ä¹ æƒ¯â€ã€‚

------

##### âœ… 2. ä½¿ç”¨ `size_t` æˆ– `long long`

```
size_t i = (size_t)blockIdx.x * blockDim.x + threadIdx.x;
if (i < n) ...
```

> é¿å…å¤§è§„æ¨¡é—®é¢˜æ—¶æº¢å‡ºã€‚

------

##### âœ… 3. å¤šç»´æ•°ç»„å†™æ³•è¦æ˜¾å¼ stride4

```
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;
if (row < N && col < M) {
    a[row * M + col] = ...; // æ˜¾å¼å†™å‡º M
}
```

------

##### âœ… 4. grid-stride loop æ¨¡æ¿

æ¨èå†™æˆï¼š

```
for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
     i < n;
     i += blockDim.x * gridDim.x) {
    ...
}
```

> è¿™æ˜¯ NVIDIA å®˜æ–¹æ¨èçš„â€œé»„é‡‘æ¨¡æ¿â€ï¼Œæ—¢é¿å…è¶Šç•Œï¼Œåˆä¿è¯æ‰©å±•æ€§ã€‚

------

##### âœ… 5. è°ƒè¯•å·¥å…·

- **cuda-memcheck**ï¼šè¿è¡Œæ—¶æ•æ‰è¶Šç•Œè®¿é—®ã€‚
- **`assert(i<n)`**ï¼šåœ¨ debug build é‡ŒåŠ æ–­è¨€ï¼Œç›´æ¥æŠ¥é”™ã€‚
- **sanity check**ï¼šåœ¨ host ä¸Šéšæœº spot-check kernel è¾“å‡ºã€‚

------

#### ğŸ¯ æ€»ç»“

1. **å‘**ï¼šå¿˜è¾¹ç•Œã€ç»´åº¦è®¡ç®—é”™è¯¯ã€int æº¢å‡ºã€stride å†™é”™ã€‚
2. **é˜²å¾¡**ï¼š
   - æ°¸è¿œå†™ `if (i<n)`
   - ç”¨ `size_t` ä»£æ›¿ `int`
   - æ˜¾å¼ strideï¼ˆrow * M + colï¼‰
   - é‡‡ç”¨ grid-stride loop æ¨¡æ¿
   - ç”¨ `cuda-memcheck` å’Œ `assert` è¾…åŠ©

ğŸ‘‰ CUDA çš„æœ€ä½³å®è·µæ˜¯ï¼š**å†™ä»»ä½• kernel æ—¶ï¼Œéƒ½å‡è®¾å¯èƒ½è¶Šç•Œï¼Œå…ˆæŠŠç´¢å¼•è¾¹ç•Œé˜²ä½ï¼Œå†è€ƒè™‘ä¼˜åŒ–**

### 3. register å‹åŠ›å¦‚ä½•å½±å“ occupancy ä¸æº¢å‡ºåˆ°æœ¬åœ°å†…å­˜ï¼Ÿ

æ¶‰åŠ CUDA **å¯„å­˜å™¨èµ„æºã€çº¿ç¨‹å¹¶å‘åº¦ (occupancy)ã€æœ¬åœ°å†…å­˜æº¢å‡º** ä¸‰è€…ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬æ‹†å¼€æ¥çœ‹ï¼š

------

#### 1ï¸âƒ£ CUDA å¯„å­˜å™¨èµ„æºæœ‰é™

- æ¯ä¸ª **SMï¼ˆStreaming Multiprocessorï¼‰** æœ‰å›ºå®šæ•°é‡çš„ç‰©ç†å¯„å­˜å™¨ï¼ˆä¾‹å¦‚ A100 ä¸Šæ˜¯ 64K 32-bit å¯„å­˜å™¨ï¼‰ã€‚
- è¿™äº›å¯„å­˜å™¨è¦è¢«åˆ†é…ç»™ SM ä¸Šçš„æ‰€æœ‰ **çº¿ç¨‹** ä½¿ç”¨ã€‚

ğŸ‘‰ å½“ä½ ç¼–è¯‘ä¸€ä¸ª kernel æ—¶ï¼Œç¼–è¯‘å™¨ä¼šä¼°ç®— **æ¯ä¸ªçº¿ç¨‹éœ€è¦å¤šå°‘å¯„å­˜å™¨**ã€‚
 ä¾‹å¦‚ï¼š

- kernel A æ¯çº¿ç¨‹ 32 ä¸ªå¯„å­˜å™¨
- kernel B æ¯çº¿ç¨‹ 128 ä¸ªå¯„å­˜å™¨

------

#### 2ï¸âƒ£ Occupancy ä¸å¯„å­˜å™¨å…³ç³»

**Occupancy = å·²ç»è°ƒåº¦çš„çº¿ç¨‹æ•° / ç†è®ºæœ€å¤§çº¿ç¨‹æ•°**
 å¯„å­˜å™¨æ•°ç›®ä¼šé™åˆ¶èƒ½åŒæ—¶é©»ç•™çš„çº¿ç¨‹æ•°ã€‚

ä¾‹å­ï¼š

- å‡è®¾ä¸€ä¸ª SM æœ‰ 64K ä¸ªå¯„å­˜å™¨ã€‚
- å¦‚æœæ¯ä¸ªçº¿ç¨‹è¦ 64 ä¸ªå¯„å­˜å™¨ï¼š
  - ä¸€ä¸ª block æœ‰ 256 çº¿ç¨‹ â†’ éœ€è¦ 256 Ã— 64 = 16,384 ä¸ªå¯„å­˜å™¨
  - æœ€å¤šèƒ½æ”¾ 64K / 16,384 â‰ˆ 4 ä¸ª block
- å¦‚æœæ¯ä¸ªçº¿ç¨‹è¦ 128 ä¸ªå¯„å­˜å™¨ï¼š
  - ä¸€ä¸ª block å°±éœ€è¦ 256 Ã— 128 = 32,768 ä¸ªå¯„å­˜å™¨
  - æœ€å¤šåªèƒ½æ”¾ 2 ä¸ª block
     ğŸ‘‰ **å¯„å­˜å™¨ç”¨å¾—è¶Šå¤šï¼Œèƒ½å¹¶å‘çš„ block è¶Šå°‘ï¼Œoccupancy è¶Šä½**ã€‚

------

#### 3ï¸âƒ£ Register Spill åˆ° Local Memory

å½“ **å•çº¿ç¨‹å¯„å­˜å™¨éœ€æ±‚è¶…è¿‡ç¼–è¯‘å™¨å¯åˆ†é…ä¸Šé™** æ—¶ï¼Œå¤šå‡ºæ¥çš„å¯„å­˜å™¨ä¼šâ€œæº¢å‡º (spill)â€åˆ° **local memory**ã€‚

âš ï¸ æ³¨æ„ï¼š

- **local memory** å¹¶ä¸æ˜¯å¯„å­˜å™¨çš„ä¸€éƒ¨åˆ†ï¼Œè€Œæ˜¯æ˜¾å­˜çš„ä¸€æ®µåŒºåŸŸã€‚
- è®¿é—®å¯„å­˜å™¨å‡ ä¹æ˜¯ 1 cycleï¼Œè€Œè®¿é—® local memory éœ€è¦èµ°æ˜¾å­˜ï¼ˆä¸Šç™¾ä¸ª cycleï¼Œå¯èƒ½è¢« L1/L2 ç¼“å­˜ç¼“è§£ï¼‰ã€‚
- è¿™ä¼šä¸¥é‡æ‹–æ…¢ kernelã€‚

------

#### 4ï¸âƒ£ å½±å“æ€»ç»“

- **å¯„å­˜å™¨è¿‡å°‘**ï¼šå¯èƒ½å¯¼è‡´åå¤è®¿å­˜ï¼ˆæ€§èƒ½å·®ï¼‰ã€‚
- **å¯„å­˜å™¨é€‚ä¸­**ï¼šèƒ½è®© occupancy è¾ƒé«˜ï¼Œä¿æŒååé‡ã€‚
- **å¯„å­˜å™¨è¿‡å¤š**ï¼š
  1. é™ä½ occupancyï¼ˆSM ä¸ŠåŒæ—¶é©»ç•™çš„çº¿ç¨‹å‡å°‘ï¼‰ã€‚
  2. å¯èƒ½è§¦å‘ register spill â†’ æœ¬åœ°å†…å­˜è®¿é—®ï¼Œå»¶è¿Ÿæ˜¾è‘—å¢åŠ ã€‚

------

#### 5ï¸âƒ£ å¦‚ä½•è§‚æµ‹å’Œä¼˜åŒ–

1. **ç”¨ `nvcc` æŸ¥çœ‹å¯„å­˜å™¨ä½¿ç”¨æƒ…å†µ**

   ```
   nvcc -Xptxas -v my_kernel.cu -o my_kernel
   ```

   è¾“å‡ºç¤ºä¾‹ï¼š

   ```
   ptxas info    : Used 64 registers, 48 bytes smem, 0 bytes cmem[0]
   ```

2. **ç”¨ Nsight Compute** æŸ¥çœ‹ï¼š

   - Registers per Thread
   - Achieved Occupancy
   - Local Memory Access

3. **ä¼˜åŒ–æ–¹æ³•**ï¼š

   - é‡æ„ kernelï¼Œå‡å°‘ä¸´æ—¶å˜é‡ã€‚

   - ç”¨ `--maxrregcount=N` é™åˆ¶å¯„å­˜å™¨æ•°ï¼Œå¼ºåˆ¶ç¼–è¯‘å™¨å‡å°‘ä½¿ç”¨ã€‚

     > ä½†è¦å°å¿ƒï¼Œè®¾å¾—å¤ªå°ä¼šå¢åŠ  spillã€‚

   - åˆç†é€‰æ‹© blockDimï¼Œé¿å…è¿‡åº¦ä½¿ç”¨å¯„å­˜å™¨ã€‚

   - å°½é‡ä½¿ç”¨ shared memory æ›¿ä»£ä¸€äº›å¯„å­˜å™¨å‹åŠ›ã€‚

------

âœ… æ€»ç»“ä¸€å¥ï¼š
 **å¯„å­˜å™¨æ˜¯æœ€å¿«çš„èµ„æºï¼Œä½†æœ‰é™ã€‚ç”¨å¤ªå¤šä¼šé™ä½ occupancyï¼Œç”šè‡³ spill åˆ°æœ¬åœ°å†…å­˜ï¼Œå¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚CUDA ä¼˜åŒ–æ—¶è¦åœ¨â€œå¯„å­˜å™¨æ•°é‡ vs å¹¶å‘åº¦ vs spill å¼€é”€â€ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚**

### 4. block ç»´åº¦ï¼ˆ1D/2D/3Dï¼‰é€‰æ‹©çš„ä¾æ®ï¼Ÿ

#### 1ï¸âƒ£ ç»´åº¦ä¸æ•°æ®ç»“æ„çš„æ˜ å°„å…³ç³»

- **1D Block**
  - é€‚åˆä¸€ç»´æ•°æ®ï¼ˆå‘é‡ã€æ•°ç»„ï¼‰ã€‚
  - çº¿ç¨‹ç´¢å¼•è®¡ç®—ç®€å•ï¼š`i = blockIdx.x * blockDim.x + threadIdx.x`ã€‚
  - ç”¨äºå‘é‡åŠ æ³•ã€å‘é‡å½’çº¦ã€1D å·ç§¯ç­‰ã€‚
- **2D Block**
  - é€‚åˆäºŒç»´æ•°æ®ï¼ˆçŸ©é˜µã€å›¾åƒï¼‰ã€‚
  - ç´¢å¼•ç›´è§‚ï¼š`r = blockIdx.y * blockDim.y + threadIdx.y; c = blockIdx.x * blockDim.x + threadIdx.x`ã€‚
  - æ–¹ä¾¿è¡Œåˆ—å¯¹åº”ï¼Œå‡å°‘æ‰‹åŠ¨å±•å¼€è®¡ç®—ã€‚
  - å¸¸ç”¨äºçŸ©é˜µåŠ æ³•/ä¹˜æ³•ã€å›¾åƒå¤„ç†ã€å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„ feature mapã€‚
- **3D Block**
  - é€‚åˆä¸‰ç»´æ•°æ®ï¼ˆä½“æ•°æ®ã€3D ç½‘æ ¼ã€è§†é¢‘å¸§å †å ï¼‰ã€‚
  - ç´¢å¼•ï¼š`z = blockIdx.z * blockDim.z + threadIdx.z`ã€‚
  - åº”ç”¨ï¼šåŒ»å­¦ CT/MRI ä¸‰ç»´å·ç§¯ã€æµä½“æ¨¡æ‹Ÿã€3D ä½“æ¸²æŸ“ã€‚

ğŸ‘‰ **ç»éªŒæ³•åˆ™**ï¼šblock ç»´åº¦å°½é‡ä¸æ•°æ®ç»´åº¦ä¿æŒä¸€è‡´ï¼Œä»£ç æ›´ç›´è§‚ã€è®¿å­˜æ›´è§„æ•´ã€‚

------

#### 2ï¸âƒ£ ç¡¬ä»¶çº¦æŸ

- **æ¯ä¸ª block çš„æœ€å¤§çº¿ç¨‹æ•° â‰¤ 1024**ï¼ˆå¤§å¤šæ•°æ¶æ„ï¼‰ã€‚
- **æ¯ä¸ªç»´åº¦çš„æœ€å¤§é•¿åº¦æœ‰é™åˆ¶**ï¼š
  - blockDim.x â‰¤ 1024
  - blockDim.y â‰¤ 1024
  - blockDim.z â‰¤ 64
- å› æ­¤ï¼š
  - 1D block å¸¸è§é…ç½®ï¼š128, 256, 512, 1024 çº¿ç¨‹ã€‚
  - 2D block å¸¸è§é…ç½®ï¼š16Ã—16 = 256ï¼Œ32Ã—32 = 1024ã€‚
  - 3D block å¸¸è§é…ç½®ï¼š8Ã—8Ã—8 = 512ï¼Œ16Ã—8Ã—8 = 1024ã€‚

------

#### 3ï¸âƒ£ å†…å­˜è®¿é—®æ¨¡å¼ï¼ˆCoalescingï¼‰

CUDA çš„å…¨å±€å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ä¾èµ– **warp å†…çº¿ç¨‹æ˜¯å¦é¡ºåºè®¿é—®å†…å­˜**ã€‚

- **1D block**ï¼šå¤©ç„¶å¯¹é½ï¼ˆè¿ç»­æ•°ç»„ â†’ çº¿ç¨‹è¿ç»­è®¿é—®ï¼‰ã€‚
- **2D block**ï¼šå¦‚æœçŸ©é˜µæŒ‰è¡Œå­˜å‚¨ï¼ˆrow-majorï¼‰ï¼Œä¿è¯ `threadIdx.x` å˜åŒ–å¿«ï¼Œè®¿é—®è¿ç»­å†…å­˜ã€‚
- **3D block**ï¼šéœ€è¦ä¿è¯ innermost ç»´åº¦ï¼ˆx æ–¹å‘ï¼‰å¯¹åº”è¿ç»­å†…å­˜ï¼Œå¦åˆ™ä¼šæœ‰è®¿å­˜ä¸å¯¹é½é—®é¢˜ã€‚

ğŸ‘‰ é€‰æ‹© block ç»´åº¦æ—¶ï¼Œè¦ç¡®ä¿ **warp å†… 32 ä¸ªçº¿ç¨‹è®¿é—®ç›¸é‚»å†…å­˜**ï¼Œæ‰èƒ½å®ç°æœ€ä½³å¸¦å®½åˆ©ç”¨ã€‚

------

#### 4ï¸âƒ£ Occupancy ä¸å¯„å­˜å™¨å‹åŠ›

- block å¤§å°ä¸èƒ½ä¸€å‘³æ±‚å¤§ï¼š
  - å¤ªå¤§ â†’ å ç”¨è¿‡å¤šå¯„å­˜å™¨ / shared memory â†’ é™ä½ occupancyã€‚
  - å¤ªå° â†’ GPU æ ¸å¿ƒåˆ©ç”¨ç‡ä¸è¶³ã€‚
- ä¸€èˆ¬æ¨èï¼š
  - **128â€“512 çº¿ç¨‹/block**ï¼Œåœ¨å¤§å¤šæ•° GPU ä¸Šèƒ½å–å¾—è¾ƒå¥½å¹³è¡¡ã€‚
  - å¦‚æœç”¨ 2D/3D blockï¼Œå°½é‡è®©æ€»çº¿ç¨‹æ•°è½åœ¨è¿™ä¸ªèŒƒå›´ï¼ˆå¦‚ 16Ã—16ã€8Ã—8Ã—8ï¼‰ã€‚

------

#### 5ï¸âƒ£ å·¥ç¨‹å®è·µç»éªŒ

- **1D block**ï¼šå‘é‡è¿ç®—ï¼ˆç‚¹ç§¯ã€å‘é‡åŠ å‡ã€å‰ç¼€å’Œï¼‰ã€‚
- **2D block**ï¼šçŸ©é˜µè¿ç®—ã€å›¾åƒå·ç§¯ï¼ˆCNNã€æ»¤æ³¢ã€Poolingï¼‰ã€‚
- **3D block**ï¼š3D å·ç§¯ã€æµä½“æ¨¡æ‹Ÿã€3D æ¸²æŸ“ã€‚

å¦‚æœæ˜¯ **æ··åˆç»´åº¦æ•°æ®**ï¼ˆå¦‚ batch Ã— channel Ã— height Ã— widthï¼‰ï¼Œé€šå¸¸ä¼šï¼š

- æŠŠ **å†…å­˜å¸ƒå±€å’Œ block ç»´åº¦åŒ¹é…**ï¼Œé€‰æ‹©æœ€å¿«å˜åŒ–çš„ç»´åº¦å¯¹åº” `threadIdx.x`ã€‚
- å…¶ä½™ç»´åº¦æ˜ å°„åˆ° `threadIdx.y` / `threadIdx.z`ã€‚

------

#### âœ… æ€»ç»“å£è¯€

> **å‡ ç»´æ•°æ®å°±ä¼˜å…ˆç”¨å‡ ç»´ blockï¼Œä¿è¯ warp å†…å­˜ coalescingï¼›
>  ä¿æŒ 128â€“512 çº¿ç¨‹/blockï¼Œæ—¢èƒ½å……åˆ†åˆ©ç”¨ GPUï¼Œåˆä¸è‡³äºå¯„å­˜å™¨å‹åŠ›è¿‡å¤§ã€‚**

### 5. `__launch_bounds__` å¯¹ç¼–è¯‘å™¨å¯„å­˜å™¨åˆ†é…ä¸æ€§èƒ½çš„å½±å“ï¼Ÿ

CUDA æä¾›äº† **kernel ä¿®é¥°ç¬¦** `__launch_bounds__(maxThreadsPerBlock, minBlocksPerSM)`ï¼š

```
__global__ __launch_bounds__(maxThreadsPerBlock, minBlocksPerSM)
void my_kernel(...) { ... }
```

å®ƒå‘Šè¯‰ç¼–è¯‘å™¨ï¼š

1. **`maxThreadsPerBlock`**ï¼šè¯¥ kernel **æ¯ä¸ª block çš„çº¿ç¨‹æ•°ä¸Šé™**ã€‚
   - ç¼–è¯‘å™¨å¯æ®æ­¤é™åˆ¶å¯„å­˜å™¨åˆ†é…ï¼Œä¿è¯å³ä½¿çº¿ç¨‹æ•°ç­‰äºè¿™ä¸ªä¸Šé™æ—¶ï¼Œä¹Ÿèƒ½å¹¶å‘æ‰§è¡Œã€‚
2. **`minBlocksPerSM`ï¼ˆå¯é€‰ï¼‰**ï¼šæœŸæœ›æ¯ä¸ª SM è‡³å°‘å¹¶è¡Œè¿è¡Œå¤šå°‘ä¸ª blockã€‚
   - ç¼–è¯‘å™¨ä¼šè¿›ä¸€æ­¥çº¦æŸå¯„å­˜å™¨æ•°é‡ï¼Œä»¥æ»¡è¶³è¿™ä¸ªå¹¶å‘åº¦ã€‚

------

#### ğŸ§© å¯¹å¯„å­˜å™¨åˆ†é…çš„å½±å“

GPU æ¯ä¸ª SM çš„ç‰©ç†å¯„å­˜å™¨æ•°é‡æ˜¯æœ‰é™çš„ï¼Œä¾‹å¦‚ï¼š

- A100: 64K (65536) ä¸ª 32-bit å¯„å­˜å™¨
- RTX 30xx: 64K ä¸ª

å¯„å­˜å™¨åˆ†é…å…³ç³»ï¼š

```
å¯„å­˜å™¨å ç”¨ = æ¯çº¿ç¨‹å¯„å­˜å™¨æ•° Ã— æ¯ block çº¿ç¨‹æ•°
å¯å¹¶å‘ block æ•° = SMå¯„å­˜å™¨æ€»æ•° / æ¯ block å¯„å­˜å™¨éœ€æ±‚
```

------

##### 1ï¸âƒ£ å¦‚æœä¸åŠ  `__launch_bounds__`

- ç¼–è¯‘å™¨ä¼šå€¾å‘äº **ä¼˜åŒ–å¯„å­˜å™¨ä½¿ç”¨**ï¼Œå¯èƒ½è®©æ¯çº¿ç¨‹ç”¨å¾ˆå¤šå¯„å­˜å™¨ï¼ˆæ¯”å¦‚ 128ï¼‰ã€‚
- ç¼ºç‚¹ï¼š
  - æ¯ä¸ª block å¯„å­˜å™¨å ç”¨è¿‡å¤§ï¼Œå¯¼è‡´ **å¯å¹¶å‘ block æ•°å‡å°‘** â†’ occupancy ä½ã€‚
  - æœ‰æ—¶ä¸€ä¸ª SM ä¸Šåªèƒ½è·‘ 1 ä¸ª blockã€‚

------

##### 2ï¸âƒ£ å¦‚æœåŠ äº† `__launch_bounds__`

- ç¼–è¯‘å™¨ä¼šè¢«è¿« **å‡å°‘å•çº¿ç¨‹å¯„å­˜å™¨åˆ†é…**ï¼Œä»¥æ»¡è¶³æœŸæœ›çš„å¹¶å‘åº¦ã€‚

- ä¾‹å­ï¼š

  ```
  __launch_bounds__(256, 2)
  ```

  å«ä¹‰ï¼š

  - kernel æœ€å¤šä¼šç”¨ 256 çº¿ç¨‹/blockã€‚

  - è¦æ±‚ç¼–è¯‘å™¨ä¿è¯ **è‡³å°‘ 2 ä¸ª block/SM å¹¶å‘**ã€‚

  - å¦‚æœ SM æœ‰ 64K å¯„å­˜å™¨ï¼š

    ```
    64K / (256 Ã— 2) = æ¯çº¿ç¨‹æœ€å¤š 128 å¯„å­˜å™¨
    ```

  - ç¼–è¯‘å™¨å¯èƒ½ä¼šä¼˜åŒ–æˆ ~64 å¯„å­˜å™¨/çº¿ç¨‹ï¼Œä»¥ç•™å‡ºä½™é‡ã€‚

------

#### âš–ï¸ æ€§èƒ½å½±å“

##### ğŸ‘ ä¼˜ç‚¹

- å¯ä»¥äººä¸ºçº¦æŸç¼–è¯‘å™¨ï¼Œ**é¿å…å¯„å­˜å™¨è¿‡åº¦è†¨èƒ€**ã€‚
- å¢åŠ  **occupancy**ï¼Œæå‡å¹¶è¡Œåº¦å’Œååé‡ã€‚
- å¯¹è®¿å­˜å‹ kernelï¼ˆmemory boundï¼‰æ•ˆæœæ˜æ˜¾ï¼Œå› ä¸º occupancy é«˜äº†èƒ½æ›´å¥½åœ°æ©ç›–è®¿å­˜å»¶è¿Ÿã€‚

##### ğŸ‘ ç¼ºç‚¹

- å¦‚æœå¯„å­˜å™¨æ•°è¢«å‹ç¼©å¤ªç‹  â†’ ä¼šå‘ç”Ÿ **register spill**ï¼ˆå¯„å­˜å™¨æº¢å‡ºåˆ° local memoryï¼‰ã€‚
- è®¿é—® local memory å»¶è¿Ÿå¾ˆé«˜ï¼ˆå‡ ç™¾ cycleï¼‰ï¼Œåè€Œæ‹–æ…¢æ€§èƒ½ã€‚
- å¯¹è®¡ç®—å‹ kernelï¼ˆcompute boundï¼‰ï¼Œå¯èƒ½é™ä½å•çº¿ç¨‹æ•ˆç‡ã€‚

------

#### ğŸ”¬ å®é™…åº”ç”¨åœºæ™¯

- **éœ€è¦é«˜ occupancy çš„å†…æ ¸**ï¼ˆæ¯”å¦‚æœ‰å¤§é‡å…¨å±€å†…å­˜è®¿é—®çš„ kernelï¼‰ï¼š
   ç”¨ `__launch_bounds__` æå‡å¹¶å‘åº¦ï¼Œæ©ç›–å†…å­˜å»¶è¿Ÿã€‚
- **è®¡ç®—å¯†é›†å‹å†…æ ¸**ï¼ˆä¾‹å¦‚çŸ©é˜µä¹˜æ³•ã€å·ç§¯ï¼‰ï¼š
   å¯èƒ½å®æ„¿è®©å•çº¿ç¨‹ç”¨æ›´å¤šå¯„å­˜å™¨ï¼ˆé¿å… spillï¼‰ï¼Œè€Œä¸æ˜¯å¼ºè¡Œæé«˜ occupancyã€‚

ğŸ‘‰ æ‰€ä»¥ï¼Œ**`__launch_bounds__` æ˜¯ä¸€ç§â€œåˆ€åˆƒâ€**ï¼Œè¦ç»“åˆ profile å·¥å…·ï¼ˆNsight Computeï¼‰æµ‹è¯•ï¼š

- Registers Per Thread
- Achieved Occupancy
- Local Memory Access

------

#### âœ… æ€»ç»“ä¸€å¥

`__launch_bounds__` æ˜¯ CUDA ç¼–è¯‘å™¨çš„ä¼˜åŒ– hintï¼š

- **å¼ºåˆ¶ç¼–è¯‘å™¨é™ä½å¯„å­˜å™¨åˆ†é…**ï¼Œä»¥ä¿è¯ä¸€å®šçš„ occupancyã€‚
- **å¥½å¤„**ï¼šæå‡å¹¶è¡Œåº¦ï¼Œæ©ç›–è®¿å­˜å»¶è¿Ÿã€‚
- **é£é™©**ï¼šå¯„å­˜å™¨è¿‡å°‘å¯èƒ½å¯¼è‡´ spillï¼Œæœ¬åœ°å†…å­˜è®¿é—®æ‹–æ…¢æ€§èƒ½ã€‚

### 6. `nvcc -O2/-O3` ä¸ `--use_fast_math` çš„é£é™©ï¼Ÿ

#### 1ï¸âƒ£ `-O2` / `-O3` ä¼˜åŒ–é£é™©

##### ğŸš€ åšäº†ä»€ä¹ˆ

- `-O2`ï¼šå¸¸è§„ä¼˜åŒ–ï¼Œå¯ç”¨å¤§å¤šæ•°å®‰å…¨ä¼˜åŒ–ï¼ˆå¾ªç¯å±•å¼€ã€å¸¸é‡ä¼ æ’­ã€å¯„å­˜å™¨åˆ†é…ä¼˜åŒ–ï¼‰ã€‚
- `-O3`ï¼šæ¯” `-O2` æ›´æ¿€è¿›ï¼Œå¢åŠ äº† **å‘é‡åŒ–/å¾ªç¯èåˆ/å‡½æ•°å†…è”** ç­‰ã€‚

##### âš ï¸ é£é™©

1. **æµ®ç‚¹æ•°è¿ç®—é‡æ’**

   - ç¼–è¯‘å™¨å¯èƒ½æ”¹å˜è®¡ç®—é¡ºåºï¼Œä»¥å‡å°‘è®¿å­˜æˆ–æé«˜å¹¶è¡Œåº¦ã€‚

   - ä½†æµ®ç‚¹æ•°åŠ å‡æ³• **ä¸æ»¡è¶³ç»“åˆå¾‹**ï¼Œç»“æœå¯èƒ½å‡ºç°å¾®å°å·®å¼‚ã€‚

   - ä¾‹å¦‚ï¼š

     ```
     float x = (a+b)+c;   // ç²¾ç¡®
     float y = a+(b+c);   // O3 å¯èƒ½é‡æ’
     ```

     â†’ æ•°å€¼å·®å¼‚åœ¨ 1e-6 ~ 1e-8 é‡çº§ã€‚

2. **æ¿€è¿›çš„å¾ªç¯ä¼˜åŒ–**

   - `-O3` ä¼šå°è¯•å±•å¼€å’Œé‡æ–°æ’å¸ƒå¾ªç¯ã€‚
   - æŸäº›å†…å­˜è®¿é—®æ¨¡å¼å¯èƒ½å› ä¸ºå±•å¼€åå¯„å­˜å™¨å‹åŠ›è¿‡å¤§ï¼Œå¯¼è‡´ **spill åˆ° local memory**ï¼Œåè€Œå˜æ…¢ã€‚

3. **è°ƒè¯•å›°éš¾**

   - åœ¨ `-O3` ä¸‹ï¼Œç¼–è¯‘å™¨å¯èƒ½å†…è”/ä¼˜åŒ–æ‰æŸäº›å˜é‡ï¼Œä½¿å¾—è°ƒè¯•ä¿¡æ¯ç¼ºå¤±ã€‚
   - `cuda-gdb`/`printf` è°ƒè¯•ä¼šæ›´éš¾ç”¨ã€‚

ğŸ‘‰ å»ºè®®ï¼š

- ç”Ÿäº§ç¯å¢ƒå¸¸ç”¨ `-O2`ï¼Œæ€§èƒ½å’Œç¨³å®šæ€§å¹³è¡¡ã€‚
- `-O3` è¦ç»“åˆ Nsight Compute/Systems å®æµ‹ï¼Œé¿å…å¯„å­˜å™¨çˆ†ç‚¸ã€‚

------

#### 2ï¸âƒ£ `--use_fast_math` çš„é£é™©

##### ğŸš€ åšäº†ä»€ä¹ˆ

- å°†æ•°å­¦å‡½æ•°æ›¿æ¢ä¸º **å¿«é€Ÿè¿‘ä¼¼å®ç°**ï¼ˆå¦‚ `rsqrtf` ä»£æ›¿ `1/sqrtf`ï¼‰ã€‚
- å¯ç”¨ç¼–è¯‘é€‰é¡¹ï¼š
  - `--ftz=true`ï¼ˆflush-to-zeroï¼Œå°æ•°æ¥è¿‘ 0 çš„å€¼ç›´æ¥ç½® 0ï¼‰
  - `--prec-div=false`ï¼ˆè¿‘ä¼¼é™¤æ³•ï¼‰
  - `--prec-sqrt=false`ï¼ˆè¿‘ä¼¼å¹³æ–¹æ ¹ï¼‰

##### âš ï¸ é£é™©

1. **æ•°å€¼ç²¾åº¦ä¸‹é™**
   - ç»“æœè¯¯å·®é€šå¸¸åœ¨ **1e-4 ~ 1e-6** ä¹‹é—´ã€‚
   - åœ¨æ·±åº¦å­¦ä¹ æ¨ç†ä¸­å½±å“ä¸å¤§ï¼ˆæ¨¡å‹å¯¹å™ªå£°é²æ£’ï¼‰ã€‚
   - åœ¨ç§‘å­¦è®¡ç®—/é‡‘èä»¿çœŸä¸­å¯èƒ½å®Œå…¨ä¸å¯æ¥å—ã€‚
2. **ä¸å¯ç§»æ¤æ€§**
   - ä¸åŒ GPU æ¶æ„çš„è¿‘ä¼¼å®ç°å¯èƒ½ç•¥æœ‰å·®åˆ«ï¼Œå¯¼è‡´è·¨å¹³å°ç»“æœä¸ä¸€è‡´ã€‚
3. **éšè—çš„æ•°å€¼é”™è¯¯**
   - `--ftz=true` ä¼šæŠŠ denormalï¼ˆéå¸¸å°çš„æ•°ï¼Œ1e-38 çº§åˆ«ï¼‰ç›´æ¥ç½®ä¸º 0ã€‚
   - å¦‚æœç¨‹åºä¾èµ–äºè¿™äº›å°æ•°ï¼ˆå¦‚è¿­ä»£ç®—æ³•ã€ç¨³å®šæ€§åˆ†æï¼‰ï¼Œå¯èƒ½å¯¼è‡´ **æ”¶æ•›å¤±è´¥**ã€‚

------

#### 3ï¸âƒ£ ç»¼åˆæ¯”è¾ƒ

| é€‰é¡¹              | é£é™©ç­‰çº§ | é£é™©ç‚¹                           | é€‚ç”¨åœºæ™¯                   |
| ----------------- | -------- | -------------------------------- | -------------------------- |
| `-O2`             | ä½       | ç¨å¾®æ”¹å˜æµ®ç‚¹è®¡ç®—é¡ºåºï¼Œç¨³å®šæ€§å¥½   | é»˜è®¤æ¨è                   |
| `-O3`             | ä¸­ç­‰     | æµ®ç‚¹é‡æ’ã€å¯„å­˜å™¨å‹åŠ›å¤§ã€è°ƒè¯•å›°éš¾ | éœ€ Profile éªŒè¯            |
| `--use_fast_math` | é«˜       | ç²¾åº¦æŸå¤±ã€FTZã€ä¸å¯ç§»æ¤æ€§        | æ¨ç†/æ¸²æŸ“ OKï¼Œç§‘å­¦è®¡ç®—å±é™© |

------

#### âœ… å»ºè®®

- **å¼€å‘/è°ƒè¯•**ï¼š`-O0` æˆ– `-O1`ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼Œå˜é‡ä¸ä¼šè¢«ä¼˜åŒ–æ‰ï¼‰ã€‚
- **å¸¸è§„ä¼˜åŒ–**ï¼šç”¨ `-O2`ï¼Œè¶³å¤Ÿç¨³å®šã€‚
- **æé™ä¼˜åŒ–**ï¼šå°è¯• `-O3` + Profileï¼Œç¡®è®¤å¯„å­˜å™¨æ•°å’Œ occupancy æ˜¯å¦åˆç†ã€‚
- **è¿½æ±‚é€Ÿåº¦**ï¼š`-O2 --use_fast_math`ï¼Œä½†è¦éªŒè¯è¯¯å·®æ˜¯å¦èƒ½æ¥å—ã€‚

## 6ï¸âƒ£ å®éªŒ

### ğŸ§ª å®éªŒ 1ï¼šGrid-Stride Loop vs å¤§ Grid

#### ğŸ‘‰ èƒŒæ™¯ï¼š

ä¹‹å‰ `vector_add` æ˜¯ä¸€æ¬¡æ€§å¼€è¶³å¤Ÿå¤§çš„ grid è¦†ç›–æ‰€æœ‰å…ƒç´ ã€‚
 Grid-Stride Loop å…è®¸ä½ ç”¨è¾ƒå°çš„ gridï¼Œå°±èƒ½å¤„ç†ä»»æ„é•¿åº¦çš„æ•°æ®ï¼Œæ›´å…·æ‰©å±•æ€§ã€‚

#### ä»£ç ï¼š`vector_add_gridstride.cu`

```c++
#include <stdio.h>
#include <cuda_runtime.h>

// grid-stride loop å†™æ³•
__global__ void vector_add_gridstride(const float *a, const float *b, float *c, int n) {
    for (int i = blockIdx.x * blockDim.x + threadIdx.x;
         i < n;
         i += blockDim.x * gridDim.x) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    int n = 1 << 20; // 1M å…ƒç´ 
    size_t bytes = n * sizeof(float);

    float *h_a = (float*)malloc(bytes);
    float *h_b = (float*)malloc(bytes);
    float *h_c = (float*)malloc(bytes);
    for (int i = 0; i < n; i++) { h_a[i] = 1.0f; h_b[i] = 2.0f; }

    float *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, bytes); cudaMalloc(&d_b, bytes); cudaMalloc(&d_c, bytes);
    cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);

    // æ¯”è¾ƒä¸¤ç§å†™æ³•
    int blockSize = 256;
    int gridSize_small = 32;  // Grid-Stride Loop ç”¨å° grid
    int gridSize_big   = (n + blockSize - 1) / blockSize;  // ä¼ ç»Ÿå¤§ grid

    // è®¡æ—¶ç”¨äº‹ä»¶
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    // Grid-Stride Loop
    cudaEventRecord(start);
    vector_add_gridstride<<<gridSize_small, blockSize>>>(d_a, d_b, d_c, n);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    float ms1; cudaEventElapsedTime(&ms1, start, stop);

    // å•å‘å¤§ grid
    cudaEventRecord(start);
    vector_add_gridstride<<<gridSize_big, blockSize>>>(d_a, d_b, d_c, n);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    float ms2; cudaEventElapsedTime(&ms2, start, stop);

    printf("Grid-Stride Loop: %.3f ms\n", ms1);
    printf("One Big Grid:    %.3f ms\n", ms2);

    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);
    free(h_a); free(h_b); free(h_c);
    return 0;
}
```

#### ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 vector_add_gridstride.cu -o vec_gs
./vec_gs
```

#### é¢„æœŸæ•ˆæœ

ä¸¤ç§æ–¹å¼ç»“æœæ­£ç¡®

**Grid-Stride Loop** åœ¨ grid æ•°é‡ä¸è¶³æ—¶ä¹Ÿèƒ½è¦†ç›–å¤§æ•°ç»„ï¼Œæ›´æœ‰æ‰©å±•æ€§ã€‚

![image-20250831022922538](./report_day8.assets/image-20250831022922538.png)

------

### ğŸ§ª å®éªŒ 2ï¼šè¶Šç•Œè®¿é—® + `compute-sanitizer`

#### ğŸ‘‰ èƒŒæ™¯ï¼š

 GPU kernel å¦‚æœè¶Šç•Œè®¿é—®æ•°ç»„ï¼Œç»“æœå¯èƒ½æ˜¯â€œé™é»˜é”™è¯¯â€ï¼ˆä¸å´©æºƒä½†æ•°æ®é”™è¯¯ï¼‰ã€‚
 CUDA æä¾› `compute-sanitizer ` å·¥å…·æ•è·è¿™ç§ bugã€‚

#### ä¿®æ”¹ `vector_add.cu` å†…æ ¸

```c++
__global__ void vector_add(const float *a, const float *b, float *c, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    // âŒ å»æ‰ if(i<n) çš„æ£€æŸ¥ï¼Œå¼ºåˆ¶è¶Šç•Œ
    c[i] = a[i] + b[i];
}


int blockSize = 256;
int gridSize = (n + blockSize - 1) / blockSize + 10;  // æ•…æ„å¤šåŠ  10 ä¸ª block
```

#### ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 vector_add.cu -o vec_err
compute-sanitizer ./vec_err
```

#### é¢„æœŸæ•ˆæœ

ä¼šæŠ¥é”™ç±»ä¼¼ï¼š

![image-20250831033147740](./report_day8.assets/image-20250831033147740.png)

ğŸ‘‰ è¯´æ˜ `compute-sanitizer` èƒ½å¸®ä½ æ•è· GPU å†…å­˜é”™è¯¯ã€‚

------

### ğŸ§ª å®éªŒ 3ï¼šå¢å¤§ blockDimï¼Œè§‚å¯Ÿå¯„å­˜å™¨ä¸ Occupancy

ğŸ‘‰ èƒŒæ™¯ï¼š
 block å¤ªå¤§å¯èƒ½å¯¼è‡´å¯„å­˜å™¨å‹åŠ›ï¼Œoccupancyï¼ˆå¹¶å‘åº¦ï¼‰ä¸‹é™ã€‚

#### å·¥å…·ï¼šNsight Compute

```bash
nvcc -O2 vector_add.cu -o vec
ncu --metrics launch__registers_per_thread,sm__warps_active.avg.pct_of_peak_sustained_active ./vec
```

åœ¨æŠ¥å‘Šä¸­å…³æ³¨ï¼š

- **Registers Per Thread**ï¼ˆå¯„å­˜å™¨ä½¿ç”¨é‡ï¼‰
- **Achieved Occupancy**ï¼ˆå®é™…å¹¶å‘åº¦ï¼‰

#### å®éªŒæ­¥éª¤

1. ä¿®æ”¹ `blockSize = 64, 128, 256, 512, 1024`
2. æ¯æ¬¡è¿è¡Œ`ncu --metrics launch__registers_per_thread,sm__warps_active.avg.pct_of_peak_sustained_active  ./vec`
3. æ¯”è¾ƒå¯„å­˜å™¨æ•°é‡ä¸ Occupancy çš„å˜åŒ–è¶‹åŠ¿ã€‚

![image-20250831040649732](./report_day8.assets/image-20250831040649732.png)

![image-20250831040458770](./report_day8.assets/image-20250831040458770.png)

![image-20250831040722696](./report_day8.assets/image-20250831040722696.png)

![image-20250831040746637](./report_day8.assets/image-20250831040746637.png)

![image-20250831040812797](./report_day8.assets/image-20250831040812797.png)

#### ğŸ“Š å®éªŒæ•°æ®æ•´ç†

| blockSize | Time (ms) | Registers / Thread | Achieved Occupancy (%) |
| --------- | --------- | ------------------ | ---------------------- |
| 64        | 318.7     | 16                 | 45.3                   |
| 128       | 342.9     | 16                 | 52.1                   |
| 256       | 330.2     | 16                 | 55.1                   |
| 512       | 367.9     | 16                 | 55.1                   |
| 1024      | 340.3     | 16                 | 60.1                   |

------

### ğŸ§ª å®éªŒ 4ï¼š1D vs 2D Block è®¿é—®äºŒç»´æ•°ç»„

ğŸ‘‰ èƒŒæ™¯ï¼š
 äºŒç»´æ•°æ®ï¼ˆçŸ©é˜µï¼‰ç”¨ 2D block æ›´ç›´è§‚ï¼Œå¯è¯»æ€§æ›´å¥½ï¼Œè€Œä¸”èƒ½ä¼˜åŒ– **å†…å­˜ coalescingï¼ˆåˆå¹¶è®¿å­˜ï¼‰**ã€‚

#### 1ï¸âƒ£ å®éªŒç›®æ ‡

- å¯¹æ¯” **1D block** å’Œ **2D block** åœ¨çŸ©é˜µåŠ æ³•ä¸­çš„å†™æ³•å’Œæ‰§è¡Œæ•ˆæœã€‚
- ç†è§£ä¸ºä»€ä¹ˆ 2D block æ›´è‡ªç„¶ï¼Œä¸”æœ‰åˆ©äºå†…å­˜ coalescingï¼ˆåˆå¹¶è®¿å­˜ï¼‰ã€‚

------

#### 2ï¸âƒ£ å®Œæ•´ä»£ç ï¼š`matrix_add.cu`

```c++
#include <stdio.h>
#include <cuda_runtime.h>

// 1D Block çš„çŸ©é˜µåŠ æ³•
__global__ void mat_add_1D(const float *A, const float *B, float *C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < N * N) {
        C[i] = A[i] + B[i];
    }
}

// 2D Block çš„çŸ©é˜µåŠ æ³•
__global__ void mat_add_2D(const float *A, const float *B, float *C, int N) {
    int r = blockIdx.y * blockDim.y + threadIdx.y;
    int c = blockIdx.x * blockDim.x + threadIdx.x;

    if(r < N && c < N) {
        int idx = r * N + c;// è¡Œä¸»åºå±•å¼€
        C[idx] = A[idx] + B[idx];
    }
}

int main() {
    const int N = 1024; // çŸ©é˜µå¤§å°N * N
    size_t bytes = N * N * sizeof(float);

    // åˆ†é…Hostå†…å­˜
    float *host_a = (float *)malloc(bytes);
    float *host_b = (float *)malloc(bytes);
    float *host_c1d = (float *)malloc(bytes);
    float *host_c2d = (float *)malloc(bytes);

    // åˆå§‹åŒ–çŸ©é˜µæ•°æ®
    for (int i = 0; i < N * N;i++) {
        host_a[i] = 1.0f;
        host_b[i] = 2.0f;
    }

    // åˆ†é…deviceå†…å­˜
    float *device_a, *device_b, *device_c;
    cudaMalloc(&device_a, bytes);
    cudaMalloc(&device_b, bytes);
    cudaMalloc(&device_c, bytes);

    // æ‹·è´æ•°æ®
    cudaMemcpy(device_a, host_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes, cudaMemcpyHostToDevice);

    // é…ç½®1D kernel å¯åŠ¨å‚æ•°
    dim3 block1(256);
    dim3 grid1((N * N + block1.x - 1) / block1.x);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);

    // å¯åŠ¨1D å†…æ ¸
    mat_add_1D<<<grid1, block1>>>(device_a, device_b, device_c, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float c1d_ms;
    cudaEventElapsedTime(&c1d_ms, start, stop);

    cudaMemcpy(host_c1d, device_c, bytes, cudaMemcpyDeviceToHost);

    // é…ç½®2D kernel å¯åŠ¨å‚æ•°
    dim3 block2(16, 16);
    dim3 grid2((N + block2.x - 1) / block2.x, (N + block2.y - 1) / block2.y);

    cudaEventRecord(start);
    // å¯åŠ¨2D å†…æ ¸
    mat_add_2D<<<grid2, block2>>>(device_a, device_b, device_c, N);
    cudaEventRecord(stop);

    cudaEventSynchronize(stop);

    float c2d_ms;
    cudaEventElapsedTime(&c2d_ms, start, stop);
    cudaMemcpy(host_c2d, device_c, bytes, cudaMemcpyDeviceToHost);

    // éªŒè¯ç»“æœ10 ä¸ªå…ƒç´ 
    printf("check results (first 10 elements):\n");
    for (int i = 0; i < 10; i++) {
        printf("C1D[%d]=%.1f C2D[%d]=%.1f\n", i, host_c1d[i], i, host_c2d[i]);
    }

    // æ‰“å°æ€§èƒ½å¯¹æ¯”
    printf("\nPerformance comparison (Matrix %d x %d):\n", N, N);
    printf("1D Block: %.3f ms\n", c1d_ms);
    printf("2D Block: %.3f ms\n", c2d_ms);

    // æ¸…ç†èµ„æº
    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);

    free(host_a);
    free(host_b);
    free(host_c1d);
    free(host_c2d);

    return 0;
}
```

------

#### 3ï¸âƒ£ ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 matrix_add.cu -o mat_add
./mat_add
```

âœ… è¾“å‡ºï¼š

![image-20250831204922936](./report_day8.assets/image-20250831204922936.png)

è¯´æ˜ 1D å’Œ 2D å†™æ³•ç»“æœä¸€è‡´ã€‚

------

#### 4ï¸âƒ£ å¯¹æ¯”ä¸åˆ†æ

##### 1D block

- ç´¢å¼•æ˜¯ `i = blockIdx.x * blockDim.x + threadIdx.x`ã€‚
- æœ¬è´¨ä¸ŠæŠŠäºŒç»´çŸ©é˜µ **æ‹å¹³æˆä¸€ç»´æ•°ç»„**æ¥è®¿é—®ã€‚
- ç¼ºç‚¹ï¼šå¦‚æœè¦å†™çŸ©é˜µæ“ä½œé€»è¾‘ï¼ˆå¦‚çŸ©é˜µä¹˜æ³•ï¼‰ï¼Œè¡Œåˆ—è®¡ç®—ä¸ç›´è§‚ï¼Œå¯è¯»æ€§å·®ã€‚

##### 2D block

- ç´¢å¼•æ˜¯ `(r, c)`ï¼Œç›´æ¥å¯¹åº”çŸ©é˜µçš„è¡Œåˆ—ã€‚
- æ›´æ¥è¿‘æ•°å­¦å®šä¹‰ï¼Œå®¹æ˜“æ‰©å±•åˆ°çŸ©é˜µä¹˜ã€å·ç§¯ç­‰å¤æ‚è¿ç®—ã€‚
- å†…å­˜è®¿é—®æ¨¡å¼æ›´è‡ªç„¶ï¼ˆè¡Œä¸»åºå­˜å‚¨ï¼‰ï¼Œçº¿ç¨‹æ’åˆ—æ–¹å¼æ›´å®¹æ˜“ **å†…å­˜åˆå¹¶è®¿é—®ï¼ˆcoalescingï¼‰**ã€‚

------

### ğŸ§ª å®éªŒ 5ï¼š`--use_fast_math` çš„å½±å“

ğŸ‘‰ èƒŒæ™¯ï¼š
 `--use_fast_math` ä¼šå¯ç”¨å¿«é€Ÿä½†è¿‘ä¼¼çš„æ•°å­¦è¿ç®—ï¼ˆä¾‹å¦‚ç”¨å¿«é€Ÿ `rsqrt` æ›¿ä»£ç²¾ç¡® `sqrt`ï¼‰ã€‚

#### ä»£ç ï¼š`fast_math.cu`

```c++
#include <stdio.h>
#include <math.h>

// ç®€å•æ•°å­¦è¿ç®—kernel
__global__ void test_math(float *out) {
    int i = threadIdx.x;
    float x = i * 0.1f;

    // è°ƒç”¨sinf, cosf, sqrtf è¿™äº›æ•°å­¦å‡½æ•°
    out[i] = sinf(x) + cosf(x) + sqrt(x);
}

int main() {
    const int N = 128;
    float host_out[N], *device_out;

    // åˆ†é…device å†…å­˜
    cudaMalloc(&device_out, N * sizeof(float));

    // å¯åŠ¨kernel
    test_math<<<1, N>>>(device_out);

    // æ‹·å›ç»“æœ
    cudaMemcpy(host_out, device_out, N * sizeof(float), cudaMemcpyDeviceToHost);

    // æ‰“å°å‰åä¸ªå…ƒç´ 
    for (int i = 0; i < 10; i++) {
        printf("out[%d] = %.8f\n", i, host_out[i]);
    }

    cudaFree(device_out);
    return 0;
}
```

#### ç¼–è¯‘å¯¹æ¯”

```bash
nvcc -O2 fast_math.cu -o fm_normal
nvcc -O2 --use_fast_math fast_math.cu -o fm_fast
```

#### è¿è¡Œ

```bash
./fm_normal > normal.txt
./fm_fast   > fast.txt
diff normal.txt fast.txt
```

#### é¢„æœŸæ•ˆæœ

- `--use_fast_math` è¿è¡Œæ›´å¿«ï¼Œä½†ç»“æœå’Œæ ‡å‡†ç‰ˆæœ‰äº›è®¸è¯¯å·®ã€‚
- è¯¯å·®é€šå¸¸åœ¨å°æ•°ç‚¹å 3~5 ä½ï¼Œå¯¹æ·±åº¦å­¦ä¹ æ¨ç†å½±å“ä¸å¤§ï¼Œä½†å¯¹ç§‘å­¦è®¡ç®—å¯èƒ½å±é™©ã€‚

![image-20250831213915426](./report_day8.assets/image-20250831213915426.png)