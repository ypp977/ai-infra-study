# Day 4 â€” TensorRT ä¼˜åŒ–

## ä¸€ã€ğŸ¯ ä»Šæ—¥ç›®æ ‡

- å®‰è£…å¹¶é…ç½® TensorRT
- æŠŠ `mlp.onnx` è½¬æ¢æˆ TensorRT Engine (`.engine`)
- ä½¿ç”¨ Python API åŠ è½½ `.engine` åšæ¨ç†
- å­¦ä¹  FP16/INT8 é‡åŒ–åŠ é€Ÿ

## äºŒã€ç¯å¢ƒå‡†å¤‡

å¯åŠ¨dockerå®¹å™¨

```bash
docker run --gpus all -it --rm \
  -v $PWD:/workspace \
  --name ai-infer-day4 \
  my-ai-infer:lite /bin/bash
```

ä½ ç”¨çš„åŸºç¡€é•œåƒå·²ç»æœ‰ TensorRT SDKï¼ˆå¦‚æœç”¨ NGC PyTorch é•œåƒï¼‰ã€‚éªŒè¯ä¸€ä¸‹ï¼š

```bash
python - <<'PY'
import tensorrt as trt
print("TensorRT version:", trt.__version__)
PY
```

å¦‚æœæŠ¥é”™æ²¡æœ‰ `tensorrt`ï¼Œå°±éœ€è¦å®‰è£…å¯¹åº” wheelï¼ˆç‰ˆæœ¬è¦å’Œ CUDA åŒ¹é…ï¼‰ã€‚

![image-20250827022009543](./report_day4.assets/image-20250827022009543.png)

------

## ä¸‰ã€ONNX â†’ TensorRT Engine

æŠŠday3çš„mlp.onnxå¤åˆ¶åˆ°dockerç›®å½•ä¸‹

æœ€ç®€å•çš„æ–¹æ³•æ˜¯ç”¨ **`trtexec`** å·¥å…·ï¼š

```bash
trtexec --onnx=mlp.onnx --saveEngine=mlp_fp16.engine --fp16
```

- `--onnx`ï¼šè¾“å…¥ ONNX æ¨¡å‹

- `--saveEngine`ï¼šè¾“å‡º TensorRT engine

- `--fp16`ï¼šå¯ç”¨åŠç²¾åº¦åŠ é€Ÿ

- å¦‚æœè¦ INT8ï¼Œè¿˜éœ€è¦æ ¡å‡†æ•°æ®ï¼š

  ```bash
  trtexec --onnx=mlp.onnx --saveEngine=mlp_int8.engine --int8 --calib=<calib.cache>
  ```

> æ‰§è¡Œå®Œæˆåï¼Œä¼šå¾—åˆ° `mlp_fp16.engine` æ–‡ä»¶ã€‚ls -lh mlp_fp16.engineå³å¯æŸ¥çœ‹

![image-20250827022147667](./report_day4.assets/image-20250827022147667.png)

------

## å››ã€Python API åŠ è½½ TensorRT Engine

æ–°å»º `tensorrt_infer.py`ï¼š

```python
# tensorrt_infer.py  â€”â€” TensorRT 10.x / FP16 / å•æ¬¡ä¸åŸºå‡†æµ‹è¯•
import time
import numpy as np
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit  # åˆå§‹åŒ– CUDA ä¸Šä¸‹æ–‡

TRT_LOGGER = trt.Logger(trt.Logger.INFO)

class TrtRunner:
    def __init__(self, engine_path="mlp_fp16.engine", input_shape=(1,1,28,28)):
        self.input_shape = tuple(input_shape)
        with open(engine_path, "rb") as f, trt.Runtime(TRT_LOGGER) as rt:
            self.engine = rt.deserialize_cuda_engine(f.read())
        if self.engine is None:
            raise RuntimeError("Failed to deserialize engine")

        self.ctx = self.engine.create_execution_context()

        # å‘ç° 1 è¾“å…¥ / 1 è¾“å‡ºï¼ˆTRT 10 APIï¼‰
        inputs, outputs = [], []
        for i in range(self.engine.num_io_tensors):
            name = self.engine.get_tensor_name(i)
            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                inputs.append(name)
            else:
                outputs.append(name)
        assert len(inputs) == 1 and len(outputs) == 1, f"IO not 1/1: {inputs}, {outputs}"
        self.in_name, self.out_name = inputs[0], outputs[0]

        # è®¾è¾“å…¥å½¢çŠ¶å¹¶æŸ¥è¯¢è¾“å‡ºå½¢çŠ¶/ç±»å‹
        self.ctx.set_input_shape(self.in_name, self.input_shape)
        self.out_shape = tuple(self.ctx.get_tensor_shape(self.out_name))
        self.out_dtype = np.dtype(trt.nptype(self.engine.get_tensor_dtype(self.out_name)))

        # é¢„åˆ†é…æ˜¾å­˜ + ç»‘å®šåœ°å€ï¼ˆå¤ç”¨ï¼‰
        self.stream = cuda.Stream()
        self.d_in  = cuda.mem_alloc(int(np.prod(self.input_shape)) * 4)  # float32
        self.d_out = cuda.mem_alloc(int(np.prod(self.out_shape)) * self.out_dtype.itemsize)
        self.ctx.set_tensor_address(self.in_name, int(self.d_in))
        self.ctx.set_tensor_address(self.out_name, int(self.d_out))

    def infer_once(self, x_np):
        # H2D
        cuda.memcpy_htod_async(self.d_in, x_np, self.stream)
        # æ‰§è¡Œ
        ok = self.ctx.execute_async_v3(self.stream.handle)
        if not ok:
            raise RuntimeError("TensorRT execution failed")
        # D2H
        y = np.empty(self.out_shape, dtype=self.out_dtype)
        cuda.memcpy_dtoh_async(y, self.d_out, self.stream)
        self.stream.synchronize()
        return y

if __name__ == "__main__":
    # å‡†å¤‡è¾“å…¥
    x = np.random.randn(1,1,28,28).astype(np.float32)

    # åˆå§‹åŒ– & å•æ¬¡æ¨ç†
    runner = TrtRunner("mlp_fp16.engine", input_shape=x.shape)
    y = runner.infer_once(x)
    print("out shape:", y.shape)
    print("out[0][:10]:", y.reshape(-1)[:10])

    # é¢„çƒ­ + åŸºå‡†æµ‹è¯•
    WARMUP, ITERS = 100, 1000
    for _ in range(WARMUP):
        runner.infer_once(x)

    t0 = time.perf_counter()
    for _ in range(ITERS):
        runner.infer_once(x)
    cuda.Context.synchronize()  # å†ä¿é™©åŒæ­¥ä¸€æ¬¡
    t1 = time.perf_counter()

    print(f"å¹³å‡å»¶è¿Ÿ: {(t1 - t0) / ITERS * 1000:.3f} ms  (batch=1)")
```

è¿è¡Œï¼š

```bash
python tensorrt_infer.py
```

è¿è¡Œæç¤ºNo module named 'pycuda'ï¼Œæ‰§è¡Œä¸‹åˆ—å‘½ä»¤,é‡æ–°è¿è¡Œå³å¯

```bash
pip install pycuda
```

![image-20250827022219214](./report_day4.assets/image-20250827022219214.png)

![image-20250827022255871](./report_day4.assets/image-20250827022255871.png)

------

## äº”ã€å¯¹æ¯”å®éªŒ

### å®éªŒæ­¥éª¤

- å…ˆç”¨ **ONNX Runtime** è·‘ä¸€éï¼ˆDay3 çš„ `onnx_infer.py`ï¼‰ã€‚

- å†ç”¨ **TensorRT Engine** è·‘ä¸€éï¼ˆä¸Šé¢çš„ `tensorrt_infer.py`ï¼‰

- è®°å½•ä¸¤æ¬¡æ¨ç†è€—æ—¶å¯¹æ¯”ï¼ˆCPU vs CUDA EP vs TensorRT FP16ï¼‰ã€‚

### ç¯å¢ƒä¿¡æ¯

- GPU å‹å·: NVIDIA GeForce RTX 2080 Ti
- é©±åŠ¨ç‰ˆæœ¬: 550.127.05
- CUDA ç‰ˆæœ¬: 12.9 (nvcc 12.9.86)
- TensorRT ç‰ˆæœ¬: 10.11.0.33
- ONNX Runtime ç‰ˆæœ¬: 1.20.0

```python
# onnx_cpu.py
import os, time, numpy as np, onnxruntime as ort

# å›ºå®š CPU çº¿ç¨‹æ•°ï¼ˆé¿å…æŠ–åŠ¨ï¼Œå¯è‡ªè¡Œè°ƒæ•´ï¼‰
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")

WARMUP, ITERS = 100, 1000
np.random.seed(123)

def main():
    sess = ort.InferenceSession("mlp.onnx", providers=["CPUExecutionProvider"])
    name = sess.get_inputs()[0].name
    x = np.random.randn(1, 1, 28, 28).astype(np.float32)

    # é¢„çƒ­
    for _ in range(WARMUP):
        sess.run(None, {name: x})

    # è®¡æ—¶
    t0 = time.perf_counter()
    for _ in range(ITERS):
        sess.run(None, {name: x})
    t1 = time.perf_counter()

    print(f"ONNX Runtime CPU å¹³å‡å»¶è¿Ÿ: {(t1 - t0)/ITERS*1000:.3f} ms")

if __name__ == "__main__":
    main()
```

è¿è¡Œï¼š

```bash
python onnx_cpu.py
```

![image-20250827022348772](./report_day4.assets/image-20250827022348772.png)

```python
# onnx_cuda.py
import time, numpy as np, onnxruntime as ort

WARMUP, ITERS = 100, 1000
np.random.seed(123)

def main():
    sess = ort.InferenceSession("mlp.onnx", providers=["CUDAExecutionProvider"])
    name = sess.get_inputs()[0].name
    x = np.random.randn(1, 1, 28, 28).astype(np.float32)

    # é¢„çƒ­
    for _ in range(WARMUP):
        sess.run(None, {name: x})

    # è®¡æ—¶
    t0 = time.perf_counter()
    for _ in range(ITERS):
        sess.run(None, {name: x})
    t1 = time.perf_counter()

    print(f"ONNX Runtime CUDA å¹³å‡å»¶è¿Ÿ: {(t1 - t0)/ITERS*1000:.3f} ms")

if __name__ == "__main__":
    main()
```

è¿è¡Œï¼š

```bash
python onnx_cuda.py
```

![image-20250827022417487](./report_day4.assets/image-20250827022417487.png)

TensorRT FP16![image-20250827022450109](./report_day4.assets/image-20250827022450109.png)

### å®éªŒç»“æœï¼ˆbatch=1, 1Ã—1Ã—28Ã—28, iters=1000, warmup=100ï¼‰

| æ–¹æ³•          | å¹³å‡å»¶è¿Ÿ (ms) | å¤‡æ³¨                                |
| ------------- | ------------- | ----------------------------------- |
| ORT CPU       | **0.012**     | OMP/MKL å·²å›ºå®šä¸º 1 çº¿ç¨‹ï¼ˆè„šæœ¬é»˜è®¤ï¼‰ |
| ORT CUDA EP   | 0.050         | ç«¯åˆ°ç«¯ï¼ˆå« H2D/D2Hï¼‰                |
| TensorRT FP16 | 0.040         | FP16 engineï¼Œç«¯åˆ°ç«¯ï¼ˆå¤ç”¨æ˜¾å­˜ï¼‰     |

### åˆ†æ
- è¿™ä¸ªè¶…å°æ¨¡å‹ + batch=1 åœºæ™¯ä¸‹ï¼Œ**CPU æœ€å¿«**ï¼ˆ0.012 msï¼‰ï¼Œç¬¦åˆé¢„æœŸã€‚
- **TensorRT FP16 æ¯” ORT CUDA å¿« ~1.25Ã—**ï¼ˆ0.050 â†’ 0.040 msï¼‰ï¼Œä½†ä»æ…¢äº CPUï¼ŒåŸå› ä¸»è¦æ˜¯ GPU å¯åŠ¨ & æ‹·è´å¼€é”€åœ¨å°ç®—å­ä¸Šå ä¸»å¯¼ã€‚

## å…­ã€è¡¥å……batch=8192

### 0) å…ˆé‡å»º FP16 Engineï¼ˆæ”¯æŒ batch=8192ï¼‰

```bash
trtexec --onnx=mlp.onnx --saveEngine=mlp_fp16_b8192.engine --fp16 \
  --minShapes=input:1x1x28x28 \
  --optShapes=input:8192x1x28x28 \
  --maxShapes=input:8192x1x28x28
```

------

### 1) `onnx_cpu_b8192.py`ï¼ˆORT CPUï¼‰

```python
# onnx_cpu_b8192.py
import os, time, numpy as np, onnxruntime as ort

# ä¸ºäº†é¿å…æŠ–åŠ¨ï¼Œå›ºå®šçº¿ç¨‹æ•°ï¼ˆæŒ‰éœ€è°ƒæ•´æ›´å¿«ï¼‰
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")

BATCH = 8192
SHAPE = (BATCH, 1, 28, 28)
WARMUP, ITERS = 5, 20   # å¤§æ‰¹é‡å¾ˆé‡ï¼Œæ¬¡æ•°é€‚å½“å‡å°
np.random.seed(123)

def main():
    sess = ort.InferenceSession("mlp.onnx", providers=["CPUExecutionProvider"])
    name = sess.get_inputs()[0].name
    x = np.random.randn(*SHAPE).astype(np.float32)

    # é¢„çƒ­
    for _ in range(WARMUP):
        _ = sess.run(None, {name: x})

    # è®¡æ—¶
    t0 = time.perf_counter()
    for _ in range(ITERS):
        _ = sess.run(None, {name: x})
    t1 = time.perf_counter()

    avg_s = (t1 - t0) / ITERS
    thr = BATCH / avg_s
    print(f"[ORT CPU] batch={BATCH} å¹³å‡å»¶è¿Ÿ: {avg_s*1000:.3f} ms  åå: {thr:,.0f} samples/s")

if __name__ == "__main__":
    main()
```

![image-20250827022519132](./report_day4.assets/image-20250827022519132.png)

------

### 2) `onnx_cuda_b8192.py`ï¼ˆORT CUDA EPï¼‰

```python
# onnx_cuda_b8192.py
import time, numpy as np, onnxruntime as ort

BATCH = 8192
SHAPE = (BATCH, 1, 28, 28)
WARMUP, ITERS = 5, 20
np.random.seed(123)

def main():
    # éœ€è¦ onnxruntime-gpu + æ­£ç¡®çš„ CUDA/cuDNN
    sess = ort.InferenceSession("mlp.onnx", providers=["CUDAExecutionProvider"])
    name = sess.get_inputs()[0].name
    x = np.random.randn(*SHAPE).astype(np.float32)

    # é¢„çƒ­
    for _ in range(WARMUP):
        _ = sess.run(None, {name: x})

    # è®¡æ—¶
    t0 = time.perf_counter()
    for _ in range(ITERS):
        _ = sess.run(None, {name: x})
    t1 = time.perf_counter()

    avg_s = (t1 - t0) / ITERS
    thr = BATCH / avg_s
    print(f"[ORT CUDA] batch={BATCH} å¹³å‡å»¶è¿Ÿ: {avg_s*1000:.3f} ms  åå: {thr:,.0f} samples/s")

if __name__ == "__main__":
    main()
```

> è¿›é˜¶ï¼šæƒ³è¿›ä¸€æ­¥é™ä½ H2D/D2H å¼€é”€ï¼Œå¯ä»¥æ”¹ä¸º **IO Binding**ï¼ˆéœ€è¦ä½ ç®¡ç† GPU ç¼“å†²åŒºï¼Œæœ‰éœ€è¦æˆ‘å†ç»™ä½ ä¸€ç‰ˆï¼‰ã€‚

![image-20250827022541904](./report_day4.assets/image-20250827022541904.png)

------

### 3) `tensorrt_fp16_b8192.py`ï¼ˆTensorRT 10.x / FP16ï¼‰

```python
# tensorrt_fp16_b8192.py  â€”â€” TensorRT 10.x / FP16 / æ‰¹é‡=8192
import time, numpy as np, tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit  # åˆå§‹åŒ– CUDA ä¸Šä¸‹æ–‡

TRT_LOGGER = trt.Logger(trt.Logger.ERROR)
BATCH = 8192
SHAPE = (BATCH, 1, 28, 28)
WARMUP, ITERS = 5, 20
np.random.seed(123)

class TrtRunner:
    def __init__(self, engine_path="mlp_fp16_b8192.engine", input_shape=SHAPE):
        self.input_shape = tuple(input_shape)
        with open(engine_path, "rb") as f, trt.Runtime(TRT_LOGGER) as rt:
            self.engine = rt.deserialize_cuda_engine(f.read())
        if self.engine is None:
            raise RuntimeError("Failed to deserialize engine")
        self.ctx = self.engine.create_execution_context()

        # å‘ç° IOï¼ˆ1 å…¥ / 1 å‡ºï¼‰
        ins, outs = [], []
        for i in range(self.engine.num_io_tensors):
            n = self.engine.get_tensor_name(i)
            (ins if self.engine.get_tensor_mode(n)==trt.TensorIOMode.INPUT else outs).append(n)
        assert len(ins)==1 and len(outs)==1, f"IO not 1/1: {ins}, {outs}"
        self.in_name, self.out_name = ins[0], outs[0]

        # è®¾ç½®è¾“å…¥å½¢çŠ¶ï¼ŒæŸ¥è¯¢è¾“å‡ºå½¢çŠ¶ä¸ç±»å‹
        self.ctx.set_input_shape(self.in_name, self.input_shape)
        self.out_shape = tuple(self.ctx.get_tensor_shape(self.out_name))
        self.out_dtype = np.dtype(trt.nptype(self.engine.get_tensor_dtype(self.out_name)))

        # é¢„åˆ†é…æ˜¾å­˜å¹¶ç»‘å®šï¼ˆå¤ç”¨ï¼‰
        self.stream = cuda.Stream()
        self.d_in  = cuda.mem_alloc(int(np.prod(self.input_shape)) * 4)  # FP32 è¾“å…¥ï¼ˆç«¯åˆ°ç«¯ï¼‰
        self.d_out = cuda.mem_alloc(int(np.prod(self.out_shape)) * self.out_dtype.itemsize)
        self.ctx.set_tensor_address(self.in_name, int(self.d_in))
        self.ctx.set_tensor_address(self.out_name, int(self.d_out))

    def infer_once(self, x_np):
        # H2D
        cuda.memcpy_htod_async(self.d_in, x_np, self.stream)
        # æ‰§è¡Œ
        ok = self.ctx.execute_async_v3(self.stream.handle)
        if not ok:
            raise RuntimeError("TensorRT execution failed")
        # D2H
        y = np.empty(self.out_shape, dtype=self.out_dtype)
        cuda.memcpy_dtoh_async(y, self.d_out, self.stream)
        self.stream.synchronize()
        return y

def main():
    runner = TrtRunner()
    x = np.random.randn(*SHAPE).astype(np.float32)

    # é¢„çƒ­
    for _ in range(WARMUP):
        runner.infer_once(x)

    # è®¡æ—¶
    t0 = time.perf_counter()
    for _ in range(ITERS):
        runner.infer_once(x)
    cuda.Context.synchronize()
    t1 = time.perf_counter()

    avg_s = (t1 - t0) / ITERS
    thr = BATCH / avg_s
    print(f"[TensorRT FP16] batch={BATCH} å¹³å‡å»¶è¿Ÿ: {avg_s*1000:.3f} ms  åå: {thr:,.0f} samples/s")

if __name__ == "__main__":
    main()
```

![image-20250827022602864](./report_day4.assets/image-20250827022602864.png)

| æ–¹æ³•          | å¹³å‡å»¶è¿Ÿ (ms) | åå (samples/s) | å¤‡æ³¨                  |
| ------------- | ------------: | ---------------: | --------------------- |
| ORT CPU       |         2.915 |        2,810,563 | OMP/MKL=1             |
| ORT CUDA EP   |         2.368 |        3,459,472 | onnxruntime-gpu       |
| TensorRT FP16 |         2.260 |        3,624,212 | mlp_fp16_b8192.engine |

