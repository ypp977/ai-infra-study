# Day 10 - CUDA ä¼˜åŒ–æŠ€å·§

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£ warp divergence ä¸ memory coalescing
- ä½¿ç”¨ shared memory ä¼˜åŒ–çŸ©é˜µä¹˜æ³•
- å­¦ä¼šç”¨ Nsight åˆ†æ occupancyã€è®¿å­˜å¸¦å®½ã€å¯„å­˜å™¨ä½¿ç”¨

  

## 1ï¸âƒ£ åŸºç¡€ç†è®º

### (1) Warp divergenceï¼ˆåˆ†æ”¯å‘æ•£ï¼‰

- ä¸€ä¸ª **warp = 32 ä¸ªçº¿ç¨‹**ã€‚
- å¦‚æœ warp å†…çº¿ç¨‹é‡åˆ°åˆ†æ”¯ï¼ˆif/elseï¼‰ï¼Œä¸åŒçº¿ç¨‹èµ°ä¸åŒè·¯å¾„ â†’ GPU å¿…é¡»é¡ºåºæ‰§è¡Œä¸åŒåˆ†æ”¯ â†’ **æ€§èƒ½ä¸‹é™**ã€‚

ä¾‹å­ï¼š

```c++
if (threadIdx.x % 2 == 0) { ... } else { ... }
```

ğŸ‘‰ ä¸€åŠçº¿ç¨‹åœ¨ç­‰å¦ä¸€åŠï¼Œæ•ˆç‡å‡åŠã€‚

### (2) Memory coalescingï¼ˆå†…å­˜åˆå¹¶è®¿é—®ï¼‰

- Global Memory å¸¦å®½åˆ©ç”¨ç‡å–å†³äº warp å†…çº¿ç¨‹æ˜¯å¦è®¿é—®è¿ç»­åœ°å€ã€‚
- **è¿ç»­è®¿é—®**ï¼ˆ0,1,2,3...ï¼‰ â†’ åˆå¹¶æˆä¸€æ¬¡å¤§å†…å­˜äº‹åŠ¡ã€‚
- **ä¹±åºè®¿é—®** â†’ æ¯ä¸ªçº¿ç¨‹å•ç‹¬è®¿é—®ï¼Œå¸¦å®½åˆ©ç”¨ç‡æä½ã€‚

### (3) Shared memory çŸ©é˜µä¹˜

- Naive GEMMï¼šæ¯ä¸ªçº¿ç¨‹ä» global memory å¤šæ¬¡åŠ è½½åŒä¸€å…ƒç´ ï¼Œæµªè´¹å¸¦å®½ã€‚
- Shared memory tilingï¼šæŠŠæ•°æ®å—è¯»è¿› shared memoryï¼Œè®© block å†…çº¿ç¨‹å…±äº«ï¼Œå‡å°‘ global memory è®¿é—®ã€‚


### (4) Occupancy

- **å®šä¹‰**ï¼šä¸€ä¸ª SM ä¸ŠåŒæ—¶æ´»è·ƒçš„ warp æ•° / æœ€å¤§ warp æ•°ã€‚
- Occupancy é«˜æœ‰åŠ©äºæ©ç›–å†…å­˜å»¶è¿Ÿï¼Œä½†ä¸æ˜¯è¶Šé«˜è¶Šå¥½ï¼ˆè®¡ç®—å¯†é›†å‹å†…æ ¸åè€Œå¯èƒ½æ€§èƒ½ä¸‹é™ï¼‰ã€‚


## 2ï¸âƒ£ çŸ©é˜µä¹˜æ³•

### (1) Naive çŸ©é˜µä¹˜ï¼š`mmul_naive.cu`

```c++
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void mmul_native(const float* A, const float* B, float* C, int N)
{
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N)
    {
        float sum = 0;
        for (int k = 0; k < N; k++)
        {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

int main()
{
    const int N = 512;
    size_t bytes = N * N * sizeof(float);

    float host_a[N * N], host_b[N * N], host_c[N * N];

    for (int i = 0; i < N * N; i++)
    {
        host_a[i] = 1.0f;
        host_b[i] = 2.0f;
    }

    float *device_a, *device_b, *device_c;
    cudaMalloc(&device_a, bytes);
    cudaMalloc(&device_b, bytes);
    cudaMalloc(&device_c, bytes);

    cudaMemcpy(device_a, host_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes, cudaMemcpyHostToDevice);

    dim3 block(16, 16);
    dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);

    float ms = 0;
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    mmul_native<<<grid, block>>>(device_a, device_b, device_c, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&ms, start, stop);
    cudaMemcpy(host_c, device_c, bytes, cudaMemcpyDeviceToHost);

    printf("Naive GEMM time = %.3f ms, result C[0]=%.1f\n", ms, host_c[0]);

    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);

    return 0;
}

```

é¢„æœŸè¾“å‡ºï¼š

![image-20250903205315885](./report_day10.assets/image-20250903205315885.png)

C = AÃ—B = 1Ã—2Ã—N = 8

### (2) Shared memory çŸ©é˜µä¹˜ï¼š`mmul_shared.cu`

```c++
#include <cuda_runtime.h>
#include <stdio.h>

#define TILE 16
// shared memory
__global__ void mmul_shared(const float* A, const float* B, float* C, int N)
{
    __shared__ float a_shared[TILE][TILE];
    __shared__ float b_shared[TILE][TILE];

    int row = blockIdx.y * TILE + threadIdx.y;
    int col = blockIdx.x * TILE + threadIdx.x;

    float sum = 0;
    for (int t = 0; t < (N + TILE - 1) / TILE; t++)
    {
        if (row < N && (t * TILE + threadIdx.x) < N)
        {
            a_shared[threadIdx.y][threadIdx.x] = A[row * N + t * TILE + threadIdx.x];
        }
        else
        {
            a_shared[threadIdx.y][threadIdx.x] = 0;
        }

        if (col < N && (t * TILE + threadIdx.y) < N)
        {
            b_shared[threadIdx.y][threadIdx.x] = B[(t * TILE + threadIdx.y) * N + col];
        }
        else
        {
            b_shared[threadIdx.y][threadIdx.x] = 0;
        }

        __syncthreads();

        for (int k = 0; k < TILE; k++)
        {
            sum += a_shared[threadIdx.y][k] * b_shared[k][threadIdx.x];
        }

        __syncthreads();
    }
    if (row < N && col < N)
    {
        C[row * N + col] = sum;
    }
}

int main()
{
    const int N = 512;
    size_t bytes = N * N * sizeof(float);

    float* host_a = (float*)malloc(bytes);
    float* host_b = (float*)malloc(bytes);
    float* host_c = (float*)malloc(bytes);

    for (int i = 0; i < N * N; i++)
    {
        host_a[i] = 1.0f;
        host_b[i] = 2.0f;
    }

    float *device_a, *device_b, *device_c;
    cudaMalloc(&device_a, bytes);
    cudaMalloc(&device_b, bytes);
    cudaMalloc(&device_c, bytes);

    cudaMemcpy(device_a, host_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes, cudaMemcpyHostToDevice);

    dim3 block(TILE, TILE);
    dim3 grid((N + TILE - 1) / TILE, (N + TILE - 1) / TILE);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    mmul_shared<<<grid, block>>>(device_a, device_b, device_c, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    cudaEventDestroy(start);
    cudaEventDestroy(stop);

    cudaMemcpy(host_c, device_c, bytes, cudaMemcpyDeviceToHost);

    printf("Shared GEMM time = %.3f ms, result C[0]=%.1f\n", ms, host_c[0]);

    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);

    free(host_a);
    free(host_b);
    free(host_c);

    return 0;
}

```

é¢„æœŸè¾“å‡ºï¼š

![image-20250903212610092](./report_day10.assets/image-20250903212610092.png)

ğŸ‘‰ å¯ä»¥çœ‹åˆ°æ¯” Naive å¿«å¾ˆå¤šã€‚  

### (3) Coalesced vs Non-Coalesced è®¿å­˜ï¼š`mmul_coalesce.cu`

```c++
#include <cuda_runtime.h>
#include <stdio.h>

// éåˆå¹¶è®¿å­˜ï¼ˆè¡Œé”™ä½ï¼‰
__global__ void kernel_non_coalesce(float* out, const float* in, int N)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < N)
    {
        // æ•…æ„ä¹±åº
        out[tid] = in[(tid * 17) % N];
    }
}

// åˆå¹¶è®¿å­˜
__global__ void kernel_coalesce(float* out, const float* in, int N)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < N)
    {
        // é¡ºåºè®¿é—®
        out[tid] = in[tid];
    }
}

int main()
{
    const int N = 1 << 20; // 1Må…ƒç´ 
    size_t bytes = N * sizeof(float);

    float* host_in = (float*)malloc(bytes);
    float* host_out = (float*)malloc(bytes);
    for (int i = 0; i < N; i++)
    {
        host_in[i] = i;
    }

    float *device_in, *device_out;
    cudaMalloc(&device_in, bytes);
    cudaMalloc(&device_out, bytes);

    cudaMemcpy(device_in, host_in, bytes, cudaMemcpyHostToDevice);

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    float ms_1;
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    kernel_non_coalesce<<<grid, block>>>(device_out, device_in, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&ms_1, start, stop);
    cudaMemcpy(host_out, device_out, bytes, cudaMemcpyDeviceToHost);
    printf("Non-coalesced: time=%.3f ms, out[0]=%.1f\n", ms_1, host_out[0]);

    cudaEventRecord(start);
    kernel_coalesce<<<grid, block>>>(device_out, device_in, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&ms_1, start, stop);
    cudaMemcpy(host_out, device_out, bytes, cudaMemcpyDeviceToHost);
    printf("Coalesced: time=%.3f ms, out[0]=%.1f\n", ms_1, host_out[0]);

    cudaFree(device_in);
    cudaFree(device_out);

    free(host_in);
    free(host_out);

    return 0;
}

```

### è¾“å‡º

ğŸ‘‰ **åˆå¹¶è®¿å­˜æ˜æ˜¾æ›´å¿«**ï¼Œåœ¨ Nsight é‡Œèƒ½çœ‹åˆ° **Global Load Efficiency ç”± ~30% â†’ ~100%**ã€‚

![image-20250904002152301](./report_day10.assets/image-20250904002152301.png)  

## 3ï¸âƒ£ æ·±åº¦è¿½é—®

### 1. åˆ†æ”¯æ¶ˆé™¤ä¸æŸ¥è¡¨æ³•åœ¨ GPU ä¸Šçš„é€‚ç”¨æ€§ï¼Ÿ

#### ğŸ” 1. åˆ†æ”¯æ¶ˆé™¤ (Branch Elimination)

##### èƒŒæ™¯

- GPU çš„æ‰§è¡Œå•å…ƒä»¥ **warp (32ä¸ªçº¿ç¨‹)** ä¸ºè°ƒåº¦ç²’åº¦ã€‚
- å¦‚æœ warp å†…çº¿ç¨‹åœ¨ `if/else` ä¸­èµ°äº†ä¸åŒè·¯å¾„ï¼Œå°±ä¼šå‘ç”Ÿ **warp divergence (åˆ†æ”¯å‘æ•£)**ï¼š
  - GPU ä¼šä¸²è¡Œæ‰§è¡Œæ¯æ¡åˆ†æ”¯è·¯å¾„ï¼›
  - æ²¡èµ°è¯¥åˆ†æ”¯çš„çº¿ç¨‹åœ¨ç­‰å¾…ï¼Œæµªè´¹èµ„æºã€‚

##### åˆ†æ”¯æ¶ˆé™¤çš„æ€è·¯

- ç”¨ **ä¸‰ç›®è¿ç®—ç¬¦ `?:`** æˆ– **PTX `selp` æŒ‡ä»¤** æ›¿ä»£ if-elseï¼Œè®©æ‰€æœ‰çº¿ç¨‹æ‰§è¡Œç›¸åŒæŒ‡ä»¤æµã€‚

ä¾‹å­ï¼š

```
// æœ‰åˆ†æ”¯
if (x > 0) y = a; else y = b;

// æ¶ˆé™¤åˆ†æ”¯
y = (x > 0) ? a : b; // ç¼–è¯‘æˆ selp
```

##### é€‚ç”¨æ€§

âœ… **é€‚åˆ**ï¼š

- åˆ†æ”¯é€»è¾‘ç®€å•ï¼Œå¼€é”€å°ã€‚
- ä¸¤ä¸ªåˆ†æ”¯è·¯å¾„æ‰§è¡Œä»£ä»·å·®ä¸å¤šã€‚
- æ¡ä»¶åˆ¤æ–­é¢‘ç¹å‡ºç°åœ¨æ ¸å¿ƒå¾ªç¯é‡Œã€‚

âš ï¸ **ä¸é€‚åˆ**ï¼š

- ä¸¤ä¸ªåˆ†æ”¯è·¯å¾„å·®å¼‚æå¤§ï¼ˆæ¯”å¦‚ä¸€ä¸ªåˆ†æ”¯åšå¤§é‡è®¡ç®—ï¼Œå¦ä¸€ä¸ªä»€ä¹ˆéƒ½ä¸åšï¼‰ã€‚
- è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†æ”¯æ¶ˆé™¤ä¼šå¯¼è‡´æ‰€æœ‰çº¿ç¨‹éƒ½æ‰§è¡Œâ€œé‡åˆ†æ”¯â€çš„è¿ç®—ï¼ˆå³ä½¿ä¸éœ€è¦ï¼‰ï¼Œåè€Œæ›´æ…¢ã€‚

------

#### ğŸ” 2. æŸ¥è¡¨æ³• (Lookup Table, LUT)

##### èƒŒæ™¯

- å½“åˆ†æ”¯æ¡ä»¶æ˜¯**æœ‰é™ç¦»æ•£å–å€¼**æ—¶ï¼Œå¯ä»¥é¢„å…ˆæŠŠç»“æœæ”¾åˆ°æŸ¥è¡¨æ•°ç»„é‡Œï¼Œé¿å… `switch/if`ã€‚

ä¾‹å­ï¼š

```
// æœ‰åˆ†æ”¯
if (state == 0) y = f0(x);
else if (state == 1) y = f1(x);
else y = f2(x);

// æŸ¥è¡¨
__device__ float (*funcs[3])(float) = {f0, f1, f2};
y = funcs[state](x);
```

æˆ–è€…æ›´ç®€å•ï¼š

```
__device__ int lut[4] = {0,1,3,7};
y = lut[input & 3];
```

##### é€‚ç”¨æ€§

âœ… **é€‚åˆ**ï¼š

- æ¡ä»¶æ˜¯ **å°èŒƒå›´æšä¸¾å€¼**ï¼ˆ0/1/2/3 â€¦ï¼‰ã€‚
- æ¯æ¬¡è®¡ç®—åˆ†æ”¯é€»è¾‘æ¯”ä¸€æ¬¡å†…å­˜è®¿é—®æ›´æ˜‚è´µï¼ˆæ¯”å¦‚å¤æ‚è¡¨è¾¾å¼ï¼‰ã€‚
- æŸ¥è¡¨æ•°ç»„èƒ½æ”¾åˆ° **å¸¸é‡å†…å­˜ / shared memory**ï¼Œå‘½ä¸­ç‡é«˜ï¼Œè®¿é—®å¿«ã€‚

âš ï¸ **ä¸é€‚åˆ**ï¼š

- æ¡ä»¶èŒƒå›´å¤§ï¼ˆLUT å¾ˆå¤§ï¼Œè¶…å‡ºå¸¸é‡ç¼“å­˜ï¼‰ï¼Œä¼šé€€åŒ–æˆéšæœº global memory è®¿é—®ã€‚
- æ¯ä¸ªçº¿ç¨‹æŸ¥ä¸åŒç´¢å¼•ï¼ŒLUT è®¿é—®ä¸åˆå¹¶ï¼Œcache æ•ˆç‡å·®ã€‚

------

#### ğŸ”¬ 3. å®æˆ˜ç»éªŒæ€»ç»“

- **åˆ†æ”¯æ¶ˆé™¤**ï¼š
  - åœ¨ GPU æ ¸å¿ƒå¾ªç¯ä¸­éå¸¸å¸¸è§ï¼ˆé¿å… warp divergenceï¼‰ã€‚
  - ä½†è¦å¹³è¡¡â€œé¿å… divergenceâ€ vs â€œå¼ºåˆ¶å¤šåšæ— ç”¨è®¡ç®—â€ã€‚
- **æŸ¥è¡¨æ³•**ï¼š
  - å¯¹ **å°èŒƒå›´ç¦»æ•£å€¼** æ˜¯åˆ©å™¨ã€‚
  - å°¤å…¶é€‚åˆæ”¾åœ¨ **å¸¸é‡å†…å­˜** æˆ– **shared memory**ã€‚
  - å¦‚æœç´¢å¼•å¤§ä¸”åˆ†å¸ƒä¹±ï¼Œå°±ä¸å¦‚ç›´æ¥åˆ†æ”¯ã€‚

------

#### âœ… æ€»ç»“

- **åˆ†æ”¯æ¶ˆé™¤**ï¼šé€‚åˆè½»é‡åˆ†æ”¯ï¼Œé¿å… warp divergenceï¼›ä½†é‡åˆ†æ”¯ä¼šå˜æ…¢ã€‚
- **æŸ¥è¡¨æ³•**ï¼šé€‚åˆå°èŒƒå›´æšä¸¾ï¼Œå¸¸é‡/å…±äº«å†…å­˜å­˜ LUT æ—¶éå¸¸é«˜æ•ˆï¼›å¤§è¡¨/ä¹±è®¿é—®ä¸é€‚åˆã€‚

### 2. loop unrolling çš„æ”¶ç›Šä¸ register è†¨èƒ€çš„æƒè¡¡ï¼Ÿ

#### ğŸ” 1. Loop Unrolling çš„æ”¶ç›Š

##### (1) å‡å°‘å¾ªç¯å¼€é”€

- å±•å¼€åä¸éœ€è¦æ¯æ¬¡è¿­ä»£åˆ¤æ–­ `i < N`ï¼Œä¹Ÿå°‘äº† `i++`ã€‚
- èŠ‚çœäº†æ§åˆ¶æµæŒ‡ä»¤ï¼ˆbranch/jumpï¼‰ã€‚

##### (2) å¢åŠ æŒ‡ä»¤çº§å¹¶è¡Œåº¦ (ILP)

- ç¼–è¯‘å™¨å¯ä»¥æŠŠå±•å¼€åçš„å¤šæ¬¡è¿­ä»£æŒ‡ä»¤äº¤å‰è°ƒåº¦ï¼Œæå‡ pipeline åˆ©ç”¨ç‡ã€‚
- èƒ½æ›´å¥½åœ°æ©ç›–è®¿å­˜å»¶è¿Ÿã€‚

##### (3) æ›´é€‚åˆæŒ‡ä»¤ä¼˜åŒ–

- GPU çš„ warp scheduler å¯ä»¥æŠŠå±•å¼€åçš„ç®—å­æ›´å¥½åœ°å¡æ»¡æ‰§è¡Œå•å…ƒã€‚
- å¯¹è®¿å­˜å‹ä»£ç ï¼Œå¯ä»¥å¢åŠ  prefetch æ•ˆæœã€‚

------

#### ğŸ” 2. Loop Unrolling çš„ä»£ä»·

##### (1) **å¯„å­˜å™¨å‹åŠ›å¢åŠ **

- å±•å¼€åçš„å¤šä¸ªè¿­ä»£å˜é‡åŒæ—¶å­˜åœ¨ â†’ éœ€è¦æ›´å¤šå¯„å­˜å™¨ã€‚
- ä¾‹å¦‚ï¼š
  - ä¸å±•å¼€ï¼šä¸€ä¸ªå¾ªç¯ä½“ç”¨ 10 ä¸ªå¯„å­˜å™¨ã€‚
  - å±•å¼€ 4 å€ï¼šå¯èƒ½è¦ç”¨ 30â€“40 ä¸ªå¯„å­˜å™¨ã€‚

##### (2) **Occupancy ä¸‹é™**

- æ¯ä¸ª SM çš„å¯„å­˜å™¨æ•°æ˜¯æœ‰é™çš„ï¼ˆAmpere é€šå¸¸ 64K Ã— 32-bitï¼‰ã€‚
- å¦‚æœå•çº¿ç¨‹å¯„å­˜å™¨ç”¨é‡è¿‡å¤§ â†’ èƒ½åŒæ—¶é©»ç•™çš„çº¿ç¨‹æ•°å‡å°‘ â†’ **å¹¶è¡Œåº¦ä¸‹é™**ã€‚

##### (3) **å¯èƒ½è§¦å‘ spill**

- å¦‚æœå¯„å­˜å™¨ä¸å¤Ÿï¼Œç¼–è¯‘å™¨ä¼šæŠŠéƒ¨åˆ†å˜é‡ spill åˆ° **local memoryï¼ˆæ˜¾å­˜ï¼‰**ã€‚
- Local memory å»¶è¿Ÿå‡ ç™¾ cycleï¼Œæ¯”å¯„å­˜å™¨æ…¢å‡ ä¸ªæ•°é‡çº§ã€‚

------

#### ğŸ” 3. æƒè¡¡ç‚¹

- **è®¿å­˜å—é™ (memory bound) å†…æ ¸**ï¼š
  - Loop unrolling å¯ä»¥æ©ç›–è®¿å­˜å»¶è¿Ÿ â†’ æœ‰æ”¶ç›Šã€‚
  - é€‚åº¦å¢åŠ å¯„å­˜å™¨ä¸€èˆ¬æ²¡å…³ç³»ã€‚
- **è®¡ç®—å—é™ (compute bound) å†…æ ¸**ï¼š
  - Loop unrolling æå‡ä¸å¤§ï¼Œåè€Œå› ä¸ºå¯„å­˜å™¨å¢åŠ  â†’ occupancy ä¸‹é™ â†’ æ€§èƒ½ä¸‹é™ã€‚
- **ç»éªŒæ³•åˆ™**ï¼š
  - å±•å¼€ 2Ã— æˆ– 4Ã— é€šå¸¸æ”¶ç›Šæ˜æ˜¾ã€‚
  - å±•å¼€ 8Ã— æˆ–ä»¥ä¸Š â†’ å¯„å­˜å™¨æ•°è†¨èƒ€ï¼Œå®¹æ˜“å¾—ä¸å¿å¤±ã€‚

------

#### âœ… æ€»ç»“

- **Unrolling æå‡ ILPï¼Œå‡å°‘æ§åˆ¶æµå¼€é”€ â†’ æœ‰åŠ©äºæ€§èƒ½**ã€‚
- **ä½†ä¼šå¢åŠ å¯„å­˜å™¨ä½¿ç”¨ â†’ é™ä½ occupancyï¼Œç”šè‡³å¯¼è‡´ spill**ã€‚
- **æœ€ä½³ç‚¹åœ¨ 2Ã— æˆ– 4Ã— unroll**ï¼Œè¦ç”¨ Nsight è§‚å¯Ÿå¯„å­˜å™¨ & occupancy æ‰¾å¹³è¡¡ã€‚

------

### 3. è®¿å­˜å¯¹é½ä¸ coalescing çš„å…³ç³»ï¼Ÿ

#### 1ï¸âƒ£ ä»€ä¹ˆæ˜¯è®¿å­˜å¯¹é½ (Memory Alignment)

- GPU global memory æ˜¯æŒ‰ **transaction (å†…å­˜äº‹åŠ¡)** æ¥è®¿é—®çš„ï¼Œå¤§å°é€šå¸¸æ˜¯ **32B / 64B / 128B**ã€‚
- **å¯¹é½ (aligned)**ï¼šçº¿ç¨‹è®¿é—®çš„åœ°å€è½åœ¨è¿™äº› transaction çš„è¾¹ç•Œä¸Šã€‚
- **ä¸å¯¹é½ (misaligned)**ï¼šè®¿é—®è·¨è¶Šè¾¹ç•Œï¼Œä¼šè§¦å‘é¢å¤–çš„ transactionã€‚

ä¾‹å­ï¼ˆwarp=32ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹è®¿é—®4å­—èŠ‚ï¼‰ï¼š

- å¦‚æœä»åœ°å€ **0 å¼€å§‹**ï¼šä¸€æ¬¡ 128B transaction å°±èƒ½æå®šã€‚
- å¦‚æœä»åœ°å€ **4 å¼€å§‹**ï¼šå¯èƒ½éœ€è¦ 2 æ¬¡ transactionï¼Œå› ä¸ºåœ°å€ 4â€“131 è·¨è¶Šäº† 128B è¾¹ç•Œã€‚

------

#### 2ï¸âƒ£ ä»€ä¹ˆæ˜¯ Coalescing (åˆå¹¶è®¿å­˜)

- ä¸€ä¸ª warp (32çº¿ç¨‹) å‘èµ·çš„è®¿é—®ä¼šå°½é‡è¢« GPU **åˆå¹¶æˆå°½å¯èƒ½å°‘çš„ transaction**ã€‚
- åˆå¹¶æˆåŠŸæ¡ä»¶ï¼š
  1. **çº¿ç¨‹è®¿é—®è¿ç»­åœ°å€**
  2. **åœ°å€æŒ‰æ•°æ®å¤§å°å¯¹é½**
- åˆå¹¶å¤±è´¥ï¼šwarp å†…æ¯ä¸ªçº¿ç¨‹çš„è®¿é—®è¢«æ‹†æˆå¤šä¸ª transaction â†’ å¸¦å®½åˆ©ç”¨ç‡å¤§å¹…ä¸‹é™ã€‚

------

#### 3ï¸âƒ£ å¯¹é½ä¸ Coalescing çš„å…³ç³»

âœ… **å¯¹é½æ˜¯ coalescing çš„å‰æä¹‹ä¸€**ï¼š

- å¦‚æœ warp å†…çº¿ç¨‹è®¿é—®æ˜¯è¿ç»­çš„ï¼Œä½†èµ·å§‹åœ°å€ä¸å¯¹é½ â†’ ä»å¯èƒ½éœ€è¦å¤šä¸€æ¬¡ transactionã€‚

ä¸¾ä¾‹ï¼š

- **å¯¹é½ + è¿ç»­**ï¼š
  - çº¿ç¨‹0 â†’ addr0, çº¿ç¨‹1 â†’ addr4, â€¦ â†’ çº¿ç¨‹31 â†’ addr124
  - å®Œç¾åˆå¹¶ â†’ 1 æ¬¡ 128B transaction
- **ä¸å¯¹é½ + è¿ç»­**ï¼š
  - çº¿ç¨‹0 â†’ addr4, çº¿ç¨‹1 â†’ addr8, â€¦ â†’ çº¿ç¨‹31 â†’ addr128
  - è·¨è¶Š 128B è¾¹ç•Œ â†’ éœ€è¦ 2 æ¬¡ transaction
- **ä¸è¿ç»­**ï¼š
  - çº¿ç¨‹0 â†’ addr0, çº¿ç¨‹1 â†’ addr64, çº¿ç¨‹2 â†’ addr128 â€¦
  - å®Œå…¨æ— æ³•åˆå¹¶ â†’ æ¯ä¸ªçº¿ç¨‹å•ç‹¬ä¸€ä¸ª transaction

------

#### 4ï¸âƒ£ å®è·µä¸­çš„ä¼˜åŒ–æŠ€å·§

1. **ç»“æ„ä½“å¯¹é½**

   - é¿å… AoS (Array of Structures)ï¼Œå› ä¸ºç»“æ„ä½“å¤§å°å¯èƒ½ä¸æ˜¯ 4B/8B/16B çš„å€æ•°ã€‚
   - æ”¹æˆ SoA (Structure of Arrays)ï¼Œä¿è¯æ¯ä¸ªæ•°ç»„æ˜¯è¿ç»­çš„ float/intã€‚

2. **æ˜¾å¼å¯¹é½**

   - ä½¿ç”¨ `__align__(16)` æˆ– `float4` æ¥ä¿è¯å†…å­˜å¯¹é½ã€‚

   - ä¾‹å¦‚ï¼š

     ```
     struct __align__(16) Particle { float x, y, z, w; };
     ```

3. **pitch åˆ†é…**

   - å¯¹ 2D æ•°ç»„ä½¿ç”¨ `cudaMallocPitch`ï¼Œä¿è¯è¡Œå¯¹é½åˆ° 128Bã€‚

------

#### âœ… æ€»ç»“ä¸€å¥

- **è®¿å­˜å¯¹é½** å†³å®š warp çš„å†…å­˜è®¿é—®æ˜¯å¦è½åœ¨å†…å­˜äº‹åŠ¡è¾¹ç•Œä¸Šã€‚
- **coalescing** éœ€è¦ **å¯¹é½ + è¿ç»­è®¿é—®**ï¼Œå¦åˆ™ä¼šæ‹† transactionï¼Œé™ä½ Global Load Efficiencyã€‚
- æ‰€ä»¥ï¼š
  - **å¯¹é½æ˜¯ coalescing çš„å¿…è¦æ¡ä»¶**ï¼Œ
  - ä½† **è¿ç»­è®¿é—®**åŒæ ·å¿…ä¸å¯å°‘ã€‚

### 4. inline PTX çš„ä»·å€¼ä¸å¯ç»´æŠ¤æ€§é£é™©ï¼Ÿ

#### ğŸ” 1. Inline PTX çš„ä»·å€¼

##### (1) ä½¿ç”¨ç¼–è¯‘å™¨æœªå…¬å¼€çš„ç¡¬ä»¶æŒ‡ä»¤

- CUDA C++ æä¾›çš„ API å¹¶ä¸è¦†ç›– GPU çš„æ‰€æœ‰æŒ‡ä»¤ã€‚
- ç”¨ `asm volatile("...");` å¯ä»¥ç›´æ¥è°ƒ PTX æŒ‡ä»¤ï¼Œæ¯”å¦‚ï¼š
  - `shfl.sync` warp shuffle
  - `bar.sync` warp å±éšœ
  - `ldmatrix` / `mma.sync` (Tensor Core æŒ‡ä»¤)
- åœºæ™¯ï¼šä½ éœ€è¦æ‰‹å·¥è°ƒç”¨æœ€æ–°æ¶æ„æ”¯æŒçš„ç‰¹æ®Šç¡¬ä»¶æŒ‡ä»¤ï¼Œè€Œ CUDA å¤´æ–‡ä»¶è¿˜æ²¡å°è£…ã€‚

------

##### (2) ç²¾ç»†æ§åˆ¶å¯„å­˜å™¨ / å†…å­˜æ“ä½œ

- ä½ å¯ä»¥å†³å®šå“ªäº›å˜é‡æ”¾å¯„å­˜å™¨ã€å¦‚ä½•ä½¿ç”¨ load/store æŒ‡ä»¤ã€‚
- å¯¹ **è®¿å­˜æ¨¡å¼æ•æ„Ÿçš„å†…æ ¸**ï¼ˆå¦‚çŸ©é˜µä¹˜ã€å·ç§¯ï¼‰å¯ä»¥æ‰‹åŠ¨ä¼˜åŒ–ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¿å­˜æˆ–é¿å…ç¼–è¯‘å™¨ç”Ÿæˆä½æ•ˆæŒ‡ä»¤ã€‚

------

##### (3) é«˜çº§æ€§èƒ½è°ƒä¼˜

- å¯ä»¥æ‰‹åŠ¨è°ƒåº¦æŒ‡ä»¤ï¼Œå‡å°‘ pipeline stallã€‚
- åœ¨æé™åœºæ™¯ï¼ˆæ¯”å¦‚ GPU kernel ä¼˜åŒ–åˆ°ç“¶é¢ˆï¼‰ä¸­ï¼Œinline PTX èƒ½æŒ¤å‡º 10%â€“30% çš„æ€§èƒ½ã€‚

------

#### ğŸ” 2. Inline PTX çš„å¯ç»´æŠ¤æ€§é£é™©

##### (1) å¯ç§»æ¤æ€§å·®

- PTX ä¸æ˜¯æœ€ç»ˆçš„æœºå™¨ç ï¼ˆSASSï¼‰ï¼Œä¸åŒ GPU æ¶æ„å¯¹ PTX çš„å®ç°å¯èƒ½ä¸åŒã€‚
- ä¸€æ®µåœ¨ **sm_70** ä¸Šå¥½ç”¨çš„ inline PTXï¼Œå¯èƒ½åœ¨ **sm_90** ä¸Šæ€§èƒ½é€€åŒ–ç”šè‡³æŠ¥é”™ã€‚

------

##### (2) å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§å·®

- PTX æ˜¯ä½å±‚æ±‡ç¼–ï¼Œä»£ç å†—é•¿ï¼Œéš¾ä»¥é˜…è¯»ã€‚
- å›¢é˜Ÿå¼€å‘æ—¶ï¼Œåç»­ç»´æŠ¤äººå‘˜å¯èƒ½æ ¹æœ¬çœ‹ä¸æ‡‚ï¼Œéš¾ä»¥ä¿®æ”¹å’Œæ‰©å±•ã€‚

------

##### (3) ä¸ç¼–è¯‘å™¨ä¼˜åŒ–å†²çª

- CUDA ç¼–è¯‘å™¨ä¼šå¯¹ C++ å’Œ PTX æ··åˆä»£ç è¿›è¡Œå¯„å­˜å™¨åˆ†é…ã€ä¼˜åŒ–ã€‚
- å¦‚æœ PTX éƒ¨åˆ†å’Œç¼–è¯‘å™¨ç”Ÿæˆçš„ä»£ç äº§ç”Ÿå†²çªï¼ˆå¯„å­˜å™¨åˆ†é…ä¸ä¸€è‡´ã€æ§åˆ¶æµå¼‚å¸¸ï¼‰ï¼Œå¯èƒ½å¯¼è‡´éš¾ä»¥å®šä½çš„ bugã€‚

------

##### (4) å‘å‰å…¼å®¹æ€§é£é™©

- æ–°ç‰ˆ CUDA toolkit æœ‰æ—¶ä¼šæ›´æ”¹ PTX æŒ‡ä»¤é›†æˆ–è¯­ä¹‰ã€‚
- å†…åµŒ PTX å¯èƒ½ç¼–è¯‘æŠ¥é”™ï¼Œæˆ–è€…è¡Œä¸ºå’Œæ—§ç‰ˆä¸åŒã€‚

------

#### ğŸ” 3. å®é™…ä½¿ç”¨å»ºè®®

âœ… **é€‚åˆç”¨ inline PTX çš„åœºæ™¯**ï¼š

- ç ”ç©¶/å®éªŒæ€§è´¨çš„æ€§èƒ½æé™ä¼˜åŒ–ã€‚
- CUDA å®˜æ–¹ API æš‚æœªè¦†ç›–çš„æœ€æ–°ç¡¬ä»¶æŒ‡ä»¤ã€‚
- å†…æ ¸çš„æ€§èƒ½ç“¶é¢ˆæéš¾ä¼˜åŒ–ï¼Œå¿…é¡»ç²¾ç»†æ§åˆ¶ã€‚

âš ï¸ **ä¸é€‚åˆç”¨çš„åœºæ™¯**ï¼š

- é¡¹ç›®éœ€è¦é•¿æœŸç»´æŠ¤ã€è·¨ GPU æ¶æ„è¿è¡Œã€‚
- ä»£ç ç”±å¤šäººåä½œå¼€å‘ã€‚
- ä¼˜åŒ–æ”¶ç›Šå¾ˆå°ï¼ˆ<10%ï¼‰ã€‚

------

#### âœ… æ€»ç»“

- **ä»·å€¼**ï¼šinline PTX èƒ½è®©ä½ è°ƒç”¨ CUDA API ä¸æä¾›çš„æŒ‡ä»¤ã€ç²¾ç»†æ§åˆ¶å¯„å­˜å™¨å’Œè®¿å­˜ï¼Œæ˜¯æ€§èƒ½æé™ä¼˜åŒ–çš„åˆ©å™¨ã€‚
- **é£é™©**ï¼šå¯è¯»æ€§å·®ã€å¯ç§»æ¤æ€§å·®ã€å’Œç¼–è¯‘å™¨ä¼˜åŒ–å®¹æ˜“å†²çªï¼Œé•¿æœŸç»´æŠ¤æˆæœ¬æé«˜ã€‚
- **å®è·µå»ºè®®**ï¼šèƒ½ç”¨ CUDA intrinsics/åº“å‡½æ•°å°±åˆ«ç”¨ PTXï¼ŒæŠŠ inline PTX ç•™åˆ°â€œæ€§èƒ½å¡æ­»åœ¨ç“¶é¢ˆ + å®˜æ–¹ API è¦†ç›–ä¸åˆ°â€çš„ç‰¹æ®Šåœºæ™¯ã€‚

### 5. occupancy å¹¶éè¶Šé«˜è¶Šå¥½ï¼Œä½•æ—¶ä¼šåå‘å½±å“æ€§èƒ½ï¼Ÿ

#### ğŸ” 1. ä»€ä¹ˆæ˜¯ Occupancy

- **å®šä¹‰**ï¼šä¸€ä¸ª SM ä¸Šçš„æ´»è·ƒ warp æ•° / è¯¥ SM çš„æœ€å¤§ warp æ•°ã€‚
- é«˜ occupancy â†’ æ›´å¤š warp å¹¶å‘ï¼Œèƒ½æ›´å¥½åœ°æ©ç›–å†…å­˜å»¶è¿Ÿã€‚
- ä½ occupancy â†’ å¹¶å‘ warp å°‘ï¼ŒGPU æ ¸å¿ƒå¯èƒ½ç©ºé—²ã€‚

------

#### ğŸ” 2. ä¸ºä»€ä¹ˆ â€œå¹¶éè¶Šé«˜è¶Šå¥½â€

##### (1) å¯„å­˜å™¨ & Shared Memory è¢«å‹ç¼©

- æ¯ä¸ªçº¿ç¨‹çš„å¯„å­˜å™¨æ•° = æ€»å¯„å­˜å™¨æ•° / æ´»è·ƒçº¿ç¨‹æ•°ã€‚
- å¦‚æœ occupancy è¿‡é«˜ï¼Œç¼–è¯‘å™¨ä¼šç»™æ¯ä¸ªçº¿ç¨‹åˆ†é…æ›´å°‘çš„å¯„å­˜å™¨ã€‚
- ä¸€æ—¦å¯„å­˜å™¨ä¸å¤Ÿç”¨ â†’ **å˜é‡ spill åˆ° local memory (æ˜¾å­˜)** â†’ è®¿å­˜å»¶è¿Ÿæ¯”å¯„å­˜å™¨é«˜å‡ ä¸ªæ•°é‡çº§ â†’ æ€§èƒ½ä¸‹é™ã€‚

ğŸ‘‰ **å…¸å‹åœºæ™¯**ï¼šä½ åœ¨å®éªŒ 4 (`--maxrregcount=32 vs 64`) å·²ç»çœ‹åˆ°ï¼š

- 32 å¯„å­˜å™¨ç‰ˆæœ¬ â†’ occupancy æ›´é«˜ï¼Œä½†å› ä¸ºå¯„å­˜å™¨ä¸å¤Ÿ â†’ spill â†’ åè€Œæ¯” 64 æ…¢äº† 8 å€ã€‚

------

##### (2) å†…æ ¸æ˜¯ Compute-bound è€Œä¸æ˜¯ Memory-bound

- Memory-bound å†…æ ¸ï¼šwarp å¤šä¸€ç‚¹å¯ä»¥æ©ç›–è®¿å­˜å»¶è¿Ÿ â†’ Occupancy æå‡æœ‰æ•ˆã€‚
- Compute-bound å†…æ ¸ï¼šä¸»è¦è€—æ—¶åœ¨ç®—æœ¯è¿ç®—ï¼Œä¸æ€ä¹ˆå¡å†…å­˜ã€‚
  - åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œoccupancy é«˜ä½å¯¹æ€§èƒ½å½±å“ä¸å¤§ã€‚
  - å¦‚æœä¸ºäº†è¿½æ±‚é«˜ occupancy è€Œé™ä½å¯„å­˜å™¨ â†’ åè€Œä¼šå› ä¸ºå¯„å­˜å™¨ä¸è¶³å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

------

##### (3) Warp Scheduling Overhead

- Warp å¤ªå¤šæ—¶ï¼Œè°ƒåº¦å™¨éœ€è¦åœ¨æ›´å¤š warp ä¸­åˆ‡æ¢ â†’ ä¸Šä¸‹æ–‡åˆ‡æ¢æˆæœ¬å¢åŠ ã€‚
- å¦‚æœ kernel æœ¬èº« latency ä¸é«˜ï¼Œwarp è¿‡å¤šä¼šè®©è°ƒåº¦å¼€é”€å¤§äºæ”¶ç›Šã€‚

------

##### (4) Cache / Bandwidth äº‰ç”¨

- Occupancy é«˜æ„å‘³ç€å¹¶å‘çº¿ç¨‹æ•°å¤šï¼Œ**åŒæ—¶è®¿é—®å†…å­˜çš„è¯·æ±‚ä¹Ÿæ›´å¤š**ã€‚
- å¯èƒ½å¯¼è‡´ **L1/L2 cache thrashing** æˆ– **DRAM å¸¦å®½é¥±å’Œ**ï¼Œç”šè‡³æ€§èƒ½ä¸‹é™ã€‚

------

#### ğŸ” 3. å®é™…ç»éªŒæ³•åˆ™

- **ä½ occupancy (<25%)**ï¼šä¸€èˆ¬æ€§èƒ½å·®ï¼Œå› ä¸º warp å¤ªå°‘ï¼Œæ— æ³•æ©ç›–è®¿å­˜å»¶è¿Ÿã€‚
- **ä¸­ç­‰ occupancy (50%~70%)**ï¼šå¸¸å¸¸æ˜¯æœ€ä½³ç‚¹ï¼Œå¯„å­˜å™¨å’Œå¹¶å‘åº¦å¹³è¡¡ã€‚
- **æé«˜ occupancy (90%~100%)**ï¼šä¸ä¸€å®šæ›´å¿«ï¼Œå°¤å…¶æ˜¯ compute-bound å†…æ ¸ï¼Œç”šè‡³æ›´æ…¢ã€‚

------

#### ğŸ” 4. å®è·µæ€ä¹ˆåˆ¤æ–­

1. ç”¨ **Nsight Compute** çœ‹æŒ‡æ ‡ï¼š
   - å¦‚æœ **Memory Latency High / Warp Stall: Memory Dependency** å æ¯”é«˜ â†’ æå‡ occupancy æœ‰ç”¨ã€‚
   - å¦‚æœ **Warp Stall: Execution Dependency** å æ¯”é«˜ â†’ è¯´æ˜æ˜¯è®¡ç®—ç“¶é¢ˆï¼Œoccupancy é«˜ä½æ— å…³ã€‚
2. å¯¹æ¯”ä¸åŒ `--maxrregcount` æˆ– block sizeï¼Œæ‰¾å‡º **æ—¶é—´æœ€çŸ­**çš„é…ç½®ï¼Œè€Œä¸æ˜¯ occupancy æœ€å¤§çš„ã€‚

------

#### âœ… æ€»ç»“

Occupancy åªæ˜¯ GPU å¹¶å‘èƒ½åŠ›çš„ä¸€ä¸ªæŒ‡æ ‡ï¼Œ**ä¸æ˜¯è¶Šé«˜è¶Šå¥½**ã€‚

- **å†…å­˜å—é™å†…æ ¸** â†’ é€‚å½“æå‡ occupancy æœ‰åˆ©ã€‚
- **è®¡ç®—å—é™å†…æ ¸** â†’ ç›²ç›®è¿½æ±‚é«˜ occupancy ä¼šå¯¼è‡´å¯„å­˜å™¨ä¸è¶³ã€spillã€cache thrashingï¼Œåè€Œæ€§èƒ½ä¸‹é™ã€‚

### 6. Tensor Core å‰ç½®æ¡ä»¶ï¼ˆshape/å¯¹é½/æ•°æ®ç±»å‹ï¼‰ï¼Ÿ

#### ğŸ” 1. çŸ©é˜µå°ºå¯¸ (Shape)

Tensor Core ç¡¬ä»¶çš„åŸºæœ¬è®¡ç®—å•å…ƒæ˜¯ **16Ã—16Ã—16 tile**ï¼š

- **M, N, K** éƒ½å¿…é¡»æ˜¯ **16 çš„å€æ•°**ï¼ˆAmpere æ”¯æŒ 8 çš„å€æ•°ï¼Œä½† 16 æœ€ä¿é™©ï¼‰ã€‚
- å¦‚æœçŸ©é˜µå¤§å°ä¸æ˜¯ 16 çš„å€æ•°ï¼ŒWMMA å†…æ ¸ä¼šé€€åŒ–åˆ° CUDA Core è®¡ç®—ï¼Œæ— æ³•ä½¿ç”¨ Tensor Coreã€‚

ä¾‹å­ï¼š

- âœ… `M=N=K=1024`ï¼ˆ16 çš„å€æ•° â†’ å¯ä»¥ï¼‰
- âŒ `M=1000`ï¼ˆä¸æ˜¯ 16 çš„å€æ•° â†’ é€€åŒ–ï¼‰

------

#### ğŸ” 2. å†…å­˜å¯¹é½ (Alignment)

- çŸ©é˜µå­˜å‚¨å¿…é¡»ä¿è¯ **16 å…ƒç´ å¯¹é½**ï¼ˆä¸€èˆ¬ç”¨ `__align__(128)` æˆ– `cudaMallocPitch`ï¼‰ã€‚
- è®¿å­˜å¿…é¡»æ»¡è¶³ **ldmatrix** æŒ‡ä»¤çš„å¯¹é½è¦æ±‚ï¼Œå¦åˆ™ä¼š fallback åˆ°æ™®é€š loadã€‚

å¸¸è§è¦æ±‚ï¼š

- æ¯è¡Œ pitch è‡³å°‘æ˜¯ **16 Ã— sizeof(dtype)** çš„å€æ•°ã€‚
- æ¯”å¦‚ FP16 (2B)ï¼špitch è‡³å°‘ 32B å¯¹é½ã€‚

------

#### ğŸ” 3. æ•°æ®ç±»å‹ (Data Type)

ä¸åŒæ¶æ„æ”¯æŒçš„ Tensor Core è¾“å…¥/è¾“å‡ºç²¾åº¦ä¸åŒï¼š

- **Volta (sm_70)**ï¼š
  - è¾“å…¥ï¼šFP16
  - è¾“å‡ºï¼šFP16 æˆ– FP32
- **Turing (sm_75)**ï¼š
  - è¾“å…¥ï¼šFP16, INT8
  - è¾“å‡ºï¼šFP16, FP32, INT32
- **Ampere (sm_80/86)**ï¼š
  - è¾“å…¥ï¼šFP16, BF16, TF32, INT8
  - è¾“å‡ºï¼šFP32 (float accumulate), INT32
- **Hopper (sm_90)**ï¼š
  - è¾“å…¥ï¼šFP16, BF16, FP8
  - è¾“å‡ºï¼šFP32

ç‰¹åˆ«è¯´æ˜ï¼š

- **TF32** æ˜¯ Ampere å¼•å…¥çš„ï¼Œè¾“å…¥ 19-bit (10-bit mantissa)ï¼Œaccumulate åˆ° FP32ã€‚
- **FP8** æ˜¯ Hopper å¼•å…¥çš„ã€‚

------

#### ğŸ” 4. è½¯ä»¶æ¥å£çš„é™åˆ¶

- **WMMA API (`nvcuda::wmma`)**ï¼š
  - å›ºå®šæ”¯æŒ `16Ã—16Ã—16` çš„ tileã€‚
  - æ•°æ®ç±»å‹å¿…é¡»æ˜¯ `half` / `wmma::precision::tf32` / `__nv_bfloat16`ã€‚
- **cuBLAS / CUTLASS**ï¼š
  - ä¼šè‡ªåŠ¨é€‰æ‹© Tensor Core è·¯å¾„ï¼Œä½†å‰ææ˜¯çŸ©é˜µè§„æ¨¡ã€å¯¹é½ã€æ•°æ®ç±»å‹æ»¡è¶³è¦æ±‚ã€‚
  - å¦åˆ™ fallback åˆ° CUDA Coreã€‚

------

#### ğŸ” 5. å¦‚ä½•ç¡®è®¤è‡ªå·±ç”¨äº† Tensor Coreï¼Ÿ

- **Nsight Compute** â†’ çœ‹ **Tensor Core Utilization (%)**ã€‚
- **Instruction Statistics** â†’ æŸ¥æ˜¯å¦æœ‰ `mma.sync.aligned.m16n16k16` æŒ‡ä»¤ã€‚

------

#### âœ… æ€»ç»“

è¦ç”¨ä¸Š Tensor Coreï¼Œå¿…é¡»æ»¡è¶³ï¼š

1. **çŸ©é˜µ shape**ï¼šM, N, K æ˜¯ 16 çš„å€æ•°ï¼ˆAmpere å¯æ”¾å®½åˆ° 8 çš„å€æ•°ï¼‰ã€‚
2. **å†…å­˜å¯¹é½**ï¼špitch å¯¹é½åˆ° 16 Ã— sizeof(dtype)ï¼Œä¿è¯ `ldmatrix` å¯¹é½ã€‚
3. **æ•°æ®ç±»å‹**ï¼šFP16/BF16/TF32/FP8/INT8ï¼Œå…·ä½“å–å†³äº GPU æ¶æ„ï¼›ç´¯åŠ ç»“æœä¸€èˆ¬æ˜¯ FP32ã€‚
4. **éªŒè¯æ–¹å¼**ï¼šç”¨ Nsight Computeï¼Œæ£€æŸ¥æ˜¯å¦å‡ºç° `mma.sync` æŒ‡ä»¤ã€‚

## 4ï¸âƒ£ å®éªŒ

### ğŸ§ª å®éªŒ 1ï¼šif-else vs `selp` (é¿å… warp divergence)

#### 1ï¸âƒ£  èƒŒæ™¯

Warp å†…åˆ†æ”¯å‘æ•£ä¼šå¯¼è‡´ä¸²è¡Œæ‰§è¡Œï¼Œæ•ˆç‡ä¸‹é™ã€‚
 å¯ä»¥ç”¨ **ä¸‰ç›®è¿ç®—ç¬¦** æˆ–ç¼–è¯‘å™¨ `selp` æŒ‡ä»¤æ¶ˆé™¤åˆ†æ”¯

#### 2ï¸âƒ£ ä»£ç ï¼š`warp_diverge.cu`

```c++
#include <stdio.h>
#include <cuda_runtime.h>

__global__ void kernel_if(int *out, const int *in, int N) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < N) {
        if (in[tid] % 2 == 0)
            out[tid] = in[tid] * 2;
        else
            out[tid] = in[tid] * 3;
    }
}

__global__ void kernel_selp(int *out, const int *in, int N) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < N) {
        int val = in[tid];
        // ä¸‰ç›®è¿ç®—é¿å…åˆ†æ”¯å‘æ•£
        out[tid] = (val % 2 == 0) ? (val * 2) : (val * 3);
    }
}

int main() {
    const int N = 1<<20;
    size_t bytes = N * sizeof(int);

    int *h_in = (int*)malloc(bytes);
    int *h_out = (int*)malloc(bytes);
    for (int i = 0; i < N; i++) h_in[i] = i;

    int *d_in, *d_out;
    cudaMalloc(&d_in, bytes);
    cudaMalloc(&d_out, bytes);
    cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice);

    dim3 block(256);
    dim3 grid((N+block.x-1)/block.x);

    // è®¡æ—¶
    cudaEvent_t start, stop;
    float ms;

    cudaEventCreate(&start); cudaEventCreate(&stop);
    cudaEventRecord(start);
    kernel_if<<<grid, block>>>(d_out, d_in, N);
    cudaEventRecord(stop); cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);
    printf("if-else kernel: %.3f ms\n", ms);

    cudaEventRecord(start);
    kernel_selp<<<grid, block>>>(d_out, d_in, N);
    cudaEventRecord(stop); cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);
    printf("selp kernel: %.3f ms\n", ms);

    cudaFree(d_in); cudaFree(d_out);
    free(h_in); free(h_out);
}
```

#### 3ï¸âƒ£ è¿è¡Œ

```bash
nvcc -O2 warp_diverge.cu -o warp_diverge
./warp_diverge
```

#### 4ï¸âƒ£ è¾“å‡º

![image-20250904011949013](./report_day10.assets/image-20250904011949013.png)

- `selp kernel` æ—¶é—´æ›´çŸ­ã€‚
- Nsight Compute â†’ **Warp Execution Efficiency** æ›´é«˜ã€‚


### ğŸ§ª å®éªŒ 2ï¼šLoop Unrolling

#### 1ï¸âƒ£ èƒŒæ™¯

æ‰‹åŠ¨å±•å¼€å¾ªç¯èƒ½å‡å°‘å¾ªç¯æ§åˆ¶å¼€é”€ï¼Œä½†ä¼šå¢åŠ å¯„å­˜å™¨æ•°ã€‚

#### 2ï¸âƒ£ä»£ç ç‰‡æ®µï¼ˆä¿®æ”¹ `mmul_shared.cu` å†…éƒ¨å¾ªç¯ï¼‰

```c++
#pragma unroll 4
for (int k = 0; k < TILE; k++) {
    sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];
}
```

#### 3ï¸âƒ£ ç¼–è¯‘ï¼ˆå¯¹æ¯”ä¸‰ç§æƒ…å†µï¼‰

```bash
# baselineï¼šä¸åŠ  pragma unroll
nvcc -O2 -Xptxas -v mmul_shared.cu -o mmul_base

# unroll 4
 nvcc -O2 -Xptxas -v mmul_shared_4.cu -o mmul_unroll4

# æ‰‹åŠ¨æ”¹æˆ #pragma unroll 8
nvcc -O2 -Xptxas -v mmul_shared_8.cu -o mmul_unroll8
```

ç¼–è¯‘è¾“å‡ºé‡Œä¼šçœ‹åˆ°å¯„å­˜å™¨ä½¿ç”¨æƒ…å†µï¼Œä¾‹å¦‚ï¼š

![image-20250904013445271](./report_day10.assets/image-20250904013445271.png)

ğŸ‘‰ è®°å½•ä¸‰ç§ç‰ˆæœ¬çš„å¯„å­˜å™¨æ•°ã€‚

#### 4ï¸âƒ£ è¾“å‡º

![image-20250904013654532](./report_day10.assets/image-20250904013654532.png)

#### 5ï¸âƒ£ Nsight Compute åˆ†æ

è¿è¡Œï¼š

```
ncu --set full ./mmul_unroll4
```

é‡ç‚¹çœ‹ï¼š

- **Registers per Thread**
- **Achieved Occupancy**
- **Duration**

ğŸ‘‰ ç°è±¡ï¼š

- å¾ªç¯å±•å¼€åº¦è¶Šé«˜ â†’ æ—¶é—´æ›´çŸ­ï¼Œä½†å¯„å­˜å™¨æ•°ä¸Šå‡ã€‚
- å¦‚æœå¯„å­˜å™¨æ•°å¤ªå¤§ â†’ Occupancy ä¸‹é™ï¼Œç”šè‡³è§¦å‘ spill â†’ æ€§èƒ½åè€Œä¸‹é™ã€‚

| Unroll   | Registers/Thread (from `-Xptxas -v`) | Achieved Occupancy (%) | Duration (ms) |
|   -- |              |       ---- |     - |
| baseline | 27                                   | 85.66                  | 0.386         |
| unroll=4 | 25(ç¼–è¯‘å™¨åœ¨å±•å¼€æ—¶ä¼˜åŒ–äº†å¯„å­˜å™¨åˆ†é…)   | 86.30                  | 0.335         |
| unroll=8 | 27                                   | 86.30                  | 0.257         |

### ğŸ§ª å®éªŒ 3ï¼šAoS vs SoA

#### 1ï¸âƒ£ èƒŒæ™¯

- **AoS (Array of Structures)**ï¼šæ¯ä¸ªç²’å­æ˜¯ä¸€ä¸ªç»“æ„ä½“ `{x,y,z}`ï¼Œè¿ç»­å­˜å‚¨ã€‚

  - Warp å†…ä¸åŒçº¿ç¨‹è®¿é—® `arr[tid].x` â†’ ç›¸é‚»çº¿ç¨‹çš„åœ°å€è·¨åº¦ = `sizeof(ParticleAoS)` (12 å­—èŠ‚)ï¼Œä¸å¯¹é½ï¼Œå¯¼è‡´è®¿å­˜ä¸åˆå¹¶ã€‚

  **SoA (Structure of Arrays)**ï¼šåˆ†åˆ«å­˜å‚¨ `x[]`, `y[]`, `z[]`ï¼Œæ¯ä¸ªæ•°ç»„è¿ç»­ã€‚

  - Warp å†…ä¸åŒçº¿ç¨‹è®¿é—® `x[tid]` â†’ ç›¸é‚»çº¿ç¨‹åœ°å€ç›¸å·® 4 å­—èŠ‚ï¼Œå®Œç¾è¿ç»­ï¼Œåˆå¹¶è®¿å­˜æ•ˆç‡æ¥è¿‘ 100%ã€‚

#### 2ï¸âƒ£ ä»£ç ï¼š`aos_soa.cu`

```c++
#include <cuda_runtime.h>
#include <stdio.h>

// AoS
struct ParticleAos
{
    float x, y, z;
};

__global__ void kernel_aos(ParticleAos* arr, float* out, int N)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < N)
    {
        out[tid] = arr[tid].x + arr[tid].y + arr[tid].z;
    }
}

// SoA
__global__ void kernel_soa(float* x, float* y, float* z, float* out, int N)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < N)
    {
        out[tid] = x[tid] + y[tid] + z[tid];
    }
}

// è®¡æ—¶å‡½æ•°å°è£…
template <typename Kernel_Func, typename... Args>
float runKernel(Kernel_Func Kernel, dim3 grid, dim3 block, Args... args)
{
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    Kernel<<<grid, block>>>(args...);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    cudaEventDestroy(start);
    cudaEventDestroy(stop);

    return ms;
}

int main()
{
    const int N = 1 << 20;
    size_t bytes = N * sizeof(float);

    float* host_x = (float*)malloc(bytes);
    float* host_y = (float*)malloc(bytes);
    float* host_z = (float*)malloc(bytes);
    for (int i = 0; i < N; i++)
    {
        host_x[i] = 1.0f;
        host_y[i] = 2.0f;
        host_z[i] = 3.0f;
    }

    ParticleAos* host_aos = (ParticleAos*)malloc(N * sizeof(ParticleAos));
    for (int i = 0; i < N; i++)
    {
        host_aos[i].x = 1.0f;
        host_aos[i].y = 2.0f;
        host_aos[i].z = 3.0f;
    }

    ParticleAos* device_aos;
    float *device_x, *device_y, *device_z, *device_out;
    cudaMalloc(&device_aos, N * sizeof(ParticleAos));
    cudaMalloc(&device_x, bytes);
    cudaMalloc(&device_y, bytes);
    cudaMalloc(&device_z, bytes);
    cudaMalloc(&device_out, bytes);

    cudaMemcpy(device_aos, host_aos, N * sizeof(ParticleAos), cudaMemcpyHostToDevice);
    cudaMemcpy(device_x, host_x, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_y, host_y, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_z, host_z, bytes, cudaMemcpyHostToDevice);

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    // AoS å†…æ ¸è®¡æ—¶
    float time_aos = runKernel(kernel_aos, grid, block, device_aos, device_out, N);
    // SoA å†…æ ¸è®¡æ—¶
    float time_soa =
        runKernel(kernel_soa, grid, block, device_x, device_y, device_z, device_out, N);

    printf("AoS kernel time = %.3f ms\n", time_aos);
    printf("SoA kernel time = %.3f ms\n", time_soa);

    cudaFree(device_aos);
    cudaFree(device_x);
    cudaFree(device_y);
    cudaFree(device_z);
    cudaFree(device_out);

    free(host_x);
    free(host_y);
    free(host_z);
    free(host_aos);

    return 0;
}

```

#### 3ï¸âƒ£ ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 aos_soa.cu -o aos_soa
./aos_soa
```

#### 4ï¸âƒ£ è¾“å‡º

![image-20250904023059526](./report_day10.assets/image-20250904023059526.png)

ğŸ‘‰ **SoA æ›´å¿«**ï¼Œå› ä¸º warp å†…å­˜è®¿é—®è¿ç»­ â†’ **åˆå¹¶è®¿å­˜**ã€‚

### ğŸ§ª å®éªŒ 4ï¼šå¯„å­˜å™¨é™åˆ¶ `--maxrregcount`

#### 1ï¸âƒ£ èƒŒæ™¯

- æ¯ä¸ª SM çš„å¯„å­˜å™¨èµ„æºæ˜¯æœ‰é™çš„ã€‚
- `--maxrregcount=N` å¼ºè¡Œé™åˆ¶ç¼–è¯‘å™¨ç»™æ¯ä¸ªçº¿ç¨‹åˆ†é…çš„å¯„å­˜å™¨æ•°ã€‚
- é™åˆ¶å¯„å­˜å™¨æ•°çš„æ•ˆæœï¼š
  - **ä¼˜ç‚¹**ï¼šèƒ½è®©æ›´å¤šçº¿ç¨‹/warp å¹¶å‘ï¼ˆOccupancy â†‘ï¼‰ã€‚
  - **ç¼ºç‚¹**ï¼šå¦‚æœå¯„å­˜å™¨ä¸å¤Ÿç”¨ï¼Œå˜é‡ä¼šæº¢å‡ºåˆ° **local memoryï¼ˆæ˜¾å­˜ï¼‰**ï¼Œå»¶è¿Ÿéå¸¸å¤§ï¼Œå¯èƒ½æ€§èƒ½åè€Œæ›´å·®ã€‚

#### 2ï¸âƒ£ å‡†å¤‡ä»£ç 

æˆ‘ä»¬ç”¨ **å…±äº«å†…å­˜çŸ©é˜µä¹˜** (`mmul_shared.cu`)ï¼Œé‡Œé¢å¯„å­˜å™¨éœ€æ±‚æ¯”è¾ƒé«˜ï¼Œèƒ½æ˜æ˜¾çœ‹åˆ°å·®å¼‚ã€‚

```c++
#include <cuda_runtime.h>
#include <stdio.h>

#define TILE 16
// shared memory
__global__ void mmul_shared(const float* A, const float* B, float* C, int N)
{
    __shared__ float a_shared[TILE][TILE];
    __shared__ float b_shared[TILE][TILE];

    int row = blockIdx.y * TILE + threadIdx.y;
    int col = blockIdx.x * TILE + threadIdx.x;

    float sum = 0;
    for (int t = 0; t < (N + TILE - 1) / TILE; t++)
    {
        if (row < N && (t * TILE + threadIdx.x) < N)
        {
            a_shared[threadIdx.y][threadIdx.x] = A[row * N + t * TILE + threadIdx.x];
        }
        else
        {
            a_shared[threadIdx.y][threadIdx.x] = 0;
        }

        if (col < N && (t * TILE + threadIdx.y) < N)
        {
            b_shared[threadIdx.y][threadIdx.x] = B[(t * TILE + threadIdx.y) * N + col];
        }
        else
        {
            b_shared[threadIdx.y][threadIdx.x] = 0;
        }

        __syncthreads();

        for (int k = 0; k < TILE; k++)
        {
            sum += a_shared[threadIdx.y][k] * b_shared[k][threadIdx.x];
        }

        __syncthreads();
    }
    if (row < N && col < N)
    {
        C[row * N + col] = sum;
    }
}

int main()
{
    const int N = 512;
    size_t bytes = N * N * sizeof(float);

    float* host_a = (float*)malloc(bytes);
    float* host_b = (float*)malloc(bytes);
    float* host_c = (float*)malloc(bytes);

    for (int i = 0; i < N * N; i++)
    {
        host_a[i] = 1.0f;
        host_b[i] = 2.0f;
    }

    float *device_a, *device_b, *device_c;
    cudaMalloc(&device_a, bytes);
    cudaMalloc(&device_b, bytes);
    cudaMalloc(&device_c, bytes);

    cudaMemcpy(device_a, host_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes, cudaMemcpyHostToDevice);

    dim3 block(TILE, TILE);
    dim3 grid((N + TILE - 1) / TILE, (N + TILE - 1) / TILE);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    mmul_shared<<<grid, block>>>(device_a, device_b, device_c, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    cudaEventDestroy(start);
    cudaEventDestroy(stop);

    cudaMemcpy(host_c, device_c, bytes, cudaMemcpyDeviceToHost);

    printf("Shared GEMM time = %.3f ms, result C[0]=%.1f\n", ms, host_c[0]);

    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);

    free(host_a);
    free(host_b);
    free(host_c);

    return 0;
}

```

#### 3ï¸âƒ£ ç¼–è¯‘

åˆ†åˆ«ç¼–è¯‘ä¸åŒå¯„å­˜å™¨é™åˆ¶ç‰ˆæœ¬ï¼š

```bash
# é™åˆ¶å¯„å­˜å™¨æ•°ä¸º 32
nvcc -O2 --maxrregcount=32 mmul_shared.cu -o mmul_r32

# é™åˆ¶å¯„å­˜å™¨æ•°ä¸º 64
nvcc -O2 --maxrregcount=64 mmul_shared.cu -o mmul_r64

# ä¸é™åˆ¶ï¼ˆbaselineï¼‰
nvcc -O2 mmul_shared.cu -o mmul_base
```

#### 4ï¸âƒ£ è¿è¡Œ & å¯¹æ¯”

```
./mmul_base
./mmul_r32
./mmul_r64
```

è®°å½•æ¯ä¸ªç‰ˆæœ¬çš„è¿è¡Œæ—¶é—´ã€‚

![image-20250904030736193](./report_day10.assets/image-20250904030736193.png)

#### 5ï¸âƒ£ Nsight Compute åˆ†æ

è¿è¡Œ Nsight æŸ¥çœ‹å¯„å­˜å™¨å’Œ local memory ä½¿ç”¨æƒ…å†µï¼š

```
ncu --kernel-name regex:mmul_shared ./mmul_r32
ncu --kernel-name regex:mmul_shared ./mmul_r64
```

é‡ç‚¹çœ‹ä¸¤ä¸ªåœ°æ–¹ï¼š

- **Launch Statistics**
  - Registers Per Thread
- **Memory Workload Analysis**
  - Local Memory Access (æ˜¯å¦æœ‰ spill)


#### âœ… ç»“æœ

| å†…æ ¸ç‰ˆæœ¬                  | Registers/Thread | Achieved Occupancy | Kernel Time (ms) | è¯´æ˜                              |
|         - |     ---- |        |     ---- |           --- |
| r32 (`--maxrregcount=32`) | 32               | ~85%               | 3568 ms          | å¯„å­˜å™¨ä¸è¶³ï¼Œå¤§é‡ spill â†’ æ€§èƒ½æå·® |
| r64 (`--maxrregcount=64`) | 38               | ~85%               | 443 ms           | å¯„å­˜å™¨å¤Ÿç”¨ï¼Œspill å°‘ â†’ æ€§èƒ½æ­£å¸¸   |

**ç»“è®º**ï¼šOccupancy å¹¶éè¶Šé«˜è¶Šå¥½ã€‚å¯„å­˜å™¨é™åˆ¶è¿‡ä¸¥ä¼šå¯¼è‡´ spill åˆ° local memoryï¼Œä¸¥é‡æ‹–æ…¢æ€§èƒ½ã€‚éœ€è¦åœ¨ **å¯„å­˜å™¨æ•°é‡ vs occupancy** ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹

### ğŸ§ª å®éªŒ 5ï¼šWMMA (Tensor Core) vs æ™®é€š Shared Tiling

#### 1ï¸âƒ£ èƒŒæ™¯çŸ¥è¯†

- **æ™®é€š shared tiling GEMM**ï¼šç”¨ shared memory åš block tilingï¼Œæ¯ä¸ªçº¿ç¨‹å—è®¡ç®—ä¸€ä¸ª tileã€‚è®¡ç®—å…¨é  CUDA coreï¼Œæ•ˆç‡å—é™ã€‚
- **WMMA API**ï¼šWarp Matrix Multiply Accumulateï¼Œè°ƒç”¨ç¡¬ä»¶ **Tensor Core**ï¼Œåœ¨ FP16/TF32 ä¸‹ååé‡æ¯” CUDA core é«˜å¾ˆå¤šã€‚
- é™åˆ¶æ¡ä»¶ï¼š
  - Tile å¤§å°å›ºå®šä¸º **16Ã—16Ã—16**ã€‚
  - æ•°æ®ç±»å‹å¿…é¡»æ˜¯ `half` (FP16)ã€`bf16` æˆ– `tf32`ã€‚
  - çŸ©é˜µç»´åº¦å¿…é¡»æ˜¯ 16 çš„å€æ•°ã€‚

------

#### 2ï¸âƒ£ å®Œæ•´ä»£ç ï¼š`wmma_gemm.cu`

```c++
#include <cuda_fp16.h>
#include <mma.h>
#include <stdio.h>

using namespace nvcuda;

#define M 16
#define N 16
#define K 16

// WMMA GEMM kernel
__global__ void wmma_geem(half* a, half* b, float* c)
{
    // æ¯ä¸ª warp è®¡ç®—ä¸€ä¸ª 16 * 16 tile
    wmma::fragment<wmma::matrix_a, M, N, K, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, M, N, K, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, M, N, K, float> c_frag;

    // warp å†… id
    int warpId = (blockIdx.x * blockDim.x + threadIdx.x) / 32;

    // æ¯ä¸ª warp åŠ è½½ä¸€ä¸ª tile
    wmma::load_matrix_sync(a_frag, a + warpId * M * N * K, K);
    wmma::load_matrix_sync(b_frag, a + warpId * M * N * K, K);

    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

    wmma::store_matrix_sync(c + warpId * M * N, c_frag, N, wmma::mem_row_major);
}

int main()
{
    int numWarps = 1;
    int numThreads = 32 * numWarps;

    size_t bytes_a = M * K * sizeof(half);
    size_t bytes_b = K * N * sizeof(half);
    size_t bytes_c = M * N * sizeof(float);

    half* host_a = (half*)malloc(bytes_a);
    half* host_b = (half*)malloc(bytes_b);
    float* host_c = (float*)malloc(bytes_c);

    // åˆå§‹åŒ–
    for (int i = 0; i < M * K; i++)
    {
        host_a[i] = __float2half(1.0f);
    }

    for (int i = 0; i < K * N; i++)
    {
        host_b[i] = __float2half(2.0f);
    }

    half *device_a, *device_b;
    float* device_c;

    cudaMalloc(&device_a, bytes_a);
    cudaMalloc(&device_b, bytes_b);
    cudaMalloc(&device_c, bytes_c);

    cudaMemcpy(device_a, host_a, bytes_a, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes_b, cudaMemcpyHostToDevice);

    // å¯åŠ¨kernel
    wmma_geem<<<1, numThreads>>>(device_a, device_b, device_c);
    cudaMemcpy(host_c, device_c, bytes_c, cudaMemcpyDeviceToHost);

    printf("C[0] = %.1f\n", host_c[0]);

    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);
    free(host_a);
    free(host_b);
    free(host_c);

    return 0;
}

```

------

#### 3ï¸âƒ£ ç¼–è¯‘

æ³¨æ„ï¼šWMMA éœ€è¦ **Volta (sm_70) åŠä»¥ä¸Š**ï¼Œå»ºè®®ç”¨ `sm_75` (Turing) æˆ– `sm_80` (Ampere)ï¼š

```bash
nvcc -arch=sm_75 -O2 wmma_gemm.cu -o wmma_gemm
```

------

#### 4ï¸âƒ£ è¿è¡Œ

```bash
./wmma_gemm
```

è¾“å‡ºï¼š

![image-20250904033914669](./report_day10.assets/image-20250904033914669.png)

å› ä¸ºè®¡ç®—çš„æ˜¯ `1Ã—2` ç´¯åŠ  16 æ¬¡ã€‚

------

#### 5ï¸âƒ£ Nsight Compute å¯¹æ¯”

##### æ™®é€š Shared Tiling GEMM

```
ncu --kernel-name regex:mmul_shared ./mmul_shared
```

è§‚å¯Ÿï¼š

- **Tensor Core Utilization = 0%**
- å…¨éƒ¨è®¡ç®—åœ¨ CUDA core ä¸Šå®Œæˆã€‚

##### WMMA GEMM

```
ncu --kernel-name regex:wmma_gemm ./wmma_gemm
```

è§‚å¯Ÿï¼š

- **Tensor Core Utilization > 0%**ï¼ˆå¦‚æœçŸ©é˜µè¶³å¤Ÿå¤§ï¼Œå¯ä»¥æ¥è¿‘ 100%ï¼‰
- FP16 Tensor Core æŒ‡ä»¤ï¼ˆ`mma.sync`ï¼‰åœ¨æŒ‡ä»¤ç»Ÿè®¡ä¸­å‡ºç°ã€‚

### âœ… æ€»ç»“

1. **if-else vs selp** â†’ Warp divergence æ•ˆç‡å·®å¼‚ã€‚
2. **Unrolling** â†’ å¾ªç¯å±•å¼€æå‡æ€§èƒ½ï¼Œä½†å¯„å­˜å™¨è†¨èƒ€é£é™©ã€‚
3. **AoS vs SoA** â†’ SoA æ›´é€‚åˆåˆå¹¶è®¿å­˜ã€‚
4. **maxrregcount** â†’ é™åˆ¶å¯„å­˜å™¨ä¼šå¢åŠ  occupancyï¼Œä½†å¯èƒ½å¯¼è‡´ spillã€‚
5. **WMMA** â†’ Tensor Core æ˜¾è‘—åŠ é€Ÿ GEMMã€‚

## 5ï¸âƒ£ Nsight Profiling æŒ‡å—

#### å¸¸ç”¨æŒ‡æ ‡

- **Warp Execution Efficiency**ï¼ˆåˆ†æ”¯å‘æ•£ï¼‰
- **Global Load Efficiency**ï¼ˆè®¿å­˜æ˜¯å¦åˆå¹¶ï¼‰
- **Registers per Thread**ï¼ˆå¯„å­˜å™¨å‹åŠ›ï¼‰
- **Achieved Occupancy**ï¼ˆå¹¶å‘åº¦ï¼‰
- **Tensor Core Utilization**ï¼ˆä½¿ç”¨ WMMA æ—¶å…³æ³¨ï¼‰

#### ä½¿ç”¨æ–¹æ³•

```
ncu --set full ./mmul_naive
ncu --set full ./mmul_shared
```

æŸ¥çœ‹ Memory Workload Analysisã€Scheduler Statisticsã€‚
