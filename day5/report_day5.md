# Day 5 â€” Triton Inference Server

## ğŸ¯ ä»Šæ—¥ç›®æ ‡

1. å®‰è£…å¹¶è¿è¡Œ **Triton Inference Server**
2. ç†è§£ **æ¨¡å‹ä»“åº“ï¼ˆmodel repositoryï¼‰** ç»“æ„
3. éƒ¨ç½²ä¸€ä¸ª ONNX æ¨¡å‹åˆ° Triton
4. ç”¨ **Python Client (HTTP/gRPC)** å‘èµ·æ¨ç†è¯·æ±‚

------

## ä¸€ã€å‡†å¤‡æ¨¡å‹ä»“

### æ”¾å…¥æ¨¡å‹

æŠŠä½ ä¹‹å‰ **Day3 å¯¼å‡ºçš„ `mlp.onnx`** æ–‡ä»¶å¤åˆ¶åˆ°ï¼š

```bash
mkdir -p models/mnist_mlp/1
cp mlp.onnx models/mnist_mlp/1/model.onnx
```

### å†™ `config.pbtxt`

æ–°å»ºæ–‡ä»¶ `models/mnist_mlp/config.pbtxt`ï¼š

```bash
name: "mnist_mlp"
platform: "onnxruntime_onnx"
max_batch_size: 8192

input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [1,28,28]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [10]
  }
]
```

------

### Triton è¦æ±‚ç‰¹å®šç›®å½•ç»“æ„ï¼š

```bash
models/
â””â”€â”€ mnist_mlp/
    â””â”€â”€ 1/
        â””â”€â”€ model.onnx
    â””â”€â”€ config.pbtxt
```

- `mnist_mlp/` â†’ æ¨¡å‹åå­—
- `1/` â†’ ç‰ˆæœ¬å·
- `model.onnx` â†’ ONNX æ¨¡å‹æ–‡ä»¶
- `config.pbtxt` â†’ é…ç½®æ–‡ä»¶

## äºŒã€å‡†å¤‡ç¯å¢ƒ

æ‹‰å– Triton å®˜æ–¹é•œåƒï¼ˆæ¨è GPU ç‰ˆï¼‰ï¼š

```bash
docker pull nvcr.io/nvidia/tritonserver:23.05-py3
```

è¿è¡Œå®¹å™¨ï¼ˆæ˜ å°„ä¸‰ç§ç«¯å£ï¼šHTTP 8000, gRPC 8001, Metrics 8002ï¼‰ï¼š

```bash
docker run --gpus all --rm \
  -p8000:8000 -p8001:8001 -p8002:8002 \
  -v $PWD/models:/models \
  nvcr.io/nvidia/tritonserver:23.05-py3 \
  tritonserver --model-repository=/models
```

è¯´æ˜ï¼š

- `--gpus all` â†’ ä½¿ç”¨ GPU
- `-v $PWD/models:/models` â†’ æœ¬åœ°æ¨¡å‹ç›®å½•æŒ‚è½½åˆ°å®¹å™¨

------

## ä¸‰ã€å¯åŠ¨ Triton

è¿›å…¥ Triton å®¹å™¨åï¼Œå¯åŠ¨æ—¥å¿—é‡Œåº”è¯¥èƒ½çœ‹åˆ°ï¼š

![image-20250827034226486](./report_day5.assets/image-20250827034226486.png)

æµ‹è¯•æœåŠ¡æ˜¯å¦å¯ç”¨ï¼š

```bash
curl -v localhost:8000/v2/health/ready
```

è¿”å› `200 OK` è¡¨ç¤ºæœåŠ¡å¯ç”¨ã€‚

![image-20250827034315044](./report_day5.assets/image-20250827034315044.png)

------

## å››ã€å®‰è£…å®¢æˆ·ç«¯ SDK

æ–°å¼€ä¸€ä¸ªç»ˆç«¯ï¼Œå®‰è£… Python Clientï¼š

```bash
pip install tritonclient[all] # zsh ä½¿ç”¨ pip install "tritonclient[all]"
```

------

## äº”ã€å†™æ¨ç†å®¢æˆ·ç«¯

ä¿å­˜ä¸º `client_infer.py`ï¼š

```python
import numpy as np
import tritonclient.http as http
import time

# è¿æ¥ Triton HTTP æœåŠ¡
client = http.InferenceServerClient(url="localhost:8000")

# æ„é€ è¾“å…¥ï¼šbatch=8192ï¼Œæ¯ä¸ªæ ·æœ¬ 1x28x28
batch_size = 8192
x = np.random.randn(batch_size, 1, 28, 28).astype(np.float32)

# å®šä¹‰è¾“å…¥
inputs = [http.InferInput("input", x.shape, "FP32")]
inputs[0].set_data_from_numpy(x)

# å®šä¹‰è¾“å‡º
outputs = [http.InferRequestedOutput("output")]

# æ¨ç†å¹¶è®¡æ—¶
start = time.time()
result = client.infer(model_name="mnist_mlp", inputs=inputs, outputs=outputs)
end = time.time()

# ç»“æœ
output_data = result.as_numpy("output")
print(f"æ¨ç†ç»“æœ shape: {output_data.shape}")   # åº”è¯¥æ˜¯ (8192, 10)
print(f"è€—æ—¶: {end - start:.4f} ç§’")
print("ç¤ºä¾‹è¾“å‡ºå‰2ä¸ªæ ·æœ¬:", output_data[:2])
```

è¿è¡Œï¼š

```bash
python client_infer.py
```

![image-20250827035243247](./report_day5.assets/image-20250827035243247.png)

------

## å…­ã€ï¼ˆè¿›é˜¶ï¼‰K8s éƒ¨ç½²

ç¼–å†™ `triton-deploy.yaml`ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton
  template:
    metadata:
      labels:
        app: triton
    spec:
      containers:
      - name: triton
        image: nvcr.io/nvidia/tritonserver:23.05-py3
        args: ["tritonserver", "--model-repository=/models"]
        ports:
        - containerPort: 8000
        - containerPort: 8001
        - containerPort: 8002
        resources:
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-repo
          mountPath: /models
      volumes:
      - name: model-repo
        hostPath:
          path: /models
```

åœ¨day5ç›®å½•ä¸‹ï¼š

```bash
# è®©å®¿ä¸»æœºç›®å½•æŒ‚è½½åˆ° minikube VMï¼š

minikube mount ./models:/models
```

å¦èµ·ä¸€ä¸ªç»ˆç«¯éƒ¨ç½²ï¼š

```bash
kubectl delete -f triton-deploy.yaml
kubectl apply -f triton-deploy.yaml
kubectl get pods
# å¦‚æœå¤±è´¥å¯ä»¥ä½¿ç”¨ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹é”™è¯¯
kubectl describe pod triton-server
# æŸ¥çœ‹æ—¥å¿—
kubectl logs deployment/triton-server
```

![image-20250827164858025](./report_day5.assets/image-20250827164858025.png)

![image-20250827180225599](./report_day5.assets/image-20250827180225599.png)

çœ‹åˆ°modelåŠ è½½æˆåŠŸå³å¯
