# Day 9 - CUDA å†…å­˜ç®¡ç†

## ğŸ¯ å­¦ä¹ ç›®æ ‡

1. ç†è§£ GPU çš„å¤šçº§å­˜å‚¨ä½“ç³»ï¼ˆregisterã€sharedã€globalã€constantã€textureã€L2 cacheï¼‰ã€‚
2. å­¦ä¼šä½¿ç”¨ **constant memory** å’Œ **texture memory**ã€‚
3. æŒæ¡ shared memory **bank conflict** äº§ç”ŸåŸå› å’Œè§£å†³æ–¹æ³•ã€‚
4. ç”¨ Nsight Compute è§‚å¯Ÿä¸åŒå†…å­˜å±‚æ¬¡çš„åˆ©ç”¨ç‡ï¼ˆcache hit / bank conflictï¼‰ã€‚

------

## 1ï¸âƒ£CUDA å†…å­˜å±‚æ¬¡å¤ä¹ 

| ç±»å‹         | ä½œç”¨èŒƒå›´   | ç‰¹ç‚¹                    | å»¶è¿Ÿ           | å…¸å‹ç”¨é€”            |
| ------------ | ---------- | ----------------------- | -------------- | ------------------- |
| å¯„å­˜å™¨       | æ¯çº¿ç¨‹ç§æœ‰ | æœ€å¿«ï¼Œæ•°é‡æœ‰é™          | ~1 cycle       | ä¿å­˜å±€éƒ¨å˜é‡        |
| Shared Mem   | æ¯ä¸ª Block | Block å†…å…±äº«ï¼Œéœ€åŒæ­¥    | ~10 cycles     | çº¿ç¨‹é€šä¿¡ã€tile ç¼“å­˜ |
| Global Mem   | å…¨å±€å¯è§   | å¸¦å®½å¤§ï¼Œä½†å»¶è¿Ÿé«˜        | 400â€“800 cycles | ä¸»æ•°æ®å­˜å‚¨          |
| Constant Mem | å…¨å±€åªè¯»   | å¹¿æ’­ä¼˜åŒ–ï¼Œwarp å†…é«˜æ•ˆ   | ~å¯„å­˜å™¨é€Ÿåº¦    | è¶…å‚æ•°ã€å·ç§¯æ ¸      |
| Texture Mem  | å…¨å±€åªè¯»   | ç©ºé—´å±€éƒ¨æ€§ cache + æ’å€¼ | ~100 cycles    | å›¾åƒå¤„ç†ã€é‡‡æ ·/æ’å€¼ |
| L2 Cache     | å…¨å±€å…±äº«   | SM ä¹‹é—´å…±äº«ï¼Œ128B è¡Œå®½  | 100â€“200 cycles | ç¼“è§£å…¨å±€å†…å­˜å»¶è¿Ÿ    |

------

## 2ï¸âƒ£ åŸºç¡€å®éªŒï¼šHello Shared Memory

### èƒŒæ™¯

- æ¯ä¸ª block çš„çº¿ç¨‹å¯ä»¥é€šè¿‡ **shared memory** å…±äº«æ•°æ®ã€‚
- å¿…é¡»ç”¨ `__syncthreads()` ä¿è¯çº¿ç¨‹åŒæ­¥ï¼Œå¦åˆ™å¯èƒ½æœ‰çº¿ç¨‹è¿˜æ²¡å†™æ•°æ®å°±è¢«åˆ«äººè¯»èµ°ã€‚

### ä»£ç ï¼š`shared_hello.cu`

```c++
#include <stdio.h>

__global__ void copy_shared(float* device_out, const float* device_in, int N)
{
    // å£°æ˜ Block å†…çš„å…±äº«å†…å­˜(å›ºå®š 256 ä¸ªfloat)
    __shared__ float share_data[256];

    int global_tid = blockIdx.x * blockDim.x + threadIdx.x;
    int local_tid = threadIdx.x;

    if (global_tid < N)
    {
        // step 1:ä» global memory æ‹·è´åˆ° shared memory
        share_data[local_tid] = device_in[global_tid];

        // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆæ‹·è´
        __syncthreads();

        // step 2: ä½¿ç”¨shared memory çš„å€¼
        device_out[global_tid] = share_data[local_tid] * 2.0f;
    }
}

int main()
{
    const int N = 256;
    size_t bytes = N * sizeof(float);
    float host_in[N], host_out[N];
    for (int i = 0; i < N; i++)
    {
        host_in[i] = i;
    }

    float *device_in, *device_out;
    cudaMalloc(&device_in, bytes);
    cudaMalloc(&device_out, bytes);

    cudaMemcpy(device_in, host_in, bytes, cudaMemcpyHostToDevice);

    copy_shared<<<1, 256>>>(device_out, device_in, N);
    cudaMemcpy(host_out, device_out, bytes, cudaMemcpyDeviceToHost);

    for (int i = 0; i < N; i++)
    {
        printf("host_out[%d] = %f\n", i, host_out[i]);
    }

    cudaFree(device_in);
    cudaFree(device_out);

    return 0;
}

```

### è¿è¡Œ

```bash
nvcc -O2 shared_hello.cu -o shared_hello
./shared_hello
```

### ç»“æœ

![image-20250901000434277](./report_day9.assets/image-20250901000434277.png)

ğŸ‘‰ è¯æ˜æ•°æ®ç¡®å®åœ¨ shared memory ä¸­è¢«ä¿®æ”¹ã€‚

------

## 3ï¸âƒ£  æ·±åº¦è¿½é—®ï¼ˆæ€è€ƒé¢˜ï¼‰

### 1. shared memory bank å†²çªå…·ä½“æ˜¯æ€ä¹ˆå‘ç”Ÿçš„ï¼Ÿé¿å…ç­–ç•¥æœ‰å“ªäº›ï¼Ÿ

#### 1ï¸âƒ£ Shared Memory æ¶æ„

- **Shared Memory** åœ¨ç¡¬ä»¶ä¸Šè¢«åˆ’åˆ†ä¸º **32 ä¸ª bank**ï¼ˆå¯¹åº” warp çš„ 32 ä¸ªçº¿ç¨‹ï¼‰ã€‚
- æ¯ä¸ª bank å®½åº¦ = **4 bytes**ï¼ˆå³ 1 ä¸ª `float`ï¼‰ã€‚
- Warp å†…çš„ 32 ä¸ªçº¿ç¨‹å¦‚æœåœ¨ **åŒä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸ** å„è‡ªè®¿é—®çš„åœ°å€å±äº **ä¸åŒ bank** â†’ âœ… å¹¶è¡Œæ— å†²çªã€‚
- å¦‚æœå¤šä¸ªçº¿ç¨‹è®¿é—® **åŒä¸€ä¸ª bank çš„ä¸åŒåœ°å€** â†’ âŒ å†²çªï¼Œè®¿é—®ä¼š **ä¸²è¡ŒåŒ–**ï¼Œå»¶è¿Ÿæˆå€å¢åŠ ã€‚

ğŸ‘‰ ç±»æ¯”ï¼š32 ä¸ªæ”¶é“¶å°ï¼ˆbankï¼‰ï¼Œ32 ä¸ªäººï¼ˆçº¿ç¨‹ï¼‰åŒæ—¶æ’é˜Ÿï¼Œå¦‚æœåˆšå¥½ä¸€äººä¸€ä¸ªçª—å£ â†’ ç§’è¿‡ï¼›å¦‚æœå…¨æŒ¤åˆ°ä¸€ä¸ªçª—å£ â†’ ä¸²è¡Œå¤„ç†ã€‚

------

#### 2ï¸âƒ£ ä»€ä¹ˆæ—¶å€™å‘ç”Ÿå†²çªï¼Ÿ

å‡è®¾ `s_data[]` æ˜¯ shared memory æ•°ç»„ï¼š

- åœ°å€åˆ° bank çš„æ˜ å°„å…¬å¼å¤§è‡´æ˜¯ï¼š

  ```
  bank_id = (address / 4) % 32
  ```

- ä¸¾ä¾‹ï¼š

  - `s_data[tid]` (tid=0..31) â†’ æ¯ä¸ªçº¿ç¨‹è®¿é—®ä¸åŒ bank â†’ âœ… æ— å†²çª
  - `s_data[tid*2]` â†’ thread0â†’bank0, thread16â†’bank0 â†’ âŒ ä¸¤ä¸ªçº¿ç¨‹å†²çª
  - `s_data[tid*17]` â†’ stride=17ï¼Œæ‰€æœ‰è®¿é—®å‘¨æœŸæ€§è½åˆ°åŒä¸€ bank â†’ âŒ ä¸¥é‡å†²çª

âš ï¸ ç‰¹æ®Šæƒ…å†µï¼š**æ‰€æœ‰çº¿ç¨‹è®¿é—®åŒä¸€åœ°å€** â†’ ä¼šè¢«ç¡¬ä»¶ä¼˜åŒ–æˆ **å¹¿æ’­**ï¼Œä¸ç®—å†²çªã€‚

------

#### 3ï¸âƒ£ é¿å… bank å†²çªçš„ç­–ç•¥

1. **æŒ‰ warp é¡ºåºè®¿é—®ï¼ˆstride=1ï¼‰**

   - ä¿è¯ warp å†…çº¿ç¨‹è®¿é—®è¿ç»­åœ°å€ï¼š`s_data[threadIdx.x]`ã€‚

2. **ä½¿ç”¨ padding æ‰“æ•£æ˜ å°„**

   - åœ¨äºŒç»´ shared memory æ•°ç»„é‡Œï¼Œæ¯è¡Œå¤šåŠ  1 åˆ—ï¼š

     ```
     __shared__ float s_data[BLOCK_SIZE][BLOCK_SIZE+1];
     ```

   - é¿å… stride å¯¼è‡´å¤šä¸ªçº¿ç¨‹è½åœ¨åŒä¸€ bankã€‚

3. **ä¿è¯ warp å†…è®¿é—® 4B å¯¹é½**

   - å¦‚æœæ¯ä¸ªçº¿ç¨‹è®¿é—®çš„æ•°æ®ä¸æ˜¯ `float`ï¼Œè¦å¯¹é½åˆ° bank å®½åº¦ï¼ˆæ¯”å¦‚ `float2` è¦ 8B å¯¹é½ï¼‰ã€‚

4. **åˆ©ç”¨å¹¿æ’­ç‰¹æ€§**

   - å¦‚æœå¤šä¸ªçº¿ç¨‹ç¡®å®éœ€è¦ç›¸åŒæ•°æ®ï¼Œå¯ä»¥è®©å®ƒä»¬è®¿é—® **åŒä¸€ä¸ªåœ°å€**ï¼Œç¡¬ä»¶ä¼šè‡ªåŠ¨å¹¿æ’­ã€‚

5. **æ•°æ®å¸ƒå±€ä¼˜åŒ–**

   - å¦‚æœæ˜¯ 2D/3D æ•°æ®ï¼Œä¼˜å…ˆè®© `threadIdx.x` æ˜ å°„åˆ°è¿ç»­å…ƒç´ ï¼Œ`threadIdx.y/z` ç”¨ strideã€‚

### 2. constant memory è¯»å–çš„å¹¿æ’­æœºåˆ¶ä¸å¤±æ•ˆåœºæ™¯ï¼Ÿ

#### 1ï¸âƒ£ å¹¿æ’­æœºåˆ¶

- Constant Memory åœ¨æ¯ä¸ª SM ä¸Šæœ‰ **64KB çš„ä¸“ç”¨ cache**ã€‚
- å½“ **warp å†…çš„ 32 ä¸ªçº¿ç¨‹è®¿é—®ç›¸åŒçš„ constant åœ°å€** æ—¶ï¼š
  - **åªéœ€ 1 æ¬¡å†…å­˜å–æ•°**ï¼ˆcache line å‘½ä¸­ï¼‰ã€‚
  - ç¡¬ä»¶ä¼šè‡ªåŠ¨ **å¹¿æ’­ç»™æ•´ä¸ª warp**ã€‚
  - å»¶è¿Ÿ â‰ˆ è®¿é—®å¯„å­˜å™¨çš„é€Ÿåº¦ï¼Œéå¸¸å¿«ã€‚

ğŸ‘‰ åœºæ™¯ï¼š`out[i] = in[i] * d_coef[0];`

- æ‰€æœ‰çº¿ç¨‹éƒ½ç”¨ `d_coef[0]` â†’ ä¸€æ¬¡å–æ•°ï¼Œwarp å…¨éƒ¨å¾—åˆ°ç»“æœã€‚

------

#### 2ï¸âƒ£ å¤±æ•ˆåœºæ™¯ï¼ˆå¹¿æ’­ä¸æˆç«‹ï¼‰

å½“ **warp å†…çº¿ç¨‹è®¿é—®ä¸åŒ constant åœ°å€** æ—¶ï¼š

- æ¯ä¸ªä¸åŒåœ°å€éƒ½éœ€è¦å•ç‹¬çš„å†…å­˜è¯·æ±‚ã€‚
- è®¿é—®è¯·æ±‚ä¼šè¢« **ä¸²è¡ŒåŒ–**ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚
- æç«¯æƒ…å†µï¼š32 ä¸ªçº¿ç¨‹è®¿é—® 32 ä¸ªä¸åŒåœ°å€ â†’ é€€åŒ–ä¸º **32 æ¬¡ global memory è®¿é—®**ã€‚

ğŸ‘‰ åœºæ™¯ï¼š`out[i] = in[i] * d_coef[threadIdx.x];`

- æ¯ä¸ªçº¿ç¨‹è®¿é—®ä¸åŒçš„ `d_coef[]` â†’ æ— æ³•å¹¿æ’­ï¼Œæ€§èƒ½æ¥è¿‘ global memoryã€‚

------

#### 3ï¸âƒ£ ç‰¹æ®Šæƒ…å†µ

- **å¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ªåœ°å€**ï¼šâœ… å¹¿æ’­ï¼Œæœ€å¿«ã€‚
- **å¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ª cache line çš„ä¸åŒåœ°å€**ï¼šéƒ¨åˆ†å‘½ä¸­ï¼Œæ€§èƒ½ä»‹äºå¹¿æ’­ä¸å…¨ä¸²è¡Œä¹‹é—´ã€‚
- **è¶…å‡º 64KB constant cache å®¹é‡**ï¼šæ•°æ®ä¼šä» global memory å–ï¼Œæ€§èƒ½ä¸‹é™ã€‚

------

#### 4ï¸âƒ£ åº”ç”¨å»ºè®®

- Constant memory é€‚åˆå­˜æ”¾ **å°ä¸” warp å†…æ‰€æœ‰çº¿ç¨‹éƒ½è¦ç”¨çš„åªè¯»å‚æ•°**ï¼š
  - å·ç§¯æ ¸æƒé‡ï¼ˆå° kernelï¼‰
  - å½’ä¸€åŒ–ç³»æ•°
  - ç½‘ç»œå¸¸æ•°ï¼ˆå­¦ä¹ ç‡ã€æ¿€æ´»å‚æ•°ï¼‰
- ä¸é€‚åˆå­˜æ”¾ **å¤§æ•°ç»„æˆ–çº¿ç¨‹ç´¢å¼•è®¿é—®çš„æ•°æ®**ï¼ˆå› ä¸ºæ— æ³•åˆ©ç”¨å¹¿æ’­ï¼‰ã€‚

------

#### âœ… æ€»ç»“ï¼š

- **å¹¿æ’­æœºåˆ¶æˆç«‹**ï¼šwarp å†…è®¿é—®ç›¸åŒåœ°å€ â†’ è¶…é«˜æ•ˆã€‚
- **å¹¿æ’­å¤±æ•ˆ**ï¼šwarp å†…è®¿é—®ä¸åŒåœ°å€ â†’ ä¸¥é‡é€€åŒ–ã€‚

### 3. texture memory åœ¨é‡‡æ ·/æ’å€¼ä¸­çš„ä¼˜åŠ¿ï¼Œä½•æ—¶ä¼˜äº globalï¼Ÿ

#### 1ï¸âƒ£ èƒŒæ™¯

- CUDA æä¾›äº†ä¸€ç§ç‰¹æ®Šçš„å†…å­˜ç»‘å®šæ–¹å¼ï¼š**texture / surface memory**ã€‚
- æœ€åˆæ˜¯ä¸º **å›¾åƒå¤„ç† / å›¾å½¢æ¸²æŸ“** è®¾è®¡çš„ï¼Œä½†åœ¨ GPGPU åœºæ™¯é‡Œä¹Ÿèƒ½ç”¨ã€‚
- å…¶åº•å±‚åˆ©ç”¨äº† GPU çš„ **çº¹ç†ç¼“å­˜ (texture cache)**ï¼Œå¯¹ **2D/3D ç©ºé—´å±€éƒ¨æ€§** æœ‰ä¼˜åŒ–ã€‚

------

#### 2ï¸âƒ£ Texture Memory çš„ä¼˜åŠ¿

1. **ç©ºé—´å±€éƒ¨æ€§ç¼“å­˜ä¼˜åŒ–**
   - Texture cache ä¸“ä¸º **2D/3D ç©ºé—´è®¿é—®æ¨¡å¼** è®¾è®¡ã€‚
   - å¦‚æœç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»åƒç´ /ä½“ç´ ï¼Œcache å‘½ä¸­ç‡æ¯” global memory é«˜ã€‚
2. **æ”¯æŒç¡¬ä»¶æ’å€¼ (Interpolation)**
   - çº¹ç†å•å…ƒæ”¯æŒ **è‡ªåŠ¨åŒçº¿æ€§æ’å€¼ (bilinear interpolation)**ã€ä¸‰çº¿æ€§æ’å€¼ã€‚
   - è¿™å¯¹å›¾åƒç¼©æ”¾ã€æ»¤æ³¢ã€å·ç§¯æ“ä½œç‰¹åˆ«æœ‰ç”¨ï¼š
     - ä¸éœ€è¦è‡ªå·±å†™æ’å€¼é€»è¾‘ã€‚
     - æ’å€¼è®¡ç®—åœ¨ç¡¬ä»¶ä¸­å®Œæˆï¼Œé€Ÿåº¦å¿«ã€‚
3. **è¾¹ç•Œå¤„ç†ï¼ˆclamping / wrappingï¼‰**
   - Texture API å¯ä»¥ç›´æ¥æŒ‡å®šè¾¹ç•Œç­–ç•¥ï¼š
     - Clampï¼ˆå–è¾¹ç¼˜å€¼ï¼‰
     - Wrapï¼ˆå¾ªç¯å–å€¼ï¼‰
   - é¿å…è‡ªå·±å†™ if åˆ¤æ–­ï¼Œå‡å°‘åˆ†æ”¯å¼€é”€ã€‚
4. **åªè¯»æ•°æ®ä¼˜åŒ–**
   - çº¹ç†å†…å­˜æ˜¯ **åªè¯»çš„**ï¼ˆkernel å†…ä¸èƒ½å†™ï¼‰ï¼Œè¿™è®©ç¼“å­˜è®¾è®¡æ›´é«˜æ•ˆã€‚

------

#### 3ï¸âƒ£ ä»€ä¹ˆæ—¶å€™ä¼˜äº Global Memoryï¼Ÿ

1. **å›¾åƒ/ä½“æ•°æ®å¤„ç†**
   - æ¯”å¦‚ **å›¾åƒå·ç§¯ã€ç¼©æ”¾ã€æ—‹è½¬ã€é‡‡æ ·ã€ä½“æ¸²æŸ“**ã€‚
   - ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»åƒç´ æ—¶ â†’ texture cache æä¾›æ›´é«˜å¸¦å®½ã€‚
2. **éœ€è¦æ’å€¼é‡‡æ ·çš„åœºæ™¯**
   - ä¾‹å¦‚å…‰çº¿è¿½è¸ªä¸­çš„é‡‡æ ·ï¼Œæ·±åº¦å­¦ä¹ ä¸­çš„ä¸Šé‡‡æ ·ã€‚
   - ç”¨ global memory å¿…é¡»è‡ªå·±å†™æ’å€¼é€»è¾‘ï¼›
   - ç”¨ texture memory â†’ ç¡¬ä»¶ç›´æ¥åš bilinear/trilinear æ’å€¼ï¼Œæ›´å¿«æ›´çœä»£ç ã€‚
3. **è®¿é—®æ¨¡å¼ä¸è§„åˆ™ï¼Œä½†æœ‰å±€éƒ¨æ€§**
   - å¦‚æœçº¿ç¨‹çš„è®¿é—®æ¨¡å¼ä¸æ˜¯ä¸¥æ ¼é¡ºåºï¼ˆcoalescedï¼‰ï¼Œä½†æœ‰ç©ºé—´å±€éƒ¨æ€§ï¼Œtexture cache èƒ½å¸®å¿™ã€‚
   - è€Œ global memory åœ¨ä¸å¯¹é½æ—¶ä¼šæµªè´¹å¸¦å®½ã€‚

------

#### 4ï¸âƒ£ ä»€ä¹ˆæ—¶å€™ä¸ç”¨ Textureï¼Ÿ

- **çº¯é¡ºåºè®¿é—® (coalesced)**ï¼š
  - å¦‚æœ warp å†…çº¿ç¨‹è®¿é—®ä¸¥æ ¼è¿ç»­åœ°å€ï¼ˆæ¯”å¦‚å¤§è§„æ¨¡çŸ©é˜µä¹˜æ³•ï¼‰ï¼Œ**global memory å¸¦å®½åˆ©ç”¨ç‡æœ€é«˜**ã€‚
  - æ­¤æ—¶ç”¨ texture åè€Œæ²¡é¢å¤–ä¼˜åŠ¿ã€‚
- **éœ€è¦å†™æ“ä½œ**ï¼š
  - texture memory æ˜¯åªè¯»çš„ï¼Œå¦‚æœéœ€è¦å†™ï¼ˆæ¯”å¦‚çŸ©é˜µç»“æœå­˜å‚¨ï¼‰ï¼Œå¿…é¡»ç”¨ global æˆ– shared memoryã€‚

------

#### 5ï¸âƒ£ æ€»ç»“å£è¯€

**å›¾åƒä½“æ•°æ® â†’ Texture Memoryï¼Œç¡¬ä»¶æ’å€¼/è¾¹ç•Œå¤„ç†è¶…çœå¿ƒï¼›è§„åˆ™é¡ºåºè®¿é—® â†’ Global Memoryï¼Œå¸¦å®½åˆ©ç”¨ç‡æœ€é«˜ã€‚**

### 4. Unified Memory å¦‚ä½•è¿ç§»é¡µé¢ï¼Ÿè¿‡é‡ä½¿ç”¨ä¼šå¦‚ä½• thrashï¼Ÿ

#### ğŸ” Unified Memory çš„é¡µé¢è¿ç§»æœºåˆ¶

##### 1ï¸âƒ£ åŸºæœ¬æœºåˆ¶

- ä½¿ç”¨ `cudaMallocManaged` åˆ†é…çš„å†…å­˜ï¼ŒCPU å’Œ GPU éƒ½èƒ½è®¿é—®ã€‚
- æ•°æ®æŒ‰ **é¡µé¢ (page)** ç®¡ç†ï¼Œé€šå¸¸å¤§å°ä¸º **4KB**ï¼ˆä¹Ÿæœ‰ 64KB/2MB çš„å¤§é¡µï¼‰ã€‚
- GPU è®¿é—®æŸä¸ªé¡µé¢æ—¶ï¼š
  1. ç¡¬ä»¶æ£€æµ‹è¯¥é¡µé¢æ˜¯å¦åœ¨æ˜¾å­˜é‡Œã€‚
  2. å¦‚æœ **ä¸åœ¨æ˜¾å­˜** â†’ è§¦å‘ **Page Fault**ã€‚
  3. é©±åŠ¨ä¼šä» **ä¸»æœºå†…å­˜** æŠŠè¯¥é¡µé¢è¿ç§»åˆ° GPU æ˜¾å­˜ã€‚
  4. æ›´æ–°é¡µè¡¨ (page table)ï¼Œåç»­è®¿é—®å‘½ä¸­æ˜¾å­˜ã€‚

ğŸ‘‰ ç±»ä¼¼ CPU çš„è™šæ‹Ÿå†…å­˜åˆ†é¡µæœºåˆ¶ï¼Œåªä¸è¿‡è¿™é‡Œåœ¨ CPU â†” GPU ä¹‹é—´è¿ç§»ã€‚

------

##### 2ï¸âƒ£ è¿ç§»è§¦å‘åœºæ™¯

- **GPU è®¿é—®ä¸»æœºç«¯åˆšå†™çš„æ•°æ®** â†’ è¿ç§»åˆ°æ˜¾å­˜ã€‚
- **CPU è®¿é—® GPU åˆšå†™çš„æ•°æ®** â†’ è¿ç§»å›ä¸»æœºå†…å­˜ã€‚
- **å¤šä¸ª GPU**ï¼šå¯èƒ½éœ€è¦åœ¨ä¸åŒ GPU ä¹‹é—´æ¥å›æ‹·è´é¡µé¢ã€‚

------

##### 3ï¸âƒ£ æ€§èƒ½å¼€é”€

- ä¸€æ¬¡é¡µé¢è¿ç§» = **PCIe/NVLink æ‹·è´å»¶è¿Ÿ + é¡µè¡¨æ›´æ–°**ã€‚
- PCIe 4.0 å¸¦å®½ ~16 GB/sï¼Œä½†æ˜¾å­˜å¸¦å®½ ~800 GB/sï¼Œå·®è·çº¦ 50 å€ã€‚
- å¦‚æœé¢‘ç¹è¿ç§»ï¼Œä¼šä¸¥é‡æ‹–æ…¢æ€§èƒ½ã€‚

------

#### ğŸ” è¿‡é‡ä½¿ç”¨æ˜¾å­˜å¯¼è‡´ ThrashingÂ¢

##### 1ï¸âƒ£ ä»€ä¹ˆæ˜¯ thrashingï¼Ÿ

- **Thrashing = é¡µæŠ–åŠ¨**ã€‚
- å½“ UM åˆ†é…çš„å†…å­˜ **è¿œå¤§äº GPU æ˜¾å­˜**æ—¶ï¼š
  - GPU è®¿é—®ä¸€ä¸ªé¡µé¢ â†’ è¿ç§»è¿›æ¥ã€‚
  - è®¿é—®å¦ä¸€ä¸ªé¡µé¢ â†’ ä¸Šä¸€ä¸ªé¡µé¢å¯èƒ½è¢«é©±é€ã€‚
  - ä¸‹æ¬¡å†è®¿é—®ç¬¬ä¸€ä¸ªé¡µé¢ â†’ åˆè¦è¿ç§»å›æ¥ã€‚
- ç»“æœï¼šGPU ä¸€ç›´åœ¨ **è¿ç§»é¡µé¢ â†” é©±é€é¡µé¢**ï¼Œè€Œä¸æ˜¯åœ¨è®¡ç®—ã€‚

ğŸ‘‰ ç±»ä¼¼äº CPU å†…å­˜ä¸è¶³æ—¶çš„ swap é£æš´ã€‚

------

##### 2ï¸âƒ£ Thrashing çš„è¡¨ç°

- **æ€§èƒ½éª¤é™**ï¼škernel æ‰§è¡Œæ—¶é—´ä»æ¯«ç§’çº§ â†’ ç§’çº§ç”šè‡³åˆ†é’Ÿçº§ã€‚
- **nvidia-smi** çœ‹åˆ°æ˜¾å­˜å ç”¨æ³¢åŠ¨ï¼ˆè¿›å‡ºé¢‘ç¹ï¼‰ã€‚
- **Profiler (Nsight Systems)** é‡Œèƒ½çœ‹åˆ°å¤§é‡ â€œUnified Memory memcpyâ€ äº‹ä»¶ã€‚

------

##### 3ï¸âƒ£ å¦‚ä½•é¿å… Thrashingï¼Ÿ

1. **é¿å…è¶…æ˜¾å­˜ä½¿ç”¨**
   - åˆ†é… UM å†…å­˜æ—¶ä¸è¦è¶…è¿‡æ˜¾å­˜å®¹é‡çš„ 1.2~1.5 å€ã€‚
2. **åˆ†å—è®¡ç®— (chunking)**
   - æŠŠå¤§æ•°æ®åˆ†æˆå°å—ï¼Œé€å—è¿ç§»/è®¡ç®—ï¼Œé¿å…å…¨é‡æ”¾åœ¨ UMã€‚
3. **é¢„å– (Prefetch)**
   - ä½¿ç”¨ `cudaMemPrefetchAsync(ptr, size, device)` æŠŠæ•°æ®æå‰è¿ç§»åˆ° GPUï¼Œå‡å°‘ Page Faultã€‚
4. **å›ºå®šé©»ç•™ (cudaMemAdvise)**
   - å‘Šè¯‰é©±åŠ¨æŸäº›æ•°æ®ä¸»è¦ç”± GPU ä½¿ç”¨ (`cudaMemAdviseSetPreferredLocation`)ï¼Œå‡å°‘æ¥å›è¿ç§»ã€‚

------

#### âœ… æ€»ç»“

- **UM è¿ç§»æœºåˆ¶**ï¼šæŒ‰é¡µé¢ (4KB) åœ¨ CPU/GPU ä¹‹é—´è¿ç§»ï¼ŒPage Fault è§¦å‘ã€‚
- **è¿‡é‡ä½¿ç”¨æ˜¾å­˜**ï¼šä¼šå¯¼è‡´ thrashingï¼ˆé¡µæŠ–åŠ¨ï¼‰ï¼ŒGPU ä¸€ç›´åœ¨æ¬æ•°æ®ï¼Œæ€§èƒ½æš´è·Œã€‚
- **ä¼˜åŒ–æ‰‹æ®µ**ï¼šé¢„å– + åˆ†å— + å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–ã€‚

### 5. `cudaMemcpyAsync` ä¸ stream å…³è”çš„å‰æï¼Ÿ

#### 1ï¸âƒ£ å¿…é¡»ä½¿ç”¨ **é¡µé”å®šå†…å­˜ (Pinned Memory)**

- **Host å†…å­˜** å¿…é¡»é€šè¿‡ `cudaMallocHost()` æˆ– `cudaHostAlloc()` åˆ†é…ã€‚
- å¦‚æœç”¨æ™®é€šçš„ `malloc/new` åˆ†é…çš„ pageable memoryï¼š
  - CUDA åœ¨æ‹·è´æ—¶ä¼šè‡ªåŠ¨å…ˆæŠŠæ•°æ®æ‹·åˆ°ä¸€ä¸ª pinned bufferï¼Œå† DMA åˆ° GPUã€‚
  - è¿™ä¸ªè¿‡ç¨‹æ˜¯åŒæ­¥çš„ â†’ **å¼‚æ­¥å¤±æ•ˆ**ã€‚

âœ… æ­£ç¡®ï¼š

```
float *h_data;
cudaMallocHost(&h_data, size);   // pinned host memory
cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);
```

âŒ é”™è¯¯ï¼š

```
float *h_data = (float*)malloc(size); // pageable memory
cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream); // ä¼šé˜»å¡
```

------

#### 2ï¸âƒ£ å¿…é¡»æ˜¾å¼æŒ‡å®š **stream**

- `cudaMemcpyAsync(..., stream)` æœ€åä¸€ä¸ªå‚æ•°æ˜¯ **stream å¥æŸ„**ã€‚
- å¦‚æœä¸ä¼  â†’ é»˜è®¤ç”¨ `stream 0`ï¼Œä½†æ³¨æ„ï¼š
  - **é»˜è®¤æµ (legacy default stream)** ä¼šä¸æ‰€æœ‰å…¶ä»–æµ **åŒæ­¥**ã€‚
  - å¦‚æœæƒ³çœŸæ­£å¹¶è¡Œæ‹·è´+è®¡ç®—ï¼Œéœ€è¦ç”¨ **éé»˜è®¤æµ** (`cudaStreamCreate`)ã€‚

ğŸ‘‰ ç¤ºä¾‹ï¼š

```
cudaStream_t s1;
cudaStreamCreate(&s1);
cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, s1);
```

------

#### 3ï¸âƒ£ ç¡¬ä»¶å¿…é¡»æ”¯æŒ **å¹¶å‘æ‹·è´ä¸è®¡ç®—**

- GPU å¿…é¡»æœ‰ **ç‹¬ç«‹ copy engine**ï¼ˆé€šå¸¸ä» Kepler æ¶æ„å¼€å§‹éƒ½æœ‰ï¼‰ã€‚

- å¯ç”¨ `deviceQuery` æŸ¥çœ‹ï¼š

  ```
  Concurrent copy and kernel execution: Yes with 2 copy engines
  ```

- å¦‚æœåªæœ‰ 1 ä¸ª copy engineï¼Œåˆ™åªèƒ½åŒæ—¶åšä¸€ä¸ªæ–¹å‘çš„æ‹·è´ã€‚

------

#### 4ï¸âƒ£ API è¯­ä¹‰

- `cudaMemcpyAsync` åªæ˜¯æŠŠæ‹·è´ä»»åŠ¡ **æ’è¿›æŸä¸ª stream çš„é˜Ÿåˆ—**ï¼Œä¸ä¼šç«‹åˆ»é˜»å¡ CPUã€‚
- åªæœ‰å½“ï¼š
  - `cudaStreamSynchronize(stream)`
  - æˆ– `cudaEventSynchronize(event)`
  - æˆ– `cudaDeviceSynchronize()`
     è¿™äº›åŒæ­¥ API è¢«è°ƒç”¨æ—¶ï¼Œæ‰ä¼šç­‰å¾…æ‹·è´å®Œæˆã€‚

#### âœ… æ€»ç»“

`cudaMemcpyAsync` è¦æƒ³çœŸæ­£å¼‚æ­¥å¹¶å’Œ stream å…³è”ï¼Œå¿…é¡»æ»¡è¶³ï¼š

1. **Host å†…å­˜æ˜¯ pinned memory**ï¼ˆç”¨ `cudaMallocHost` åˆ†é…ï¼‰ã€‚
2. **ä½¿ç”¨éé»˜è®¤ stream**ï¼ˆç”¨ `cudaStreamCreate` åˆ›å»ºï¼‰ã€‚
3. **GPU æ”¯æŒå¹¶å‘æ‹·è´å’Œè®¡ç®—**ï¼ˆæœ‰ç‹¬ç«‹ copy engineï¼‰ã€‚

### 6. L2 ç¼“å­˜å‘½ä¸­ä¸ stride è®¿é—®å…³ç³»ï¼Ÿ

#### 1ï¸âƒ£ GPU L2 ç¼“å­˜ç‰¹ç‚¹

- L2 cache æ˜¯ **å…¨å±€å…±äº«**çš„ï¼ˆæ‰€æœ‰ SM è®¿é—®åŒä¸€ä¸ª L2ï¼‰ã€‚
- cache line é€šå¸¸æ˜¯ **128 å­—èŠ‚**ï¼ˆå³ 32 ä¸ª `float`ï¼‰ã€‚
- L2 å‘½ä¸­ç‡å–å†³äº **çº¿ç¨‹è®¿å­˜æ¨¡å¼æ˜¯å¦å…·æœ‰ç©ºé—´å±€éƒ¨æ€§**ã€‚

------

#### 2ï¸âƒ£ Stride è®¿é—®æ¨¡å¼

è®¾ warp å†…æœ‰ 32 ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹è®¿é—® `A[tid * stride]`ï¼š

- **stride = 1ï¼ˆè¿ç»­è®¿é—®ï¼‰**
  - çº¿ç¨‹ 0â†’A[0], çº¿ç¨‹ 1â†’A[1], ...
  - 32 ä¸ªçº¿ç¨‹è®¿é—®æ­£å¥½è½åœ¨ **ä¸€ä¸ª cache line (128B)** é‡Œã€‚
  - âœ… å®Œç¾ coalescingï¼ŒL2 å‘½ä¸­ç‡æœ€é«˜ï¼Œå¸¦å®½åˆ©ç”¨ç‡æœ€é«˜ã€‚
- **stride = 2**
  - çº¿ç¨‹ 0â†’A[0], çº¿ç¨‹ 1â†’A[2], çº¿ç¨‹ 2â†’A[4]...
  - warp å†…è®¿é—®è·¨åº¦å¤§ï¼Œå¯èƒ½éœ€è¦ **2 ä¸ª cache line**ã€‚
  - L2 å‘½ä¸­ç‡ä¸‹é™ä¸€åŠã€‚
- **stride = 4**
  - warp å†… 32 ä¸ªçº¿ç¨‹è®¿é—®é—´éš”æ›´å¤§ï¼Œå¯èƒ½éœ€è¦ **4 ä¸ª cache line**ã€‚
  - L2 å‘½ä¸­ç‡å†ä¸‹é™ã€‚
- **stride â‰¥ 32**
  - æ¯ä¸ªçº¿ç¨‹è®¿é—®çš„åœ°å€éƒ½è½åœ¨ä¸åŒçš„ cache lineã€‚
  - âŒ å®Œå…¨æ²¡æœ‰ç©ºé—´å±€éƒ¨æ€§ï¼ŒL2 å‘½ä¸­ç‡æ¥è¿‘ 0ã€‚
  - æ¯æ¬¡è®¿é—®éƒ½è¦èµ°æ˜¾å­˜ï¼Œå¸¦å®½åˆ©ç”¨ç‡æœ€ä½ã€‚

------

#### 3ï¸âƒ£ æ€»ç»“è§„å¾‹

- **å° strideï¼ˆâ‰¤1ï¼‰**ï¼šè®¿é—®é›†ä¸­åœ¨åŒä¸€ä¸ªæˆ–å°‘æ•° cache line â†’ L2 å‘½ä¸­ç‡é«˜ã€‚
- **å¤§ strideï¼ˆâ‰¥warp å¤§å°ï¼‰**ï¼šæ¯çº¿ç¨‹ç‹¬å ä¸€ä¸ª cache line â†’ L2 å‘½ä¸­ç‡å‡ ä¹ä¸º 0ã€‚
- **å‘½ä¸­ç‡ä¸ stride æˆåæ¯”**ï¼šstride è¶Šå¤§ï¼Œcache line çš„ç©ºé—´å±€éƒ¨æ€§è¶Šå·®ã€‚

------

#### 4ï¸âƒ£ é¿å… stride å¸¦æ¥çš„ L2 Miss

1. **è°ƒæ•´æ•°æ®å¸ƒå±€**
   - æ”¹å˜æ•°ç»„ç»´åº¦æ’åˆ—ï¼Œè®©çº¿ç¨‹è®¿é—®è¿ç»­å†…å­˜ã€‚
   - æ¯”å¦‚çŸ©é˜µè½¬ç½®æ—¶ï¼Œä½¿ç”¨ **shared memory tile** é‡æ’æ•°æ®ã€‚
2. **åˆ©ç”¨ shared memory ç¼“å­˜**
   - æŠŠ stride è®¿é—®çš„æ•°æ®å—å…ˆæ¬åˆ° shared memoryï¼Œå†æŒ‰è¡Œè®¿é—®ã€‚
3. **è½¯ä»¶ prefetch**
   - æå‰åŠ è½½æœªæ¥éœ€è¦çš„æ•°æ®ï¼Œå‡å°‘ L2 miss å¼€é”€ã€‚

------

#### âœ… æ€»ç»“

**GPU çš„ L2 ç¼“å­˜å‘½ä¸­ç‡é«˜åº¦ä¾èµ– warp å†…çº¿ç¨‹çš„è®¿é—®æ¨¡å¼ã€‚ è¿ç»­è®¿é—®ï¼ˆstride=1ï¼‰å‘½ä¸­ç‡æœ€é«˜ï¼›stride è¶Šå¤§ï¼Œå‘½ä¸­ç‡è¶Šä½ï¼Œæœ€ç»ˆé€€åŒ–æˆå…¨å±€æ˜¾å­˜è®¿é—®ã€‚**

## 4ï¸âƒ£ å®éªŒ

### ğŸ§ª å®éªŒ 1ï¼š`matrix_add` â€”â€” Global vs Shared

#### 1ï¸âƒ£ å®éªŒç›®æ ‡

- å¯¹æ¯” **ç›´æ¥ä½¿ç”¨ global memory** vs **tile è¿› shared memory å†è®¡ç®—** çš„æ€§èƒ½å·®å¼‚ã€‚
- ç”¨ CUDA **äº‹ä»¶ API** æµ‹é‡è€—æ—¶ï¼Œå¹¶æ ¹æ®å…¬å¼ä¼°ç®— **å†…å­˜å¸¦å®½åˆ©ç”¨ç‡**ã€‚

------

#### 2ï¸âƒ£ å‡†å¤‡ä»£ç 

åœ¨å­¦ä¹ ç›®å½•ä¸‹æ–°å»º `matrix_add_shared.cu`ï¼ŒæŠŠä¸‹é¢çš„ä»£ç ç²˜è´´è¿›å»ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

#define SIZE 1024 // çŸ©é˜µå¤§å° N * N
// Global memoryç‰ˆæœ¬
__global__ void mat_add_global(const float* A, const float* B, float* C, int N)
{
    int r = blockIdx.y * blockDim.y + threadIdx.y;
    int c = blockIdx.x * blockDim.x + threadIdx.x;

    if (r < N && c < N)
    {
        C[r * N + c] = A[r * N + c] + B[r * N + c];
    }
}

// Shared memoryç‰ˆæœ¬
__global__ void mat_add_shared(const float* A, const float* B, float* C, int N)
{
    __shared__ float A_shared[32][32];
    __shared__ float B_shared[32][32];

    int r = blockIdx.y * blockDim.y + threadIdx.y;
    int c = blockIdx.x * blockDim.x + threadIdx.x;
    int ty = threadIdx.y, tx = threadIdx.x;

    if (r < N && c < N)
    {
        // æŠŠæ•°æ®æ¬åˆ°shared memory
        A_shared[ty][tx] = A[r * N + c];
        B_shared[ty][tx] = B[r * N + c];

        // ç¡®ä¿æ‰€æœ‰çº¿ç¨‹å†™å®Œ
        __syncthreads();

        // ä» shared memory è¯»å‡ºå†è®¡ç®—
        C[r * N + c] = A_shared[ty][tx] + B_shared[ty][tx];
    }
}

int main()
{
    size_t bytes = SIZE * SIZE * sizeof(float);

    // åˆ†é… host å†…å­˜
    float* host_a = (float*)malloc(bytes);
    float* host_b = (float*)malloc(bytes);
    float* host_c = (float*)malloc(bytes);

    for (int i = 0; i < SIZE * SIZE; i++)
    {
        host_a[i] = 1.0f;
        host_b[i] = 2.0f;
    }

    // åˆ†é… device å†…å­˜
    float *device_a, *device_b, *device_c;
    cudaMalloc(&device_a, bytes);
    cudaMalloc(&device_b, bytes);
    cudaMalloc(&device_c, bytes);

    cudaMemcpy(device_a, host_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes, cudaMemcpyHostToDevice);

    // æ¯ä¸ª Block 32 * 32 ä¸ªçº¿ç¨‹
    dim3 block(32, 32);
    dim3 grid((SIZE + 31) / 32, (SIZE + 31) / 32);

    // cuda äº‹ä»¶ç”¨äºè®¡æ—¶
    cudaEvent_t start, stop;
    float ms;

    // Global memoryç‰ˆæœ¬
    cudaMemset(device_c, 0, bytes);
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    mat_add_global<<<grid, block>>>(device_a, device_b, device_c, SIZE);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&ms, start, stop);
    printf("mat_add_global: %.3f ms, å¸¦å®½â‰ˆ %.2f GB/s\n", ms,
           (3 * SIZE * SIZE * sizeof(float) / 1e9) / (ms / 1000));

    // Shared memoryç‰ˆæœ¬
    cudaMemset(device_c, 0, bytes);
    cudaEventRecord(start);
    mat_add_shared<<<grid, block>>>(device_a, device_b, device_c, SIZE);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);

    printf("mat_add_shared: %.3f ms, å¸¦å®½â‰ˆ %.2f GB/s\n", ms,
           (3 * SIZE * SIZE * sizeof(float) / 1e9) / (ms / 1000));

    // æ¸…ç†
    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);

    free(host_a);
    free(host_b);
    free(host_c);

    return 0;
}

```

------

#### 3ï¸âƒ£ ç¼–è¯‘è¿è¡Œ

```bash
nvcc -O2 matrix_add_shared.cu -o mat_add
./mat_add
```

------

#### 4ï¸âƒ£ é¢„æœŸç»“æœ

![image-20250901010339343](./report_day9.assets/image-20250901010339343.png)

- **Global ç‰ˆ**ï¼šæ¯ä¸ªå…ƒç´ ç›¸åŠ éƒ½è¦è®¿é—®ä¸¤æ¬¡ global memoryã€‚
- **Shared ç‰ˆ**ï¼šblock å†…æŠŠæ•°æ®æ¬åˆ° shared memoryï¼Œå†å±€éƒ¨è®¡ç®—ï¼Œå‡å°‘å…¨å±€è®¿å­˜ã€‚
- ç»“æœï¼š**shared memory ç‰ˆæ›´å¿«ï¼Œå¸¦å®½åˆ©ç”¨ç‡æ›´é«˜**ã€‚

------

### ğŸ§ª å®éªŒ 2ï¼šStride=17 è®¿é—® Bank Conflict & Padding æ¶ˆé™¤

#### 1ï¸âƒ£ å®éªŒç›®æ ‡

- ç†è§£ CUDA **Shared Memory çš„ bank æ¶æ„**ã€‚
- åˆ¶é€  **Bank Conflict**ï¼ˆå†²çªï¼‰ï¼Œç„¶åé€šè¿‡ **Padding** æ¶ˆé™¤ã€‚
- ç”¨ **Nsight Compute** è§‚å¯Ÿå†²çªå¯¹æ€§èƒ½çš„å½±å“ã€‚

------

#### 2ï¸âƒ£ Bank èƒŒæ™¯çŸ¥è¯†

- **Shared Memory** è¢«åˆ†æˆ **32 ä¸ª bank**ï¼Œæ¯ä¸ª bank ä¸€æ¬¡èƒ½æœåŠ¡ 1 ä¸ªçº¿ç¨‹ã€‚
- **Warp = 32 ä¸ªçº¿ç¨‹**ï¼Œç†æƒ³æƒ…å†µï¼šwarp å†…æ¯ä¸ªçº¿ç¨‹è®¿é—®ä¸åŒ bank â†’ **å¹¶è¡Œæ— å†²çª**ã€‚
- å¦‚æœå¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ª bankï¼Œå°±ä¼šäº§ç”Ÿ **å†²çª**ï¼Œè®¿é—®ä¼šè¢« **ä¸²è¡ŒåŒ–**ï¼Œå»¶è¿Ÿå¤§å¤§å¢åŠ ã€‚
- ä¸¾ä¾‹ï¼š
  - stride=1ï¼šçº¿ç¨‹ 0â†’bank0, çº¿ç¨‹ 1â†’bank1 â€¦ â†’ âœ… æ— å†²çª
  - stride=17ï¼šçº¿ç¨‹ 0â†’bank0, çº¿ç¨‹ 1â†’bank17, çº¿ç¨‹ 2â†’bank2 â€¦ çº¿ç¨‹ 16â†’bank16, çº¿ç¨‹ 17â†’bank1 â†’ âŒ å†²çªå‘ç”Ÿ

------

#### 3ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `bank_conflict_stride.cu`ï¼š

```c++
#include <stdio.h>
#include <cuda_runtime.h>

// å†²çªç‰ˆæœ¬ï¼šstride=17
__global__ void conflict(float *out) {
    __shared__ float s_data[32*17]; // stride=17
    int tid = threadIdx.x;
    s_data[tid*17] = tid;           // å¤šä¸ªçº¿ç¨‹æ˜ å°„åˆ°åŒä¸€ä¸ª bank
    __syncthreads();
    out[tid] = s_data[tid*17];
}

// æ— å†²çªç‰ˆæœ¬ï¼šstride=17 + padding
__global__ void no_conflict(float *out) {
    __shared__ float s_data[32*17+1]; // padding +1
    int tid = threadIdx.x;
    s_data[tid*17] = tid;             // padding æ‰“æ•£ bank æ˜ å°„
    __syncthreads();
    out[tid] = s_data[tid*17];
}

// è®¡æ—¶å°è£…å‡½æ•°
float run_and_time(void (*kernel)(float*), float *d_out, int N) {
    cudaEvent_t start, stop;
    cudaEventCreate(&start); cudaEventCreate(&stop);

    cudaEventRecord(start);
    kernel<<<1, N>>>(d_out);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms = 0;
    cudaEventElapsedTime(&ms, start, stop);
    cudaEventDestroy(start); cudaEventDestroy(stop);

    return ms;
}

int main() {
    const int N = 32; // warp å†… 32 çº¿ç¨‹
    size_t bytes = N * sizeof(float);

    float h_out[N];
    float *d_out;
    cudaMalloc(&d_out, bytes);

    // è®¡æ—¶å¹¶è¿è¡Œå†²çªç‰ˆæœ¬
    float t1 = run_and_time(conflict, d_out, N);
    cudaMemcpy(h_out, d_out, bytes, cudaMemcpyDeviceToHost);
    printf("Conflict kernel (%.6f ms):\n", t1);
    for (int i=0;i<5;i++) printf("out[%d]=%.1f ", i, h_out[i]);
    printf("\n");

    // è®¡æ—¶å¹¶è¿è¡Œæ— å†²çªç‰ˆæœ¬
    float t2 = run_and_time(no_conflict, d_out, N);
    cudaMemcpy(h_out, d_out, bytes, cudaMemcpyDeviceToHost);
    printf("No Conflict kernel (%.6f ms):\n", t2);
    for (int i=0;i<5;i++) printf("out[%d]=%.1f ", i, h_out[i]);
    printf("\n");

    cudaFree(d_out);
    return 0;
}

```

------

#### 4ï¸âƒ£ ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 bank_conflict_stride.cu -o bank_conflict
./bank_conflict
```

é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼š

![image-20250901173945736](./report_day9.assets/image-20250901173945736.png)

------

#### 5ï¸âƒ£ ç»“æœåˆ†æ

- ç»“æœä¸€æ ·ï¼Œè¯´æ˜ bank conflict ä¸å½±å“æ­£ç¡®æ€§ã€‚
- **æœ‰å†²çªç‰ˆæœ¬** æ—¶é—´æ˜æ˜¾æ›´é•¿ï¼ˆå†²çªå¯¼è‡´ä¸²è¡ŒåŒ–ï¼‰ã€‚
- **æ— å†²çªç‰ˆæœ¬** æ—¶é—´æ›´çŸ­ï¼ˆpadding æ¶ˆé™¤äº†å†²çªï¼‰ã€‚

------

### ğŸ§ª å®éªŒ 3ï¼šConstant Memory ä¼˜åŠ¿

#### 1ï¸âƒ£ èƒŒæ™¯çŸ¥è¯†

- **Constant Memory**
  - æ¯ä¸ª SM æœ‰ **64KB å¸¸é‡ç¼“å­˜**ï¼Œä¸»è¦ä¼˜åŒ– **warp å†…æ‰€æœ‰çº¿ç¨‹è®¿é—®ç›¸åŒåœ°å€** çš„æƒ…å†µã€‚
  - å¦‚æœä¸€ä¸ª warp çš„ 32 ä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ªå¸¸é‡åœ°å€ â†’ åªéœ€ **1 æ¬¡å–æ•° + å¹¿æ’­**ï¼Œæ•ˆç‡æé«˜ã€‚
  - å¦‚æœ warp å†…çš„çº¿ç¨‹è®¿é—®ä¸åŒåœ°å€ â†’ ä¼šå‘ç”Ÿ **åºåˆ—åŒ–**ï¼Œæ€§èƒ½å¯èƒ½æ¯” global memory è¿˜å·®ã€‚
- **åº”ç”¨åœºæ™¯**
  - CNN å·ç§¯æ ¸æƒé‡ï¼ˆå…¨çº¿ç¨‹ç”¨ç›¸åŒå‚æ•°ï¼‰ã€‚
  - å½’ä¸€åŒ–ç³»æ•°ã€è¶…å‚æ•°ï¼ˆæ¯”å¦‚å­¦ä¹ ç‡ã€ç¼©æ”¾å› å­ï¼‰ã€‚
  - ä¸é€‚åˆï¼šçº¿ç¨‹å„è‡ªè¯»å–ä¸åŒå¸¸é‡çš„æƒ…å†µã€‚

------

#### 2ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `const_memory.cu`ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

// å¸¸é‡å†…å­˜
#define COEF_SIZE 1024
__constant__ float device_coef[COEF_SIZE]; // GPUå¸¸é‡å†…å­˜

__global__ void kernel_const(const float* in, float* out, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        float val = in[i];
        for (int j = 0; j < 1000; j++)
        {
            val *= device_coef[j % COEF_SIZE];
        }
        out[i] = val;
    }
}

__global__ void kernel_global(const float* in, float* out, const float* coef, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        float val = in[i];
        for (int j = 0; j < 1000; j++)
        {
            val *= coef[j % COEF_SIZE];
        }
        out[i] = val;
    }
}

// float run_and_time(void (*kernel)(float))

int main()
{
    const int N = 1 << 24; // 16M å…ƒç´ 
    size_t bytes = N * sizeof(float);

    float* host_in = (float*)malloc(bytes);
    float* host_out = (float*)malloc(bytes);
    for (int i = 0; i < N; i++)
    {
        host_in[i] = 1.0f;
    }

    float *device_in, *device_out, *device_coef_global;
    cudaMalloc(&device_in, bytes);
    cudaMalloc(&device_out, bytes);
    cudaMalloc(&device_coef_global, COEF_SIZE * sizeof(float));

    cudaMemcpy(device_in, host_in, bytes, cudaMemcpyHostToDevice);

    float host_coef[COEF_SIZE];
    for (int i = 0; i < COEF_SIZE; i++)
    {
        host_coef[i] = 1.0f;
    }
    // æŠŠ coef æ”¾åˆ° constant memory
    cudaMemcpyToSymbol(device_coef, host_coef, COEF_SIZE * sizeof(float));
    // æŠŠ coef æ”¾åˆ° global memory
    cudaMemcpy(device_coef_global, host_coef, COEF_SIZE * sizeof(float), cudaMemcpyHostToDevice);

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    float ms;

    // constant memory
    cudaEventRecord(start);
    kernel_const<<<grid, block>>>(device_in, device_out, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);
    printf("Const memory kernel: %.3f ms\n", ms);

    // global memory
    cudaEventRecord(start);
    kernel_global<<<grid, block>>>(device_in, device_out, device_coef_global, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);
    printf("Global memory kernel: %.3f ms\n", ms);

    cudaFree(device_in);
    cudaFree(device_out);
    cudaFree(device_coef_global);

    free(host_in);
    free(host_out);

    return 0;
}

```

------

#### 3ï¸âƒ£ ç¼–è¯‘è¿è¡Œ

```bash
nvcc -O2 const_memory.cu -o const_mem
./const_mem
```

------

#### 4ï¸âƒ£ é¢„æœŸè¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰

![image-20250902172528149](./report_day9.assets/image-20250902172528149.png)

- **å¸¸é‡å†…å­˜ç‰ˆæœ¬åœ¨å¹¿æ’­è®¿é—®æ¨¡å¼ä¸‹æ›´å¿«**ï¼Œå› ä¸º warp å†…æ‰€æœ‰çº¿ç¨‹è®¿é—®åŒä¸€ä¸ªå¸¸é‡åœ°å€æ—¶ï¼Œåªéœ€ä¸€æ¬¡å–æ•°å³å¯å¹¿æ’­ç»™ 32 ä¸ªçº¿ç¨‹ã€‚
- **å…¨å±€å†…å­˜ç‰ˆæœ¬åœ¨è¿™ç§åœºæ™¯ä¸‹æ›´æ…¢**ï¼Œå³ä¾¿æœ‰ L1/L2 cacheï¼Œwarp å†…ä»è¦å¤šæ¬¡è¯·æ±‚ç›¸åŒåœ°å€ï¼Œå¼€é”€æ›´å¤§ã€‚

âš ï¸ æ³¨æ„è¡¥å……è¯´æ˜ï¼š

- å¦‚æœ warp å†…çº¿ç¨‹è®¿é—®çš„æ˜¯ **ä¸åŒåœ°å€**ï¼Œé‚£ä¹ˆå¸¸é‡å†…å­˜ä¼šå‘ç”Ÿ **åºåˆ—åŒ–**ï¼Œæ€§èƒ½å¯èƒ½ä¸å…¨å±€å†…å­˜ç›¸å½“ç”šè‡³æ›´å·®ã€‚

------

### ğŸ§ª å®éªŒ 4ï¼šUnified Memory è¶…è¿‡æ˜¾å­˜å®¹é‡

#### 1ï¸âƒ£ èƒŒæ™¯

- **Unified Memory (UM)**ï¼šç”¨ `cudaMallocManaged` åˆ†é…çš„å†…å­˜ï¼Œå¯ä»¥åœ¨ CPU å’Œ GPU ä¹‹é—´è‡ªåŠ¨è¿ç§»ã€‚
- å½“æ•°æ®é‡ **è¶…è¿‡æ˜¾å­˜å®¹é‡** æ—¶ï¼ŒGPU åœ¨è®¿é—®æ•°æ®æ—¶ä¼šè§¦å‘ **page migrationï¼ˆé¡µè¿ç§»ï¼‰**ï¼š
  - æŠŠæ•°æ®ä»ç³»ç»Ÿå†…å­˜æ¬åˆ°æ˜¾å­˜ã€‚
  - å¦‚æœæ˜¾å­˜ä¸å¤Ÿ â†’ ä¼šä¸æ–­æ¢å…¥/æ¢å‡ºï¼Œååé‡éª¤é™ã€‚

------

#### 2ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `unified_mem.cu`ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void touch(float* data, long N)
{
    long i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        data[i] += 1.0f;
    }
}

int main(int argc, char* argv[])
{
    // N å¤ªå¤§å¯èƒ½å¯¼è‡´ç³»ç»Ÿç›´æ¥OOM
    long N = (long)1e9; // é»˜è®¤ 1e9 (~4 GB)
    if (argc > 1)
    {
        N = atol(argv[1]); // å¯ä»¥ä»å‘½ä»¤è¡Œä¼  N
    }
    size_t bytes = N * sizeof(float);

    printf("Allocating %.2f GB Unified Memory...\n", bytes / 1e9);

    float* data;
    cudaMallocManaged(&data, bytes); // unified memory

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    touch<<<grid, block>>>(data, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms;
    cudaEventElapsedTime(&ms, start, stop);

    printf("Kernel with UM (N=%ld, %.2f GB): %.3f ms\n", N, bytes / 1e9, ms);

    cudaFree(data);

    return 0;
}
```

------

#### 3ï¸âƒ£ ç¼–è¯‘è¿è¡Œ

```bash
nvcc -O2 unified_mem.cu -o unified_mem
./unified_mem 1000000000
```

------

#### 4ï¸âƒ£ ç»“æœåˆ†æ

![image-20250902181010601](./report_day9.assets/image-20250902181010601.png)

- å¦‚æœ `N=1e9 (~4 GB)`ï¼Œåœ¨ 3080 (10GB æ˜¾å­˜) ä¸Šè¿è¡Œæ­£å¸¸ï¼Œæ€§èƒ½æ¥è¿‘ global memoryã€‚
- å¦‚æœ `N=2e9 (~8 GB)`ï¼Œä¾ç„¶èƒ½æ”¾è¿›æ˜¾å­˜ï¼ˆ10GBï¼‰ï¼Œæ€§èƒ½ç•¥ä¸‹é™ã€‚
- å¦‚æœä½ æ”¹æˆ `N=5e9 (~20 GB)`ï¼Œ**è¶…è¿‡æ˜¾å­˜å®¹é‡**ï¼Œå°±ä¼šè§¦å‘ **page migration**ï¼š
  - ç¨‹åºè¿˜èƒ½è¿è¡Œï¼Œä½†æ—¶é—´ä¼šæ˜æ˜¾å˜é•¿ï¼ˆå‡ åå€ï¼‰ã€‚
  - ç”¨ `ncu` æˆ– `nsys` profileï¼Œå¯ä»¥çœ‹åˆ°å¤§é‡ **UM page migration** äº‹ä»¶ã€‚

------

#### 5ï¸âƒ£ è¿›ä¸€æ­¥å®éªŒ

1. ### (a) ç”¨ Nsight Compute

   è¿è¡Œï¼š

   ```bash
   ncu --set full ./unified_mem 3000000000
   ```

   åœ¨æŠ¥å‘Šé‡Œçœ‹ `Unified Memory Memcpy`ï¼Œä¼šçœ‹åˆ°å¤§é‡è¿ç§»äº‹ä»¶ã€‚

   ### (b) ç”¨ nvidia-smi åŠ¨æ€è§‚å¯Ÿ

   å¦å¼€ä¸€ä¸ªç»ˆç«¯è¿è¡Œï¼š

   ```
   watch -n 0.5 nvidia-smi
   ```

   å¦‚æœä½ è·‘ `./unified_mem 3000000000`ï¼Œæ˜¾å­˜å ç”¨ä¼š **ä¸Šä¸‹æ³¢åŠ¨**ï¼ˆé¡µè¿ç§»è¿›è¿›å‡ºå‡ºï¼‰ã€‚

------

âš ï¸ æ³¨æ„äº‹é¡¹ï¼š

- ä¸€æ¬¡æ€§åˆ†é…è¶…è¿‡ 20â€“30GBï¼ˆè¶…ç³»ç»Ÿå†…å­˜ï¼‰å¯èƒ½ç›´æ¥æŠ¥ `cudaErrorMemoryAllocation`ã€‚
- å»ºè®® **å…ˆè¯• 4GB / 8GB / 12GB**ï¼Œé€æ­¥å¢å¤§ã€‚

------

### ğŸ§ª å®éªŒ 5ï¼š`cudaMemcpyAsync` + Stream é‡å æ‹·è´/è®¡ç®—

#### 1ï¸âƒ£ èƒŒæ™¯

- `cudaMemcpy` é»˜è®¤æ˜¯ **åŒæ­¥çš„**ï¼šCPU ä¼šç­‰æ•°æ®æ‹·è´å®Œæˆåå†ç»§ç»­æ‰§è¡Œï¼ŒGPU ä¹Ÿä¸èƒ½åŒæ—¶è®¡ç®—ã€‚
- `cudaMemcpyAsync` + **pinned memoryï¼ˆé¡µé”å®šå†…å­˜ï¼‰** + **stream** å¯ä»¥è®©ï¼š
  - æ•°æ®æ‹·è´å’Œè®¡ç®—å¹¶è¡Œè¿›è¡Œã€‚
  - æå‡æ•´ä½“ååã€‚
- âš ï¸ å…³é”®æ¡ä»¶ï¼šå¿…é¡»ç”¨ **cudaMallocHost** åˆ†é… host å†…å­˜ï¼Œå¦åˆ™æ‹·è´æ— æ³•çœŸæ­£å¼‚æ­¥ã€‚

------

#### 2ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `async_copy.cu`ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

// ç®€å•çš„è®¡ç®—kernel (æ¨¡æ‹Ÿè€—æ—¶è®¡ç®—)
__global__ void computer(float* data, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        float x = data[i];
        for (int j = 0; j < 10000; j++)
        {
            x = x * 0.999f + 0.001f;
        }
        data[i] = x;
    }
}

int main()
{
    const int N = 1 << 24;
    size_t bytes = N * sizeof(float);

    float *host_data, *device_data;
    cudaMallocHost(&host_data, bytes); // é¡µé”å®šå†…å­˜(å¿…é¡»)
    cudaMalloc(&device_data, bytes);

    for (int i = 0; i < N; i++)
    {
        host_data[i] = 1.0f;
    }

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    // åŒæ­¥ç‰ˆæœ¬
    cudaEvent_t start, stop;
    float ms_sync, ms_async;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    // åŒæ­¥æ‹·è´ H2D
    cudaMemcpy(device_data, host_data, bytes, cudaMemcpyHostToDevice);
    // è®¡ç®—
    computer<<<grid, block>>>(device_data, N);
    // åŒæ­¥æ‹·è´ D2H
    cudaMemcpy(host_data, device_data, bytes, cudaMemcpyDeviceToHost);

    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&ms_sync, start, stop);

    // å¼‚æ­¥ç‰ˆæœ¬
    cudaStream_t s1, s2;
    cudaStreamCreate(&s1);
    cudaStreamCreate(&s2);

    cudaEventRecord(start);
    // H2D å¼‚æ­¥æ‹·è´
    cudaMemcpyAsync(device_data, host_data, bytes, cudaMemcpyHostToDevice, s1);
    // è®¡ç®—æ”¾åˆ°å¦ä¸€ä¸ªstream
    computer<<<grid, block, 0, s2>>>(device_data, N);
    // D2H å¼‚æ­¥æ‹·è´
    cudaMemcpyAsync(host_data, device_data, bytes, cudaMemcpyDeviceToHost, s1);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms_async, start, stop);

    printf("Sync  version time: %.3f ms\n", ms_sync);
    printf("Async version time: %.3f ms\n", ms_async);

    cudaStreamDestroy(s1);
    cudaStreamDestroy(s2);
    cudaFree(device_data);
    cudaFreeHost(host_data);

    return 0;
}

```

------

#### 3ï¸âƒ£ ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 async_copy.cu -o async_copy
./async_copy
```

------

#### 4ï¸âƒ£ é¢„æœŸç»“æœ

![image-20250903175422021](./report_day9.assets/image-20250903175422021.png)

- **åŒæ­¥ç‰ˆ**ï¼šæ‹·è´(H2D) â†’ è®¡ç®— â†’ æ‹·è´(D2H)ï¼Œå®Œå…¨ä¸²è¡Œã€‚
- **å¼‚æ­¥ç‰ˆ**ï¼šæ‹·è´å’Œè®¡ç®— **éƒ¨åˆ†é‡å **ï¼Œæ€»æ—¶é—´æ›´çŸ­ã€‚

#### 5ï¸âƒ£ æ·±åº¦è¿½é—®

1. ä¸ºä»€ä¹ˆéœ€è¦ `cudaMallocHost`ï¼ˆpinned memoryï¼‰æ‰èƒ½çœŸæ­£å¼‚æ­¥ï¼Ÿ
   - å› ä¸ºåªæœ‰ pinned å†…å­˜æ‰èƒ½è¢« DMA å¼•æ“ç›´æ¥è®¿é—®ï¼Œé pinned å†…å­˜ä¼šéšå¼è½¬æˆåŒæ­¥æ‹·è´ã€‚
2. ä¸ºä»€ä¹ˆç”¨äº†ä¸¤ä¸ª streamï¼Ÿ
   - é¿å…æ‹·è´å’Œè®¡ç®—åœ¨åŒä¸€ä¸ª stream ä¸²è¡ŒåŒ–ã€‚
3. å¦‚ä½•éªŒè¯æ‹·è´å’Œè®¡ç®—æ˜¯å¦çœŸçš„é‡å ï¼Ÿ
   - ç”¨ `nsys profile ./async_copy` æˆ– Nsight Systems æŸ¥çœ‹æ—¶é—´çº¿ï¼Œå¯ä»¥çœ‹åˆ° **memcpy å’Œ kernel é‡å æ‰§è¡Œ**ã€‚

------

## âœ… æ€»ç»“

- **Global memory**ï¼šå¤§å¸¦å®½ï¼Œä½†å¿…é¡» coalescedã€‚
- **Shared memory**ï¼šå»¶è¿Ÿä½ï¼Œä½†è¦é¿å… bank conflictï¼ˆå¯ç”¨ paddingï¼‰ã€‚
- **Constant memory**ï¼šwarp å¹¿æ’­æå¿«ï¼Œè®¿é—®ä¸åŒåœ°å€ä¼šé€€åŒ–ã€‚
- **Texture memory**ï¼šé€‚åˆç©ºé—´å±€éƒ¨æ€§å¼ºçš„éšæœºè®¿é—® + æ’å€¼åœºæ™¯ã€‚
- **L2 cache**ï¼šå— stride è®¿é—®æ¨¡å¼å½±å“æ˜¾è‘—ã€‚