# Day 9 - CUDA å†…å­˜ç®¡ç†

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡ **CUDA å†…å­˜å±‚æ¬¡**ï¼šå¯„å­˜å™¨ã€å…±äº«å†…å­˜ (shared)ã€å…¨å±€å†…å­˜ (global)ã€‚
- å­¦ä¼š **å£°æ˜ã€ä½¿ç”¨ã€åŒæ­¥ shared memory**ã€‚
- åœ¨ **å½’çº¦ (Reduction)** åœºæ™¯ä¸­ä½“ä¼š shared memory çš„æ€§èƒ½ä¼˜åŠ¿ã€‚
- é€šè¿‡å®éªŒéªŒè¯ **block å¤§å°ã€bank conflictã€atomicAdd çš„ä½œç”¨**ã€‚

------

## 1ï¸âƒ£ CUDA å†…å­˜å±‚æ¬¡å¤ä¹ 

| ç±»å‹       | ä½œç”¨èŒƒå›´     | ç‰¹ç‚¹                         | å»¶è¿Ÿ           | ç”¨é€”                       |
| ---------- | ------------ | ---------------------------- | -------------- | -------------------------- |
| å¯„å­˜å™¨     | æ¯ä¸ªçº¿ç¨‹ç§æœ‰ | æœ€å¿«ï¼Œæ•°é‡æœ‰é™               | ~1 cycle       | ä¿å­˜ä¸´æ—¶å˜é‡               |
| Shared Mem | æ¯ä¸ª Block   | Block å†…å…±äº«ï¼Œéœ€æ‰‹åŠ¨ç®¡ç†     | ~10 cycles     | Block å†…çº¿ç¨‹é€šä¿¡ã€ç¼“å­˜æ•°æ® |
| Global Mem | å…¨å±€å¯è§     | æ‰€æœ‰çº¿ç¨‹å¯è§ï¼Œå¸¦å®½å¤§ä½†å»¶è¿Ÿé«˜ | 400â€“800 cycles | ä¸»æ•°æ®å­˜å‚¨                 |

ğŸ‘‰ **ä¼˜åŒ–å…³é”®**ï¼šå°½é‡å‡å°‘ Global Memory è®¿é—®ï¼Œåˆ©ç”¨ shared memory åœ¨ block å†…ç¼“å­˜ & å¹¶è¡Œè®¡ç®—ã€‚

------

## 2ï¸âƒ£ åŸºç¡€å®éªŒï¼šHello Shared Memory

### èƒŒæ™¯

- æ¯ä¸ª block çš„çº¿ç¨‹å¯ä»¥é€šè¿‡ **shared memory** å…±äº«æ•°æ®ã€‚
- å¿…é¡»ç”¨ `__syncthreads()` ä¿è¯çº¿ç¨‹åŒæ­¥ï¼Œå¦åˆ™å¯èƒ½æœ‰çº¿ç¨‹è¿˜æ²¡å†™æ•°æ®å°±è¢«åˆ«äººè¯»èµ°ã€‚

### ä»£ç ï¼š`shared_hello.cu`

```c++
#include <stdio.h>

__global__ void copy_shared(float* device_out, const float* device_in, int N)
{
    // å£°æ˜ Block å†…çš„å…±äº«å†…å­˜(å›ºå®š 256 ä¸ªfloat)
    __shared__ float share_data[256];

    int global_tid = blockIdx.x * blockDim.x + threadIdx.x;
    int local_tid = threadIdx.x;

    if (global_tid < N)
    {
        // step 1:ä» global memory æ‹·è´åˆ° shared memory
        share_data[local_tid] = device_in[global_tid];

        // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆæ‹·è´
        __syncthreads();

        // step 2: ä½¿ç”¨shared memory çš„å€¼
        device_out[global_tid] = share_data[local_tid] * 2.0f;
    }
}

int main()
{
    const int N = 256;
    size_t bytes = N * sizeof(float);
    float host_in[N], host_out[N];
    for (int i = 0; i < N; i++)
    {
        host_in[i] = i;
    }

    float *device_in, *device_out;
    cudaMalloc(&device_in, bytes);
    cudaMalloc(&device_out, bytes);

    cudaMemcpy(device_in, host_in, bytes, cudaMemcpyHostToDevice);

    copy_shared<<<1, 256>>>(device_out, device_in, N);
    cudaMemcpy(host_out, device_out, bytes, cudaMemcpyDeviceToHost);

    for (int i = 0; i < N; i++)
    {
        printf("host_out[%d] = %f\n", i, host_out[i]);
    }

    cudaFree(device_in);
    cudaFree(device_out);

    return 0;
}

```

### è¿è¡Œ

```bash
nvcc -O2 shared_hello.cu -o shared_hello
./shared_hello
```

### ç»“æœ

![image-20250901000434277](./report_day9.assets/image-20250901000434277.png)

ğŸ‘‰ è¯æ˜æ•°æ®ç¡®å®åœ¨ shared memory ä¸­è¢«ä¿®æ”¹ã€‚

------

## 3ï¸âƒ£  æ·±åº¦è¿½é—®ï¼ˆæ€è€ƒé¢˜ï¼‰

1. ### shared memory bank å†²çªå…·ä½“æ˜¯æ€ä¹ˆå‘ç”Ÿçš„ï¼Ÿé¿å…ç­–ç•¥æœ‰å“ªäº›ï¼Ÿ  

2. ### constant memory è¯»å–çš„å¹¿æ’­æœºåˆ¶ä¸å¤±æ•ˆåœºæ™¯ï¼Ÿ  

3. ### texture memory åœ¨é‡‡æ ·/æ’å€¼ä¸­çš„ä¼˜åŠ¿ï¼Œä½•æ—¶ä¼˜äº globalï¼Ÿ  

4. ### Unified Memory å¦‚ä½•è¿ç§»é¡µé¢ï¼Ÿè¿‡é‡ä½¿ç”¨ä¼šå¦‚ä½• thrashï¼Ÿ  

5. ### `cudaMemcpyAsync` ä¸ stream å…³è”çš„å‰æï¼Ÿ  

6. ### L2 ç¼“å­˜å‘½ä¸­ä¸ stride è®¿é—®å…³ç³»ï¼Ÿ  

## 4ï¸âƒ£ å®éªŒ

### ğŸ§ª å®éªŒ 1ï¼š`matrix_add` â€”â€” Global vs Shared

#### 1ï¸âƒ£ å®éªŒç›®æ ‡

- å¯¹æ¯” **ç›´æ¥ä½¿ç”¨ global memory** vs **tile è¿› shared memory å†è®¡ç®—** çš„æ€§èƒ½å·®å¼‚ã€‚
- ç”¨ CUDA **äº‹ä»¶ API** æµ‹é‡è€—æ—¶ï¼Œå¹¶æ ¹æ®å…¬å¼ä¼°ç®— **å†…å­˜å¸¦å®½åˆ©ç”¨ç‡**ã€‚

------

#### 2ï¸âƒ£ å‡†å¤‡ä»£ç 

åœ¨å­¦ä¹ ç›®å½•ä¸‹æ–°å»º `matrix_add_shared.cu`ï¼ŒæŠŠä¸‹é¢çš„ä»£ç ç²˜è´´è¿›å»ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

#define SIZE 1024 // çŸ©é˜µå¤§å° N * N
// Global memoryç‰ˆæœ¬
__global__ void mat_add_global(const float* A, const float* B, float* C, int N)
{
    int r = blockIdx.y * blockDim.y + threadIdx.y;
    int c = blockIdx.x * blockDim.x + threadIdx.x;

    if (r < N && c < N)
    {
        C[r * N + c] = A[r * N + c] + B[r * N + c];
    }
}

// Shared memoryç‰ˆæœ¬
__global__ void mat_add_shared(const float* A, const float* B, float* C, int N)
{
    __shared__ float A_shared[32][32];
    __shared__ float B_shared[32][32];

    int r = blockIdx.y * blockDim.y + threadIdx.y;
    int c = blockIdx.x * blockDim.x + threadIdx.x;
    int ty = threadIdx.y, tx = threadIdx.x;

    if (r < N && c < N)
    {
        // æŠŠæ•°æ®æ¬åˆ°shared memory
        A_shared[ty][tx] = A[r * N + c];
        B_shared[ty][tx] = B[r * N + c];

        // ç¡®ä¿æ‰€æœ‰çº¿ç¨‹å†™å®Œ
        __syncthreads();

        // ä» shared memory è¯»å‡ºå†è®¡ç®—
        C[r * N + c] = A_shared[ty][tx] + B_shared[ty][tx];
    }
}

int main()
{
    size_t bytes = SIZE * SIZE * sizeof(float);

    // åˆ†é… host å†…å­˜
    float* host_a = (float*)malloc(bytes);
    float* host_b = (float*)malloc(bytes);
    float* host_c = (float*)malloc(bytes);

    for (int i = 0; i < SIZE * SIZE; i++)
    {
        host_a[i] = 1.0f;
        host_b[i] = 2.0f;
    }

    // åˆ†é… device å†…å­˜
    float *device_a, *device_b, *device_c;
    cudaMalloc(&device_a, bytes);
    cudaMalloc(&device_b, bytes);
    cudaMalloc(&device_c, bytes);

    cudaMemcpy(device_a, host_a, bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(device_b, host_b, bytes, cudaMemcpyHostToDevice);

    // æ¯ä¸ª Block 32 * 32 ä¸ªçº¿ç¨‹
    dim3 block(32, 32);
    dim3 grid((SIZE + 31) / 32, (SIZE + 31) / 32);

    // cuda äº‹ä»¶ç”¨äºè®¡æ—¶
    cudaEvent_t start, stop;
    float ms;

    // Global memoryç‰ˆæœ¬
    cudaMemset(device_c, 0, bytes);
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    mat_add_global<<<grid, block>>>(device_a, device_b, device_c, SIZE);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&ms, start, stop);
    printf("mat_add_global: %.3f ms, å¸¦å®½â‰ˆ %.2f GB/s\n", ms,
           (3 * SIZE * SIZE * sizeof(float) / 1e9) / (ms / 1000));

    // Shared memoryç‰ˆæœ¬
    cudaMemset(device_c, 0, bytes);
    cudaEventRecord(start);
    mat_add_shared<<<grid, block>>>(device_a, device_b, device_c, SIZE);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);

    printf("mat_add_shared: %.3f ms, å¸¦å®½â‰ˆ %.2f GB/s\n", ms,
           (3 * SIZE * SIZE * sizeof(float) / 1e9) / (ms / 1000));

    // æ¸…ç†
    cudaFree(device_a);
    cudaFree(device_b);
    cudaFree(device_c);

    free(host_a);
    free(host_b);
    free(host_c);

    return 0;
}

```

------

#### 3ï¸âƒ£ ç¼–è¯‘è¿è¡Œ

```bash
nvcc -O2 matrix_add_shared.cu -o mat_add
./mat_add
```

------

#### 4ï¸âƒ£ é¢„æœŸç»“æœ

![image-20250901010339343](./report_day9.assets/image-20250901010339343.png)

- **Global ç‰ˆ**ï¼šæ¯ä¸ªå…ƒç´ ç›¸åŠ éƒ½è¦è®¿é—®ä¸¤æ¬¡ global memoryã€‚
- **Shared ç‰ˆ**ï¼šblock å†…æŠŠæ•°æ®æ¬åˆ° shared memoryï¼Œå†å±€éƒ¨è®¡ç®—ï¼Œå‡å°‘å…¨å±€è®¿å­˜ã€‚
- ç»“æœï¼š**shared memory ç‰ˆæ›´å¿«ï¼Œå¸¦å®½åˆ©ç”¨ç‡æ›´é«˜**ã€‚

------

### ğŸ§ª å®éªŒ 2ï¼šStride=17 è®¿é—® Bank Conflict & Padding æ¶ˆé™¤

#### 1ï¸âƒ£ å®éªŒç›®æ ‡

- ç†è§£ CUDA **Shared Memory çš„ bank æ¶æ„**ã€‚
- åˆ¶é€  **Bank Conflict**ï¼ˆå†²çªï¼‰ï¼Œç„¶åé€šè¿‡ **Padding** æ¶ˆé™¤ã€‚
- ç”¨ **Nsight Compute** è§‚å¯Ÿå†²çªå¯¹æ€§èƒ½çš„å½±å“ã€‚

------

#### 2ï¸âƒ£ Bank èƒŒæ™¯çŸ¥è¯†

- **Shared Memory** è¢«åˆ†æˆ **32 ä¸ª bank**ï¼Œæ¯ä¸ª bank ä¸€æ¬¡èƒ½æœåŠ¡ 1 ä¸ªçº¿ç¨‹ã€‚
- **Warp = 32 ä¸ªçº¿ç¨‹**ï¼Œç†æƒ³æƒ…å†µï¼šwarp å†…æ¯ä¸ªçº¿ç¨‹è®¿é—®ä¸åŒ bank â†’ **å¹¶è¡Œæ— å†²çª**ã€‚
- å¦‚æœå¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ª bankï¼Œå°±ä¼šäº§ç”Ÿ **å†²çª**ï¼Œè®¿é—®ä¼šè¢« **ä¸²è¡ŒåŒ–**ï¼Œå»¶è¿Ÿå¤§å¤§å¢åŠ ã€‚
- ä¸¾ä¾‹ï¼š
  - stride=1ï¼šçº¿ç¨‹ 0â†’bank0, çº¿ç¨‹ 1â†’bank1 â€¦ â†’ âœ… æ— å†²çª
  - stride=17ï¼šçº¿ç¨‹ 0â†’bank0, çº¿ç¨‹ 1â†’bank17, çº¿ç¨‹ 2â†’bank2 â€¦ çº¿ç¨‹ 16â†’bank16, çº¿ç¨‹ 17â†’bank1 â†’ âŒ å†²çªå‘ç”Ÿ

------

#### 3ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `bank_conflict_stride.cu`ï¼š

```c++
#include <stdio.h>
#include <cuda_runtime.h>

// å†²çªç‰ˆæœ¬ï¼šstride=17
__global__ void conflict(float *out) {
    __shared__ float s_data[32*17]; // stride=17
    int tid = threadIdx.x;
    s_data[tid*17] = tid;           // å¤šä¸ªçº¿ç¨‹æ˜ å°„åˆ°åŒä¸€ä¸ª bank
    __syncthreads();
    out[tid] = s_data[tid*17];
}

// æ— å†²çªç‰ˆæœ¬ï¼šstride=17 + padding
__global__ void no_conflict(float *out) {
    __shared__ float s_data[32*17+1]; // padding +1
    int tid = threadIdx.x;
    s_data[tid*17] = tid;             // padding æ‰“æ•£ bank æ˜ å°„
    __syncthreads();
    out[tid] = s_data[tid*17];
}

// è®¡æ—¶å°è£…å‡½æ•°
float run_and_time(void (*kernel)(float*), float *d_out, int N) {
    cudaEvent_t start, stop;
    cudaEventCreate(&start); cudaEventCreate(&stop);

    cudaEventRecord(start);
    kernel<<<1, N>>>(d_out);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms = 0;
    cudaEventElapsedTime(&ms, start, stop);
    cudaEventDestroy(start); cudaEventDestroy(stop);

    return ms;
}

int main() {
    const int N = 32; // warp å†… 32 çº¿ç¨‹
    size_t bytes = N * sizeof(float);

    float h_out[N];
    float *d_out;
    cudaMalloc(&d_out, bytes);

    // è®¡æ—¶å¹¶è¿è¡Œå†²çªç‰ˆæœ¬
    float t1 = run_and_time(conflict, d_out, N);
    cudaMemcpy(h_out, d_out, bytes, cudaMemcpyDeviceToHost);
    printf("Conflict kernel (%.6f ms):\n", t1);
    for (int i=0;i<5;i++) printf("out[%d]=%.1f ", i, h_out[i]);
    printf("\n");

    // è®¡æ—¶å¹¶è¿è¡Œæ— å†²çªç‰ˆæœ¬
    float t2 = run_and_time(no_conflict, d_out, N);
    cudaMemcpy(h_out, d_out, bytes, cudaMemcpyDeviceToHost);
    printf("No Conflict kernel (%.6f ms):\n", t2);
    for (int i=0;i<5;i++) printf("out[%d]=%.1f ", i, h_out[i]);
    printf("\n");

    cudaFree(d_out);
    return 0;
}

```

------

#### 4ï¸âƒ£ ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 bank_conflict_stride.cu -o bank_conflict
./bank_conflict
```

é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼š

![image-20250901173945736](./report_day9.assets/image-20250901173945736.png)

------

#### 5ï¸âƒ£ ç»“æœåˆ†æ

- ç»“æœä¸€æ ·ï¼Œè¯´æ˜ bank conflict ä¸å½±å“æ­£ç¡®æ€§ã€‚
- **æœ‰å†²çªç‰ˆæœ¬** æ—¶é—´æ˜æ˜¾æ›´é•¿ï¼ˆå†²çªå¯¼è‡´ä¸²è¡ŒåŒ–ï¼‰ã€‚
- **æ— å†²çªç‰ˆæœ¬** æ—¶é—´æ›´çŸ­ï¼ˆpadding æ¶ˆé™¤äº†å†²çªï¼‰ã€‚

------

### ğŸ§ª å®éªŒ 3ï¼šConstant Memory ä¼˜åŠ¿

#### 1ï¸âƒ£ èƒŒæ™¯çŸ¥è¯†

- **Constant Memory**
  - æ¯ä¸ª SM æœ‰ **64KB å¸¸é‡ç¼“å­˜**ï¼Œä¸»è¦ä¼˜åŒ– **warp å†…æ‰€æœ‰çº¿ç¨‹è®¿é—®ç›¸åŒåœ°å€** çš„æƒ…å†µã€‚
  - å¦‚æœä¸€ä¸ª warp çš„ 32 ä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ªå¸¸é‡åœ°å€ â†’ åªéœ€ **1 æ¬¡å–æ•° + å¹¿æ’­**ï¼Œæ•ˆç‡æé«˜ã€‚
  - å¦‚æœ warp å†…çš„çº¿ç¨‹è®¿é—®ä¸åŒåœ°å€ â†’ ä¼šå‘ç”Ÿ **åºåˆ—åŒ–**ï¼Œæ€§èƒ½å¯èƒ½æ¯” global memory è¿˜å·®ã€‚
- **åº”ç”¨åœºæ™¯**
  - CNN å·ç§¯æ ¸æƒé‡ï¼ˆå…¨çº¿ç¨‹ç”¨ç›¸åŒå‚æ•°ï¼‰ã€‚
  - å½’ä¸€åŒ–ç³»æ•°ã€è¶…å‚æ•°ï¼ˆæ¯”å¦‚å­¦ä¹ ç‡ã€ç¼©æ”¾å› å­ï¼‰ã€‚
  - ä¸é€‚åˆï¼šçº¿ç¨‹å„è‡ªè¯»å–ä¸åŒå¸¸é‡çš„æƒ…å†µã€‚

------

#### 2ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `const_memory.cu`ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

// å¸¸é‡å†…å­˜
#define COEF_SIZE 1024
__constant__ float device_coef[COEF_SIZE]; // GPUå¸¸é‡å†…å­˜

__global__ void kernel_const(const float* in, float* out, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        float val = in[i];
        for (int j = 0; j < 1000; j++)
        {
            val *= device_coef[j % COEF_SIZE];
        }
        out[i] = val;
    }
}

__global__ void kernel_global(const float* in, float* out, const float* coef, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        float val = in[i];
        for (int j = 0; j < 1000; j++)
        {
            val *= coef[j % COEF_SIZE];
        }
        out[i] = val;
    }
}

// float run_and_time(void (*kernel)(float))

int main()
{
    const int N = 1 << 24; // 16M å…ƒç´ 
    size_t bytes = N * sizeof(float);

    float* host_in = (float*)malloc(bytes);
    float* host_out = (float*)malloc(bytes);
    for (int i = 0; i < N; i++)
    {
        host_in[i] = 1.0f;
    }

    float *device_in, *device_out, *device_coef_global;
    cudaMalloc(&device_in, bytes);
    cudaMalloc(&device_out, bytes);
    cudaMalloc(&device_coef_global, COEF_SIZE * sizeof(float));

    cudaMemcpy(device_in, host_in, bytes, cudaMemcpyHostToDevice);

    float host_coef[COEF_SIZE];
    for (int i = 0; i < COEF_SIZE; i++)
    {
        host_coef[i] = 1.0f;
    }
    // æŠŠ coef æ”¾åˆ° constant memory
    cudaMemcpyToSymbol(device_coef, host_coef, COEF_SIZE * sizeof(float));
    // æŠŠ coef æ”¾åˆ° global memory
    cudaMemcpy(device_coef_global, host_coef, COEF_SIZE * sizeof(float), cudaMemcpyHostToDevice);

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    float ms;

    // constant memory
    cudaEventRecord(start);
    kernel_const<<<grid, block>>>(device_in, device_out, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);
    printf("Const memory kernel: %.3f ms\n", ms);

    // global memory
    cudaEventRecord(start);
    kernel_global<<<grid, block>>>(device_in, device_out, device_coef_global, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms, start, stop);
    printf("Global memory kernel: %.3f ms\n", ms);

    cudaFree(device_in);
    cudaFree(device_out);
    cudaFree(device_coef_global);

    free(host_in);
    free(host_out);

    return 0;
}

```

------

#### 3ï¸âƒ£ ç¼–è¯‘è¿è¡Œ

```bash
nvcc -O2 const_memory.cu -o const_mem
./const_mem
```

------

#### 4ï¸âƒ£ é¢„æœŸè¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰

![image-20250902172528149](./report_day9.assets/image-20250902172528149.png)

- **å¸¸é‡å†…å­˜ç‰ˆæœ¬åœ¨å¹¿æ’­è®¿é—®æ¨¡å¼ä¸‹æ›´å¿«**ï¼Œå› ä¸º warp å†…æ‰€æœ‰çº¿ç¨‹è®¿é—®åŒä¸€ä¸ªå¸¸é‡åœ°å€æ—¶ï¼Œåªéœ€ä¸€æ¬¡å–æ•°å³å¯å¹¿æ’­ç»™ 32 ä¸ªçº¿ç¨‹ã€‚
- **å…¨å±€å†…å­˜ç‰ˆæœ¬åœ¨è¿™ç§åœºæ™¯ä¸‹æ›´æ…¢**ï¼Œå³ä¾¿æœ‰ L1/L2 cacheï¼Œwarp å†…ä»è¦å¤šæ¬¡è¯·æ±‚ç›¸åŒåœ°å€ï¼Œå¼€é”€æ›´å¤§ã€‚

âš ï¸ æ³¨æ„è¡¥å……è¯´æ˜ï¼š

- å¦‚æœ warp å†…çº¿ç¨‹è®¿é—®çš„æ˜¯ **ä¸åŒåœ°å€**ï¼Œé‚£ä¹ˆå¸¸é‡å†…å­˜ä¼šå‘ç”Ÿ **åºåˆ—åŒ–**ï¼Œæ€§èƒ½å¯èƒ½ä¸å…¨å±€å†…å­˜ç›¸å½“ç”šè‡³æ›´å·®ã€‚

------

### ğŸ§ª å®éªŒ 4ï¼šUnified Memory è¶…è¿‡æ˜¾å­˜å®¹é‡

#### 1ï¸âƒ£ èƒŒæ™¯

- **Unified Memory (UM)**ï¼šç”¨ `cudaMallocManaged` åˆ†é…çš„å†…å­˜ï¼Œå¯ä»¥åœ¨ CPU å’Œ GPU ä¹‹é—´è‡ªåŠ¨è¿ç§»ã€‚
- å½“æ•°æ®é‡ **è¶…è¿‡æ˜¾å­˜å®¹é‡** æ—¶ï¼ŒGPU åœ¨è®¿é—®æ•°æ®æ—¶ä¼šè§¦å‘ **page migrationï¼ˆé¡µè¿ç§»ï¼‰**ï¼š
  - æŠŠæ•°æ®ä»ç³»ç»Ÿå†…å­˜æ¬åˆ°æ˜¾å­˜ã€‚
  - å¦‚æœæ˜¾å­˜ä¸å¤Ÿ â†’ ä¼šä¸æ–­æ¢å…¥/æ¢å‡ºï¼Œååé‡éª¤é™ã€‚

------

#### 2ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `unified_mem.cu`ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void touch(float* data, long N)
{
    long i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        data[i] += 1.0f;
    }
}

int main(int argc, char* argv[])
{
    // N å¤ªå¤§å¯èƒ½å¯¼è‡´ç³»ç»Ÿç›´æ¥OOM
    long N = (long)1e9; // é»˜è®¤ 1e9 (~4 GB)
    if (argc > 1)
    {
        N = atol(argv[1]); // å¯ä»¥ä»å‘½ä»¤è¡Œä¼  N
    }
    size_t bytes = N * sizeof(float);

    printf("Allocating %.2f GB Unified Memory...\n", bytes / 1e9);

    float* data;
    cudaMallocManaged(&data, bytes); // unified memory

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    touch<<<grid, block>>>(data, N);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms;
    cudaEventElapsedTime(&ms, start, stop);

    printf("Kernel with UM (N=%ld, %.2f GB): %.3f ms\n", N, bytes / 1e9, ms);

    cudaFree(data);

    return 0;
}
```

------

#### 3ï¸âƒ£ ç¼–è¯‘è¿è¡Œ

```bash
nvcc -O2 unified_mem.cu -o unified_mem
./unified_mem 1000000000
```

------

#### 4ï¸âƒ£ ç»“æœåˆ†æ

![image-20250902181010601](./report_day9.assets/image-20250902181010601.png)

- å¦‚æœ `N=1e9 (~4 GB)`ï¼Œåœ¨ 3080 (10GB æ˜¾å­˜) ä¸Šè¿è¡Œæ­£å¸¸ï¼Œæ€§èƒ½æ¥è¿‘ global memoryã€‚
- å¦‚æœ `N=2e9 (~8 GB)`ï¼Œä¾ç„¶èƒ½æ”¾è¿›æ˜¾å­˜ï¼ˆ10GBï¼‰ï¼Œæ€§èƒ½ç•¥ä¸‹é™ã€‚
- å¦‚æœä½ æ”¹æˆ `N=5e9 (~20 GB)`ï¼Œ**è¶…è¿‡æ˜¾å­˜å®¹é‡**ï¼Œå°±ä¼šè§¦å‘ **page migration**ï¼š
  - ç¨‹åºè¿˜èƒ½è¿è¡Œï¼Œä½†æ—¶é—´ä¼šæ˜æ˜¾å˜é•¿ï¼ˆå‡ åå€ï¼‰ã€‚
  - ç”¨ `ncu` æˆ– `nsys` profileï¼Œå¯ä»¥çœ‹åˆ°å¤§é‡ **UM page migration** äº‹ä»¶ã€‚

------

#### 5ï¸âƒ£ è¿›ä¸€æ­¥å®éªŒ

1. ### (a) ç”¨ Nsight Compute

   è¿è¡Œï¼š

   ```bash
   ncu --set full ./unified_mem 3000000000
   ```

   åœ¨æŠ¥å‘Šé‡Œçœ‹ `Unified Memory Memcpy`ï¼Œä¼šçœ‹åˆ°å¤§é‡è¿ç§»äº‹ä»¶ã€‚

   ### (b) ç”¨ nvidia-smi åŠ¨æ€è§‚å¯Ÿ

   å¦å¼€ä¸€ä¸ªç»ˆç«¯è¿è¡Œï¼š

   ```
   watch -n 0.5 nvidia-smi
   ```

   å¦‚æœä½ è·‘ `./unified_mem 3000000000`ï¼Œæ˜¾å­˜å ç”¨ä¼š **ä¸Šä¸‹æ³¢åŠ¨**ï¼ˆé¡µè¿ç§»è¿›è¿›å‡ºå‡ºï¼‰ã€‚

------

âš ï¸ æ³¨æ„äº‹é¡¹ï¼š

- ä¸€æ¬¡æ€§åˆ†é…è¶…è¿‡ 20â€“30GBï¼ˆè¶…ç³»ç»Ÿå†…å­˜ï¼‰å¯èƒ½ç›´æ¥æŠ¥ `cudaErrorMemoryAllocation`ã€‚
- å»ºè®® **å…ˆè¯• 4GB / 8GB / 12GB**ï¼Œé€æ­¥å¢å¤§ã€‚

------

### ğŸ§ª å®éªŒ 5ï¼š`cudaMemcpyAsync` + Stream é‡å æ‹·è´/è®¡ç®—

#### 1ï¸âƒ£ èƒŒæ™¯

- `cudaMemcpy` é»˜è®¤æ˜¯ **åŒæ­¥çš„**ï¼šCPU ä¼šç­‰æ•°æ®æ‹·è´å®Œæˆåå†ç»§ç»­æ‰§è¡Œï¼ŒGPU ä¹Ÿä¸èƒ½åŒæ—¶è®¡ç®—ã€‚
- `cudaMemcpyAsync` + **pinned memoryï¼ˆé¡µé”å®šå†…å­˜ï¼‰** + **stream** å¯ä»¥è®©ï¼š
  - æ•°æ®æ‹·è´å’Œè®¡ç®—å¹¶è¡Œè¿›è¡Œã€‚
  - æå‡æ•´ä½“ååã€‚
- âš ï¸ å…³é”®æ¡ä»¶ï¼šå¿…é¡»ç”¨ **cudaMallocHost** åˆ†é… host å†…å­˜ï¼Œå¦åˆ™æ‹·è´æ— æ³•çœŸæ­£å¼‚æ­¥ã€‚

------

#### 2ï¸âƒ£ å®éªŒä»£ç 

ä¿å­˜ä¸º `async_copy.cu`ï¼š

```c++
#include <cuda_runtime.h>
#include <stdio.h>

// ç®€å•çš„è®¡ç®—kernel (æ¨¡æ‹Ÿè€—æ—¶è®¡ç®—)
__global__ void computer(float* data, int N)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N)
    {
        float x = data[i];
        for (int j = 0; j < 10000; j++)
        {
            x = x * 0.999f + 0.001f;
        }
        data[i] = x;
    }
}

int main()
{
    const int N = 1 << 24;
    size_t bytes = N * sizeof(float);

    float *host_data, *device_data;
    cudaMallocHost(&host_data, bytes); // é¡µé”å®šå†…å­˜(å¿…é¡»)
    cudaMalloc(&device_data, bytes);

    for (int i = 0; i < N; i++)
    {
        host_data[i] = 1.0f;
    }

    dim3 block(256);
    dim3 grid((N + block.x - 1) / block.x);

    // åŒæ­¥ç‰ˆæœ¬
    cudaEvent_t start, stop;
    float ms_sync, ms_async;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    // åŒæ­¥æ‹·è´ H2D
    cudaMemcpy(device_data, host_data, bytes, cudaMemcpyHostToDevice);
    // è®¡ç®—
    computer<<<grid, block>>>(device_data, N);
    // åŒæ­¥æ‹·è´ D2H
    cudaMemcpy(host_data, device_data, bytes, cudaMemcpyDeviceToHost);

    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    cudaEventElapsedTime(&ms_sync, start, stop);

    // å¼‚æ­¥ç‰ˆæœ¬
    cudaStream_t s1, s2;
    cudaStreamCreate(&s1);
    cudaStreamCreate(&s2);

    cudaEventRecord(start);
    // H2D å¼‚æ­¥æ‹·è´
    cudaMemcpyAsync(device_data, host_data, bytes, cudaMemcpyHostToDevice, s1);
    // è®¡ç®—æ”¾åˆ°å¦ä¸€ä¸ªstream
    computer<<<grid, block, 0, s2>>>(device_data, N);
    // D2H å¼‚æ­¥æ‹·è´
    cudaMemcpyAsync(host_data, device_data, bytes, cudaMemcpyDeviceToHost, s1);
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&ms_async, start, stop);

    printf("Sync  version time: %.3f ms\n", ms_sync);
    printf("Async version time: %.3f ms\n", ms_async);

    cudaStreamDestroy(s1);
    cudaStreamDestroy(s2);
    cudaFree(device_data);
    cudaFreeHost(host_data);

    return 0;
}

```

------

#### 3ï¸âƒ£ ç¼–è¯‘ & è¿è¡Œ

```bash
nvcc -O2 async_copy.cu -o async_copy
./async_copy
```

------

#### 4ï¸âƒ£ é¢„æœŸç»“æœ

![image-20250903175422021](./report_day9.assets/image-20250903175422021.png)

- **åŒæ­¥ç‰ˆ**ï¼šæ‹·è´(H2D) â†’ è®¡ç®— â†’ æ‹·è´(D2H)ï¼Œå®Œå…¨ä¸²è¡Œã€‚
- **å¼‚æ­¥ç‰ˆ**ï¼šæ‹·è´å’Œè®¡ç®— **éƒ¨åˆ†é‡å **ï¼Œæ€»æ—¶é—´æ›´çŸ­ã€‚

#### 5ï¸âƒ£ æ·±åº¦è¿½é—®

1. ä¸ºä»€ä¹ˆéœ€è¦ `cudaMallocHost`ï¼ˆpinned memoryï¼‰æ‰èƒ½çœŸæ­£å¼‚æ­¥ï¼Ÿ
   - å› ä¸ºåªæœ‰ pinned å†…å­˜æ‰èƒ½è¢« DMA å¼•æ“ç›´æ¥è®¿é—®ï¼Œé pinned å†…å­˜ä¼šéšå¼è½¬æˆåŒæ­¥æ‹·è´ã€‚
2. ä¸ºä»€ä¹ˆç”¨äº†ä¸¤ä¸ª streamï¼Ÿ
   - é¿å…æ‹·è´å’Œè®¡ç®—åœ¨åŒä¸€ä¸ª stream ä¸²è¡ŒåŒ–ã€‚
3. å¦‚ä½•éªŒè¯æ‹·è´å’Œè®¡ç®—æ˜¯å¦çœŸçš„é‡å ï¼Ÿ
   - ç”¨ `nsys profile ./async_copy` æˆ– Nsight Systems æŸ¥çœ‹æ—¶é—´çº¿ï¼Œå¯ä»¥çœ‹åˆ° **memcpy å’Œ kernel é‡å æ‰§è¡Œ**ã€‚

------

### âœ… æ€»ç»“

- **å®éªŒ 1**ï¼šShared Memory æ˜¾è‘—å‡å°‘å…¨å±€è®¿å­˜ï¼Œå¸¦å®½åˆ©ç”¨æ›´é«˜ã€‚
- **å®éªŒ 2**ï¼šstride=17 å¯¼è‡´ bank conflictï¼ŒåŠ  padding æ¶ˆé™¤åæ€§èƒ½æ¢å¤ã€‚
- **å®éªŒ 3**ï¼šconstant memory å¹¿æ’­è®¿é—®æ•ˆç‡æé«˜ã€‚
- **å®éªŒ 4**ï¼šUnified Memory è¶…æ˜¾å­˜æ—¶é¢‘ç¹è¿ç§»ï¼Œæ€§èƒ½éª¤é™ã€‚
- **å®éªŒ 5**ï¼šå¼‚æ­¥æ‹·è´ + stream å¯å®ç°æ‹·è´/è®¡ç®—é‡å ï¼Œæ˜¾è‘—åŠ é€Ÿã€‚