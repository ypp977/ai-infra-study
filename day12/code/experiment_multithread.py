import ctypes
import numpy as np
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.tools as cuda_tools
import pycuda.autoinit
import threading,time

# åŠ è½½æ’ä»¶
ctypes.CDLL("./libswish.so")

# åˆ›å»ºæ—¥å¿—å™¨
logger = trt.Logger(trt.Logger.INFO)

def build_engine():
    # åˆ›å»ºæ„å»ºå™¨
    builder = trt.Builder(logger)
    # åˆ›å»ºç½‘ç»œ
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    # åˆ›å»ºé…ç½®
    config = builder.create_builder_config()
    # åˆå§‹åŒ–æ’ä»¶
    trt.init_libnvinfer_plugins(logger,'')

    # åˆ›å»ºè¾“å…¥
    input_tensor = network.add_input("input",trt.DataType.FLOAT,(1,16))
    # è·å–æ’ä»¶åˆ—è¡¨
    creator_list = trt.get_plugin_registry().plugin_creator_list
    # è·å– Swish æ’ä»¶
    swish_creator = [c for c in creator_list if c.name == "SwishPlugin"][0]
    # åˆ›å»º Swish æ’ä»¶
    plugin = swish_creator.create_plugin("swish_layer",trt.PluginFieldCollection([]))
    # åˆ›å»º Swish å±‚
    swish_layer = network.add_plugin_v2([input_tensor],plugin)
    # æ ‡è®°è¾“å‡º
    network.mark_output(swish_layer.get_output(0))

    # åºåˆ—åŒ–ç½‘ç»œ
    serialized_engine = builder.build_serialized_network(network,config)
    # åˆ›å»ºè¿è¡Œæ—¶
    runtime = trt.Runtime(logger)
    # ååºåˆ—åŒ–ç½‘ç»œ
    return runtime.deserialize_cuda_engine(serialized_engine)

# åˆ›å»ºå¼•æ“
engine = build_engine()

def run_infer(thread_id,n_iters=50):
    # åˆ›å»º CUDA ä¸Šä¸‹æ–‡
    ctx = cuda.Device(0).make_context()
    try:
        # åˆ›å»º TensorRT æ‰§è¡Œä¸Šä¸‹æ–‡
        trt_context = engine.create_execution_context()
        # åˆ›å»ºè¾“å…¥
        inp = np.random.randn(1,16).astype(np.float32)
        # åˆ›å»ºè¾“å‡º
        out = np.empty_like(inp)

        # åˆ›å»ºè¾“å…¥ GPU å†…å­˜
        d_input = cuda.mem_alloc(inp.nbytes)
        # åˆ›å»ºè¾“å‡º GPU å†…å­˜
        d_output = cuda.mem_alloc(out.nbytes)

        # è®°å½•å¼€å§‹æ—¶é—´
        start = time.perf_counter()

        for _ in range(n_iters):
            # Host â†’ Device
            cuda.memcpy_htod(d_input,inp)
            # æ‰§è¡Œæ¨ç†
            trt_context.execute_v2([int(d_input),int(d_output)])
            # Device â†’ Host
            cuda.memcpy_dtoh(out,d_output)
        # åŒæ­¥ CUDA ä¸Šä¸‹æ–‡
        cuda.Context.synchronize()
        # è®°å½•ç»“æŸæ—¶é—´
        end = time.perf_counter()

        # è®¡ç®—å¹³å‡è€—æ—¶
        avg_time = (end - start) / n_iters * 1000
        print(f"[çº¿ç¨‹ {thread_id}] å¹³å‡è€—æ—¶: {avg_time:.3f} ms")
    finally:
        # å¼¹å‡º CUDA ä¸Šä¸‹æ–‡
        ctx.pop()

def test_multithread(n_threads=4):
    # åˆ›å»ºçº¿ç¨‹åˆ—è¡¨
    threads = []
    # è®°å½•å¼€å§‹æ—¶é—´
    start = time.perf_counter()
    # åˆ›å»ºçº¿ç¨‹
    for i in range(n_threads):
        t = threading.Thread(target=run_infer,args=(i,))
        # å¯åŠ¨çº¿ç¨‹
        t.start()
        # æ·»åŠ çº¿ç¨‹åˆ°åˆ—è¡¨
        threads.append(t)
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in threads:
        # ç­‰å¾…çº¿ç¨‹å®Œæˆ
        t.join()
    # è®°å½•ç»“æŸæ—¶é—´
    end = time.perf_counter()
    print(f"ğŸ”¥ {n_threads} çº¿ç¨‹æ€»è€—æ—¶: {(end - start)*1000:.2f} ms")

if __name__ == "__main__":
    # å•çº¿ç¨‹æµ‹è¯•
    print("=====å•çº¿ç¨‹====")
    test_multithread(1)

    # åŒçº¿ç¨‹æµ‹è¯•
    print("\n====åŒçº¿ç¨‹====")
    test_multithread(2)

    # å››çº¿ç¨‹æµ‹è¯•
    print("\n====å››çº¿ç¨‹====")
    test_multithread(4)
