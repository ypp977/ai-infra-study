# Day 3 â€” PyTorch â†’ ONNX ä¿å§†çº§æ•™ç¨‹

## ä¸€ã€ğŸ¯å­¦ä¹ ç›®æ ‡

- ç”¨ PyTorch è®­ç»ƒä¸€ä¸ªç®€å•æ¨¡å‹ï¼ˆMNIST/CIFAR10ï¼‰
- å¯¼å‡ºä¸º ONNX æ ¼å¼
- ç”¨ ONNX Runtime åšæ¨ç†æµ‹è¯•
- ï¼ˆè¿›é˜¶ï¼‰å†™ä¸€ä¸ª K8s éƒ¨ç½²æ–‡ä»¶è¿è¡Œ onnxruntime å®¹å™¨

## äºŒã€å‡†å¤‡ç¯å¢ƒ

ç¡®ä¿ä½ åœ¨ Day1 å·²ç»æœ‰äº† **PyTorch å®¹å™¨**ã€‚å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥ç”¨å®˜æ–¹å¸¦ CUDA çš„é•œåƒï¼š

```bash
docker run --gpus all -it --rm \
  -v $PWD:/workspace \
  ai-infra:day1 /bin/bash
```

è¿›å…¥å®¹å™¨åæ£€æŸ¥ä¾èµ–ï¼š

```python
import importlib

for pkg in ["onnx", "onnxruntime", "torchvision"]:
    try:
        m = importlib.import_module(pkg)
        print(f"âœ… {pkg} å­˜åœ¨, ç‰ˆæœ¬: {getattr(m, '__version__', 'æœªçŸ¥')}")
    except ImportError:
        print(f"âŒ {pkg} æœªå®‰è£…")
```

![image-20250826044942998](./report_day3.assets/image-20250826044942998.png)

ç¼ºå°‘å“ªä¸ªä¾èµ–å®‰è£…å“ªä¸ªä¾èµ–ï¼š

```bash
pip install onnx onnxruntime-gpu torchvision
```

------

## äºŒã€è®­ç»ƒä¸€ä¸ªç®€å•çš„ PyTorch æ¨¡å‹

æˆ‘ä»¬ç”¨ **MNIST æ‰‹å†™æ•°å­—åˆ†ç±»**æ¥åšå®éªŒã€‚

```python
# train_mnist.py
import torch, torch.nn as nn, torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# 1. æ•°æ®é›†
transform = transforms.Compose([transforms.ToTensor()])
trainset = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# 2. æ¨¡å‹
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)
    def forward(self, x):
        x = x.view(-1, 28*28)
        return self.fc2(self.relu(self.fc1(x)))

model = MLP()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 3. è®­ç»ƒ
for epoch in range(1):
    for imgs, labels in trainloader:
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"epoch {epoch}, loss={loss.item():.4f}")

# 4. ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), "mlp.pth")
print("æ¨¡å‹ä¿å­˜æˆåŠŸ mlp.pth")
```

è¿è¡Œï¼š

```bash
python train_mnist.py
```

![image-20250826044459940](./report_day3.assets/image-20250826044459940.png)

è¿è¡Œåç›®å½•å¦‚ä¸‹æ‰€ç¤ºï¼š

![image-20250826044547242](./report_day3.assets/image-20250826044547242.png)

------

## ä¸‰ã€å¯¼å‡ºä¸º ONNX

å†™ä¸ªè„šæœ¬å¯¼å‡ºï¼š

```python
# export_onnx.py
import torch, torch.nn as nn

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)
    def forward(self, x):
        x = x.view(-1, 28*28)
        return self.fc2(self.relu(self.fc1(x)))

model = MLP()
model.load_state_dict(torch.load("mlp.pth"))
model.eval()

dummy_input = torch.randn(1, 1, 28, 28)
torch.onnx.export(
    model, dummy_input, "mlp.onnx",
    input_names=["input"], output_names=["output"],
    dynamic_axes={"input": {0: "batch"}, "output": {0: "batch"}},
    opset_version=12   # å¼ºåˆ¶å¯¼å‡º ONNX opset 12
)
print("ONNX æ¨¡å‹å·²å¯¼å‡º: mlp.onnx")
```

è¿è¡Œï¼š

```bash
python export_onnx.py
```

ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š![image-20250826045118175](./report_day3.assets/image-20250826045118175.png)

------

## å››ã€ç”¨ ONNX Runtime æ¨ç†æµ‹è¯•

```python
# onnx_infer.py
import onnxruntime as ort
import numpy as np

# åŠ è½½ ONNX
sess = ort.InferenceSession("mlp.onnx", providers=["CUDAExecutionProvider", "CPUExecutionProvider"])

# éšæœºè¾“å…¥
x = np.random.randn(1, 1, 28, 28).astype(np.float32)
inputs = {sess.get_inputs()[0].name: x}
outputs = sess.run(None, inputs)

print("æ¨ç†ç»“æœ:", outputs[0])
```

è¿è¡Œï¼š

```bash
python onnx_infer.py
```

![image-20250826045235525](./report_day3.assets/image-20250826045235525.png)

------

## äº”ã€ï¼ˆè¿›é˜¶ï¼‰éƒ¨ç½²åˆ° K8s

åˆ›å»º`k8s_onnx_infer.py`é’ˆå¯¹k8sçš„onnx_infer

```python
import onnxruntime as ort
import numpy as np

# æ¨¡å‹è·¯å¾„ï¼ˆé€šè¿‡ ConfigMap æŒ‚è½½åˆ° /workspace/modelsï¼‰
MODEL_PATH = "/workspace/models/mlp.onnx"

print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH}")
sess = ort.InferenceSession(MODEL_PATH, providers=["CPUExecutionProvider"])

# éšæœºè¾“å…¥æµ‹è¯•
x = np.random.randn(1, 1, 28, 28).astype(np.float32)
inputs = {sess.get_inputs()[0].name: x}
outputs = sess.run(None, inputs)

print("âœ… æ¨ç†ç»“æœ:", outputs[0])
```

åˆ›å»ºé€‚åˆ **æ¨ç†å®éªŒ**çš„ Dockerfile

```dockerfile
# NGC å®˜æ–¹ PyTorch é•œåƒï¼ˆå« CUDA/cuDNN/NCCL + TRT SDKï¼‰
FROM nvcr.io/nvidia/pytorch:25.06-py3

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_NO_CACHE_DIR=1 PIP_DEFAULT_TIMEOUT=120 PIP_RETRIES=5

# åªè¡¥å¿…è¦å·¥å…·
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates curl && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

# ä»…è¡¥ ONNX + ORTï¼ˆTRT SDK å·²åœ¨åŸºé•œåƒé‡Œï¼‰
RUN python -m pip install --extra-index-url https://pypi.org/simple -i https://mirrors.aliyun.com/pypi/simple \
    --only-binary=:all: --prefer-binary \
    onnx==1.16.1 onnxruntime-gpu==1.20.0

# å¿«é€Ÿè‡ªæ£€ï¼Œæ„å»ºæ—¶å°±èƒ½æ—©å‘ç°ç¯å¢ƒé—®é¢˜ï¼ˆå¯æ³¨é‡Šï¼‰
RUN python - <<'PY'
import sys, torch, onnx, onnxruntime as ort
print("Python:", sys.version.split()[0])
print("Torch:", torch.__version__, "| CUDA:", torch.version.cuda, "| cuDNN:", torch.backends.cudnn.version())
print("ONNX:", onnx.__version__)
print("ORT:", ort.__version__, "| Providers:", ort.get_available_providers())
PY

# æ‹·è´æ¨ç†è„šæœ¬
COPY k8s_onnx_infer.py /workspace/k8s_onnx_infer.py

# é»˜è®¤æ‰§è¡Œè„šæœ¬ï¼ˆä¹Ÿå¯åœ¨ K8s é‡Œè¦†ç›–ï¼‰
CMD ["python", "/workspace/k8s_onnx_infer.py"]
```

æ„å»ºé•œåƒ

```bash
minikube image build -t my-ai-infer:lite .
```

![image-20250826194311109](./report_day3.assets/image-20250826194311109.png)![image-20250826194430418](./report_day3.assets/image-20250826194430418.png)

å†™ä¸ªç®€å•çš„ Deployment æ–‡ä»¶ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: onnxruntime-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: onnxruntime
  template:
    metadata:
      labels:
        app: onnxruntime
    spec:
      volumes:
      - name: model-cm
        configMap:
          name: mlp-onnx-cm
      containers:
      - name: onnxruntime
        image: my-ai-infer:lite
        imagePullPolicy: IfNotPresent
        command: ["python", "/workspace/onnx_infer.py"]
        volumeMounts:
        - name: model-cm
          mountPath: /workspace
        ports:
        - containerPort: 8001
---
apiVersion: v1
kind: Service
metadata:
  name: onnxruntime-svc
spec:
  selector:
    app: onnxruntime
  type: ClusterIP
  ports:
  - port: 8001
    targetPort: 8001
```

éƒ¨ç½²ï¼š

```bash
# å…ˆåˆ›å»º ConfigMapï¼ˆæŒ‚è½½æ¨¡å‹æ–‡ä»¶ï¼‰
kubectl delete configmap mlp-onnx-cm --ignore-not-found
kubectl create configmap mlp-onnx-cm --from-file=mlp.onnx

kubectl delete -f k8s/onnxruntime.yaml

kubectl apply -f k8s/onnxruntime.yaml

kubectl logs deploy/onnxruntime-demo -c onnxruntime
```

![image-20250826195829771](./report_day3.assets/image-20250826195829771.png)
